总结机器学习基础的一些问题，包括一些基础概念问题，损失函数

# 基础知识





------

## 特征工程

### 特征缩放

特征缩放主要分为两种方法，归一化和正则化。

#### 归一化

1. **归一化(Normalization)，也称为标准化**，这里不仅仅是对特征，实际上对于原始数据也可以进行归一化处理，它是将特征（或者数据）都**缩放到一个指定的大致相同的数值区间内**。
2. **归一化的两个原因**：

- 某些算法要求样本数据或特征的数值**具有零均值和单位方差**；
- 为了消除样本数据或者特征之间的**量纲影响，即消除数量级的影响**。如下图所示是包含两个属性的目标函数的等高线
  - **数量级的差异将导致量级较大的属性占据主导地位**。从下图左看到量级较大的属性会让椭圆的等高线压缩为直线，使得目标函数仅依赖于该属性。
  - **数量级的差异会导致迭代收敛速度减慢**。原始的特征进行梯度下降时，每一步梯度的方向会偏离最小值（等高线中心点）的方向，**迭代次数较多，且学习率必须非常小**，否则非常容易引起**宽幅震荡**。但经过标准化后，每一步梯度的方向都几乎指向最小值（等高线中心点）的方向，**迭代次数较少**。
  - **所有依赖于样本距离的算法对于数据的数量级都非常敏感**。比如 KNN 算法需要计算距离当前样本最近的 k 个样本，当属性的量级不同，选择的最近的 k 个样本也会不同。

![图来自《百面机器学习》](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/normalization_example.png)

3. 常用的两种归一化方法：

- **线性函数归一化(Min-Max Scaling)**。它对原始数据进行线性变换，使得结果映射到`[0,1]`的范围，实现对原始数据的等比缩放，公式如下：

$$
X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}}
$$

其中 X 是原始数据，$X_{max}, X_{min}$分别表示数据最大值和最小值。

- **零均值归一化(Z-Score Normalization)**。它会将原始数据映射到均值为 0，标准差为 1 的分布上。假设原始特征的均值是$\mu$、方差是$\sigma$，则公式如下：

$$
z = \frac{x-\mu}{\sigma}
$$

4. 如果数据集分为训练集、验证集、测试集，那么**三个数据集都采用相同的归一化参数，数值都是通过训练集计算得到**，即上述两种方法中分别需要的数据最大值、最小值，方差和均值都是通过训练集计算得到（这个做法类似于深度学习中批归一化，BN的实现做法）。
5. 归一化不是万能的，实际应用中，**通过梯度下降法求解的模型是需要归一化的，这包括线性回归、逻辑回归、支持向量机、神经网络等模型**。但**决策树模型不需要**，以 C4.5 算法为例，决策树在分裂结点时候主要依据数据集 D 关于特征 x 的信息增益比，而信息增益比和特征是否经过归一化是无关的，归一化不会改变样本在特征 x 上的信息增益。

#### 正则化

1. 正则化是**将样本或者特征的某个范数（如 L1、L2 范数）缩放到单位 1**。

假设数据集为：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/20170417171702256.png)

对样本首先计算 Lp 范数，得到：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/20170417171715565.png)

正则化后的结果是：每个属性值除以其 Lp 范数

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/20170417171731785.png)

2. 正则化的过程是针对**单个样本**的，对每个样本将它缩放到单位范数。

   归一化是针对**单个属性**的，需要用到所有样本在该属性上的值。

3. 通常如果使用**二次型（如点积）或者其他核方法计算两个样本之间的相似性**时，该方法会很有用。

### 特征编码

#### 序号编码(Ordinal Encoding)

**定义**：序号编码一般用于**处理类别间具有大小关系**的数据。

比如成绩，可以分为高、中、低三个档次，并且存在“高>中>低”的大小关系，那么序号编码可以对这三个档次进行如下编码：高表示为 3，中表示为 2，低表示为 1，这样转换后依然保留了大小关系。

#### 独热编码(One-hot Encoding)

**定义**：独热编码通常用于处理类别间不具有大小关系的特征。

独热编码是采用 **N** 位状态位来对 **N** 个可能的取值进行编码。比如血型，一共有 4 个取值（A、B、AB 以及 O 型），那么独热编码会将血型转换为一个 4 维稀疏向量，分别表示上述四种血型为：

- A型：(1,0,0,0)
- B型：(0,1,0,0)
- AB型：(0,0,1,0)
- O型：(0,0,0,1)

独热编码的优点有以下几个：

- 能够处理**非数值属性**。比如血型、性别等
- 一定程度上扩充了特征。
- 编码后的向量是稀疏向量，只有一位是 1，其他都是 0，可以利用向量的稀疏来**节省存储空间**。
- **能够处理缺失值**。当所有位都是 0，表示发生了缺失。此时可以采用处理缺失值提到的**高维映射**方法，用第 **N+1** 位来表示缺失值。

当然，独热编码也存在一些缺点：

1.高维度特征会带来以下几个方面问题：

- KNN 算法中，**高维空间下两点之间的距离很难得到有效的衡量**；
- 逻辑回归模型中，参数的数量会随着维度的增高而增加，导致**模型复杂，出现过拟合问题**；
- 通常只有部分维度是对分类、预测有帮助，需要**借助特征选择来降低维度**。

2.决策树模型不推荐对离散特征进行独热编码，有以下两个主要原因：

- **产生样本切分不平衡问题，此时切分增益会非常小**。

  比如对血型做独热编码操作，那么对每个特征`是否 A 型、是否 B 型、是否 AB 型、是否 O 型`，会有少量样本是 1 ，大量样本是 0。

  这种划分的增益非常小，因为拆分之后：

	- 较小的那个拆分样本集，它占总样本的比例太小。无论增益多大，乘以该比例之后几乎可以忽略。

	- 较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零。

- **影响决策树的学习**。

    决策树依赖的是**数据的统计信息**。而独热码编码会把数据切分到零散的小空间上。在这些零散的小空间上，统计信息是不准确的，学习效果变差。

    本质是因为**独热编码之后的特征的表达能力较差**。该特征的预测能力被人为的拆分成多份，每一份与其他特征竞争最优划分点都失败。最终该特征得到的重要性会比实际值低。

#### 二进制编码(Binary Encoding)

二进制编码主要分为两步：

1. 先采用序号编码给每个类别赋予一个类别 ID；
2. 接着将类别 ID 对应的二进制编码作为结果。

继续以血型为例子，如下表所示：

| 血型 | 类别 ID | 二进制表示 | 独热编码 |
| :--: | :-----: | :--------: | :------: |
|  A   |    1    |   0 0 1    | 1 0 0 0  |
|  B   |    2    |   0 1 0    | 0 1 0 0  |
|  AB  |    3    |   0 1 1    | 0 0 1 0  |
|  O   |    4    |   1 0 0    | 0 0 0 1  |

从上表可以知道，二进制编码本质上是利用**二进制对类别 ID 进行哈希映射，最终得到 0/1 特征向量，并且特征维度小于独热编码，更加节省存储空间**。

#### 二元化

**定义**：特征二元化就是将数值型的属性转换为布尔型的属性。通常用于假设属性取值分布是伯努利分布的情形。

特征二元化的算法比较简单。对属性 `j` 指定一个阈值 `m`。

- 如果样本在属性 `j` 上的值大于等于 `m`, 则二元化后为 1；
- 如果样本在属性 `j` 上的值小于 `m`，则二元化为 0

根据上述定义，`m` 是一个关键的超参数，它的取值需要结合模型和具体的任务来选择。

#### 离散化

**定义**：顾名思义，离散化就是将连续的数值属性转换为离散的数值属性。

那么什么时候需要采用特征离散化呢？

这背后就是需要采用“**海量离散特征+简单模型**”，还是“**少量连续特征+复杂模型**”的做法了。

- 对于线性模型，通常使用“海量离散特征+简单模型”。
  - 优点：模型简单
  - 缺点：特征工程比较困难，但一旦有成功的经验就可以推广，并且可以很多人并行研究。
- 对于非线性模型（比如深度学习），通常使用“少量连续特征+复杂模型”。
  - 优点：不需要复杂的特征工程
  - 缺点：模型复杂

**分桶**

1.离散化的常用方法是**分桶**：

- 将所有样本在连续的数值属性 `j` 的取值从小到大排列 ${a_0, a_1, ..., a_N}$ 。
- 然后从小到大依次选择分桶边界$b_1, b_2, ..., b_M$  。其中：
  - `M` 为分桶的数量，它是一个超参数，需要人工指定。
  - 每个桶的大小$b_{k+1}-b_k$  也是一个超参数，需要人工指定。
- 给定属性 `j` 的取值$a_i$，对其进行分桶：
  - 如果$a_i < b_1$，则分桶编号是 0。分桶后的属性的取值为 0；
  - 如果$b_k \le a_i \le b_{k+1}$，则分桶编号是 `k`。分桶后的属性取值是 `k`；
  - 如果 $a_i \ge b_M$, 则分桶编号是 `M`。分桶后的属性取值是 `M`。

2.分桶的数量和边界通常需要人工指定。一般有两种方法：

- **根据业务领域的经验来指定**。如：对年收入进行分桶时，根据 2017 年全国居民人均可支配收入约为 2.6 万元，可以选择桶的数量为5。其中：
    - 收入小于 1.3 万元（人均的 0.5 倍），则为分桶 0 。
    - 年收入在 1.3 万元 ～5.2 万元（人均的 0.5～2 倍），则为分桶 1 。
    - 年收入在 5.3 万元～26 万元（人均的 2 倍～10 倍），则为分桶 2 。
    - 年收入在 26 万元～260 万元（人均的 10 倍～100 倍），则为分桶 3 。
    - 年收入超过 260 万元，则为分桶 4 。
- **根据模型指定**。根据具体任务来训练分桶之后的数据集，通过超参数搜索来确定最优的分桶数量和分桶边界。

3.选择分桶大小时，有一些经验指导：

- **分桶大小必须足够小**，使得桶内的属性取值变化对样本标记的影响基本在一个不大的范围。

  即不能出现这样的情况：单个分桶的内部，样本标记输出变化很大。

- **分桶大小必须足够大，使每个桶内都有足够的样本**。

  如果桶内样本太少，则随机性太大，不具有统计意义上的说服力。

- 每个桶内的样本尽量**分布均匀**。

**特性**

1.在工业界很少直接将连续值作为逻辑回归模型的特征输入，而是**将连续特征离散化为一系列 0/1 的离散特征**。

其优势有：

- 离散化之后得到的稀疏向量，**内积乘法运算速度更快，计算结果方便存储**。

- 离散化之后的特征对于**异常数据具有很强的鲁棒性**。

  如：销售额作为特征，当销售额在 `[30,100)` 之间时，为1，否则为 0。如果未离散化，则一个异常值 10000 会给模型造成很大的干扰。由于其数值较大，它对权重的学习影响较大。

- 逻辑回归属于广义线性模型，表达能力受限，只能描述线性关系。特征离散化之后，相当于**引入了非线性，提升模型的表达能力，增强拟合能力**。

  假设某个连续特征 `j`  ，它离散化为 `M` 个 0/1 特征 $j_1, j_2, ..., j_M$  。则：$w_j * x_j -> w_{j1} * x_{j1}^` + w_{j2} * x_{j2}^` + ...+w_{jM} * x_{jM}^` $。其中 $x_{j1}^`，x_{j2}^`，..., x_{jM}^`$ 是离散化之后的新的特征，它们的取值空间都是  {0, 1}。

  上式右侧是一个分段线性映射，其表达能力更强。

- **离散化之后可以进行特征交叉**。假设有连续特征`j` ，离散化为 `N` 个 0/1 特征；连续特征 `k`，离散化为 `M` 个 0/1 特征，则分别进行离散化之后引入了 `N+M` 个特征。

  假设离散化时，并不是独立进行离散化，而是特征 `j,k`  联合进行离散化，则可以得到  `N*M` 个组合特征。**这会进一步引入非线性，提高模型表达能力**。

- **离散化之后，模型会更稳定**。

  如对销售额进行离散化，`[30,100)` 作为一个区间。当销售额在40左右浮动时，并不会影响它离散化后的特征的值。

  但是**处于区间连接处的值要小心处理，另外如何划分区间也是需要仔细处理**。

2.**特征离散化简化了逻辑回归模型，同时降低模型过拟合的风险**。

能够对抗过拟合的原因：**经过特征离散化之后，模型不再拟合特征的具体值，而是拟合特征的某个概念**。因此能够**对抗数据的扰动，更具有鲁棒性**。

另外它使得模型要**拟合的值大幅度降低，也降低了模型的复杂度**。

### 特征选择

**定义**：从给定的特征集合中选出相关特征子集的过程称为特征选择(feature selection)。

1.对于一个学习任务，给定了属性集，其中某些属性可能对于学习来说很关键，但有些属性意义就不大。

- 对当前学习任务有用的属性或者特征，称为**相关特征**(relevant feature)；
- 对当前学习任务没用的属性或者特征，称为**无关特征**(irrelevant feature)。

2.特征选择可能会降低模型的预测能力，因为被剔除的特征中可能包含了有效的信息，抛弃这部分信息一定程度上会降低模型的性能。但这也是计算复杂度和模型性能之间的取舍：

- 如果保留尽可能多的特征，模型的性能会提升，但同时模型就变复杂，计算复杂度也同样提升；
- 如果剔除尽可能多的特征，模型的性能会有所下降，但模型就变简单，也就降低计算复杂度。

3.常见的特征选择分为三类方法：

- 过滤式(filter)
- 包裹式(wrapper)
- 嵌入式(embedding)

#### 特征选择原理

1.采用特征选择的原因：

- **维数灾难问题**。因为属性或者特征过多造成的问题，如果可以选择重要的特征，使得仅需要一部分特征就可以构建模型，可以大大减轻维数灾难问题，从这个意义上讲，特征选择和降维技术有相似的动机，事实上它们也是处理高维数据的两大主流技术。
- **去除无关特征可以降低学习任务的难度，也同样让模型变得简单，降低计算复杂度**。

2.特征选择最重要的是**确保不丢失重要的特征**，否则就会因为缺少重要的信息而无法得到一个性能很好的模型。

- 给定数据集，学习任务不同，相关的特征很可能也不相同，因此特征选择中的**不相关特征指的是与当前学习任务无关的特征**。
- 有一类特征称作**冗余特征(redundant feature)**，它们所包含的信息可以从其他特征中推演出来。
  - 冗余特征通常都不起作用，去除它们可以减轻模型训练的负担；
  - 但如果冗余特征恰好对应了完成学习任务所需要的某个中间概念，则它是有益的，可以降低学习任务的难度。

3.在没有任何先验知识，即领域知识的前提下，要想从初始特征集合中选择一个包含所有重要信息的特征子集，唯一做法就是遍历所有可能的特征组合。

但这种做法并不实际，也不可行，因为会遭遇组合爆炸，特征数量稍多就无法进行。

一个可选的方案是：

- 产生一个候选子集，评价出它的好坏。
- 基于评价结果产生下一个候选子集，再评价其好坏。
- 这个过程持续进行下去，直至无法找到更好的后续子集为止。

这里有两个问题：如何根据评价结果获取下一个候选特征子集？如何评价候选特征子集的好坏？

##### 子集搜索

1.**子集搜索**方法步骤如下：

- 给定特征集合$A={A_1,A_2,...,A_d}$  ，首先将每个特征看作一个候选子集（即每个子集中只有一个元素），然后对这 $d$ 个候选子集进行评价。

  假设 $A_2$ 最优，于是将  $A_2$  作为第一轮的选定子集。

- 然后在上一轮的选定子集中加入一个特征，构成了包含两个特征的候选子集。

  假定 $A_2$,A_5   最优，且优于  $A_2$  ，于是将  $A_2,A_5$  作为第二轮的选定子集。

- ....

- 假定在第 `k+1` 轮时，**本轮的最优的特征子集不如上一轮的最优的特征子集**，则停止生成候选子集，并将上一轮选定的特征子集作为特征选择的结果。

2.这种逐渐增加相关特征的策略称作**前向 `forward`搜索**

类似地，如果从完整的特征集合开始，每次尝试去掉一个无关特征，这种逐渐减小特征的策略称作**后向`backward`搜索**

3.也可以将前向和后向搜索结合起来，每一轮逐渐增加选定的相关特征（这些特征在后续迭代中确定不会被去除），同时减少无关特征，这样的策略被称作是**双向`bidirectional`搜索**。

4该策略是贪心的，因为它们仅仅考虑了使本轮选定集最优。但是除非进行穷举搜索，否则这样的问题无法避免。

##### 子集评价

1.子集评价的做法如下：

给定数据集 D，假设所有属性均为离散型。对属性子集 A，假定根据其取值将 D 分成了 V 个子集：${D_1, D_2, \cdots,  D_V}$

可以计算属性子集 A 的信息增益：
$$
g(D, A) = H(D) - H(D|A)=H(D)-\sum^V_{v=1}\frac{|D_v|}{|D|}H(D_v)
$$
其中，$|•|$表示集合大小，$H(•)$表示熵。

**信息增益越大，表明特征子集 A 包含的有助于分类的信息越多**。所以对于每个候选特征子集，可以基于训练集 D 来计算其信息增益作为评价准则。

2.更一般地，特征子集 A 实际上确定了对数据集 D 的一个划分规则。

- 每个划分区域对应着 A 上的一个取值，而样本标记信息 y 则对应着 D 的真实划分。
- 通过估算这两种划分之间的差异，就能对 A 进行评价：与 y 对应的划分的差异越小，则说明 A 越好。
- **信息熵仅仅是判断这个差异的一种方法，其他能判断这两个划分差异的机制都能够用于特征子集的评价**。

3.**将特征子集搜索机制与子集评价机制结合就能得到特征选择方法**。

- 事实上，决策树可以用于特征选择，所有树结点的划分属性所组成的集合就是选择出来的特征子集。
- 其他特征选择方法本质上都是显式或者隐式地结合了某些子集搜索机制和子集评价机制。

4.常见的特征选择方法分为以下三种，主要区别在于特征选择部分是否使用后续的学习器。

- **过滤式**(filter)：先对数据集进行特征选择，其过程与后续学习器无关，即设计一些统计量来过滤特征，并不考虑后续学习器问题
- **包裹式**(wrapper)：实际上就是一个分类器，它是将后续的学习器的性能作为特征子集的评价标准。
- **嵌入式**(embedding)：实际上是学习器自主选择特征。

5.最简单的特征选择方法是：**去掉取值变化小的特征**。

假如某特征只有 0 和 1 的两种取值，并且所有输入样本中，95% 的样本的该特征取值都是 1 ，那就可以认为该特征作用不大。

当然，该方法的一个前提是，特征值都是**离散型**才使用该方法；如果是连续型，需要离散化后再使用，并且实际上一般不会出现 95% 以上都取某个值的特征的存在。

所以，这个方法简单，但不太好用，可以作为特征选择的一个预处理，先去掉变化小的特征，然后再开始选择上述三种类型的特征选择方法。

#### 过滤式选择

该方法**先对数据集进行特征选择，然后再训练学习器**。特征选择过程与后续学习器无关。

也就是先采用特征选择对初始特征进行过滤，然后用过滤后的特征训练模型。

- 优点是**计算时间上比较高效，而且对过拟合问题有较高的鲁棒性**；
- 缺点是**倾向于选择冗余特征**，即没有考虑到特征之间的相关性。

##### Relief 方法

1.`Relief:Relevant Features`是一种著名的过滤式特征选择方法。该方法设计了一个**相关统计量来度量特征的重要性**。

- 该统计量是一个向量，其中每个分量都对应于一个初始特征。特征子集的重要性则是由**该子集中每个特征所对应的相关统计量分量之和来决定的**。

- 最终只需要指定一个阈值 $\gamma$，然后**选择比 $\gamma$ 大的相关统计量分量所对应的特征**即可。

  也可以**指定特征个数 m** ，然后选择相关统计量分量最大的 m 个特征。

2.给定训练集$D={(\vec{x_1}, \breve{y_1}), \cdots, (\vec{x_N}, \breve{y_N})}, \breve{y_i}\in{0, 1}$。对于每个样本$\vec{x_i}$：

- `Relief` 先在$\vec{x_i}$同类样本中寻找其最近邻 $\vec{x_{nm_i}}$，称作猜中近邻 `near-hit` ；
- 然后从$\vec{x_i}$ 的异类样本寻找最近邻$\vec{x_{nm_i}}$，称作猜错近邻`near-miss`。
- 然后相关统计量对应于属性`j`的分量为：

$$
\delta_j = \sum^N_{i=1}(-diff(x_{i,j}, x_{nh_i,j})^2 + diff(x_{i,j},x_{nm_i,j})^2)
$$

其中$diff(x_{a,j},x_{b,j})$是两个样本在属性 `j` 上的差异值，其结果取决于该属性是离散的还是连续的：

- 如果是离散的，则有

$$
    diff(x{a,j},x{b,j})=
    \begin{cases}
    0,\quad if\; x_{a,j}=x_{b,j}\\
    1, \quad else
    \end{cases}
$$

- 如果是连续的，则有

$$
diff(x{a,j},x{b,j})=|x_{a,j} - x_{b,j}|
$$

     注意，此时需要对$x_{a,j},x_{b,j}$进行标准化到`[0,1]`区间。

3.根据 3 的公式可以知道，如果

- 如果 $\vec{x_i}$ 与其猜中近邻  $\vec{x_{nh_i}}$ 在属性 `j` 上的距离小于 $\vec{x_i}$ 与其猜错近邻 $\vec{x_{nm_i}}$ 的距离，则说明属性 `j` 对于区分同类与异类样本是有益的，于是增大属性 `j` 所对应的统计量分量。
- 如果 $\vec{x_i}$ 与其猜中近邻  $\vec{x_{nh_i}}$ 在属性 `j` 上的距离大于$\vec{x_i}$  与其猜错近邻 $\vec{x_{nm_i}}$ 的距离，则说明属性 `j` 对于区分同类与异类样本是起负作用的，于是减小属性 `j` 所对应的统计量分量。
- 最后对基于不同样本得到的估计结果进行平均，就得到各属性的相关统计量分量。**分量值越大，则对应属性的分类能力越强**。

4.`Relief` 是为二分类问题设计的，其拓展变体 `Relief-F` 可以处理多分类问题。

假定数据集 D 中的样本类别为：${c_1, c_2, \cdots, c_K}$  。对于样本 $\vec{x_i}$ ，假设 $\breve{y_i}=c_k$ 。

- `Relief-F` 先在类别 $c_k$ 的样本中寻找 $\vec{x_i}$ 的最近邻 $\vec{x_{nh_i}}$ 作为猜中近邻。
- 然后在 $c_k$  之外的每个类别中分别找到一个 $\vec{x_i}$的最近邻 $\vec{x_{nm_i^l}}, l=1,2,\cdots,K; l \neq k$$   作为猜错近邻。
- 于是相关统计量对应于属性  j 的分量为：

$$
\delta_j = \sum^N_{i=1}(-diff(x_{i,j},x_{nh_{i,j}})^2+\sum_{l\neq k}(p_l\times diff(x_{i,j},x_{nm_{i,j}^l})^2))
$$

     其中 $p_l$ 作为第 $l$ 类的样本在数据集 D 中所占的比例。

##### 方差选择法

使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用 sklearn 的 feature_selection 库的 VarianceThreshold 类来选择特征的代码如下：

```python
from sklearn.feature_selection import VarianceThreshold

#方差选择法，返回值为特征选择后的数据
#参数threshold为方差的阈值
VarianceThreshold(threshold=3).fit_transform(data)
```

##### 相关系数法

使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的 P 值。用 feature_selection 库的 SelectKBest 类结合相关系数来选择特征的代码如下：

```python
from sklearn.feature_selection import SelectKBest
from scipy.stats import pearsonr

#选择 K 个最好的特征，返回选择特征后的数据
#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数
#参数k为选择的特征个数
SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
```



##### 卡方检验

经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有 N 种取值，因变量有 M 种取值，考虑自变量等于 i 且因变量等于 j 的样本频数的观察值与期望的差距，构建统计量：
$$
X^2 = \sum\frac{(A-E)^2}{E}
$$
不难发现，这个统计量的含义简而言之就是自变量对因变量的相关性。用 feature_selection 库的 SelectKBest 类结合卡方检验来选择特征的代码如下：

```python
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

#选择K个最好的特征，返回选择特征后的数据
SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)
```

##### 互信息法

经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：
$$
I(X;Y)=\sum_{x\in X}\sum_{y\in Y}p(x,y)log\frac{p(x,y)}{p(x)p(y)}
$$
为了处理定量数据，**最大信息系数法**被提出，使用 feature_selection 库的 SelectKBest 类结合最大信息系数法来选择特征的代码如下

```python
from sklearn.feature_selection import SelectKBest
 from minepy import MINE
 
 #由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5
 def mic(x, y):
     m = MINE()
     m.compute_score(x, y)
     return (m.mic(), 0.5)

#选择K个最好的特征，返回特征选择后的数据
SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
```



#### 包裹式选择

1. 相比于过滤式特征选择不考虑后续学习器，包裹式特征选择**直接把最终将要使用的学习器的性能作为特征子集的评价原则**。其目的就是为给定学习器选择最有利于其性能、量身定做的特征子集。

- 优点是直接针对特定学习器进行优化，考虑到特征之间的关联性，因此**通常包裹式特征选择比过滤式特征选择能训练得到一个更好性能的学习器**，
- 缺点是由于特征选择过程需要多次训练学习器，故计算**开销要比过滤式特征选择要大得多**。

2. LVW:Las Vegas Wrapper`是一个典型的包裹式特征选择方法。它是` Las Vegas method`   框架下使用随机策略来进行子集搜索，并以最终分类器的误差作为特征子集的评价标准。

3. `LVW` 算法：

- 输入：数据集 D，特征集 A，学习器 estimator, 迭代停止条件 T

- 输出：最优特征子集 $A^*$

- 算法步骤：

  - 初始化：令候选的最优特征子集$\breve{A^*}=A$, 然后学习器 estimator 在特征子集 $\breve{A^*}$上使用交叉验证法进行学习，通过学习结果评估学习器的误差 $err^*$。
  - 迭代，停止条件为迭代次数达到 T。迭代过程是：
    - 随机产生特征子集 $A^\prime$
    - 学习器在特征子集 $A^\prime$ 上使用交叉验证法进行学习，通过学习评估学习器的误差 $err^\prime$
    - 如果 $err^\prime$ 小于 $err^*$，或者$err^\prime = err^*$，但是特征子集 $A^\prime$ 的特征数量少于候选的最优特征子集$\breve{A^*}$，则令$A^\prime$ 为候选的最优特征子集，即 $\breve{A^*} = A^\prime; err^* = err^\prime$

  - 最终有 $A^* = \breve{A^*}$

4. 由于 `LVW` 算法中**每次特征子集评价都需要训练学习器，计算开销很大**，因此算法设计了停止条件控制参数 $T$。

但是如果初始特征数量很多、$T$ 设置较大、以及每一轮训练的时间较长， 则很可能算法运行很长时间都不会停止。即：**如果有运行时间限制，则有可能给不出解**。

5. **递归特征消除法**：使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用 feature_selection 库的 RFE 类来选择特征的代码如下：

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

#递归特征消除法，返回特征选择后的数据
#参数estimator为基模型
#参数n_features_to_select为选择的特征个数
RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target)
```



#### 嵌入式选择

1. 在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别。

嵌入式特征选择是将特征选择与学习器训练过程融为一体，两者在同一个优化过程中完成的。**即学习器训练过程中自动进行了特征选择**。

常用的方法包括：

- 利用正则化，如`L_1, L_2` 范数，主要应用于如线性回归、逻辑回归以及支持向量机(SVM)等算法；
- 使用决策树思想，包括决策树、随机森林、Gradient Boosting 等。

2. 以线性回归模型为例。给定数据集 $D={(\vec{x_1}, \breve{y_1}), \cdots, (\vec{x_N}, \breve{y_N})}, \breve{y_i}\in R$。以平方差为损失函数，则优化目标是 $min_{\vec{w}} \sum^N_{i=1}(\breve{y_i}-\vec{w^T}\vec{x_i})^2$

- 如果使用 $L_2$ 范数正则化，优化目标就是 $min_{\vec{w}} \sum^N_{i=1}(\breve{y_i}-\vec{w^T}\vec{x_i})^2+ \lambda|| \vec{w}||^2_2 , \lambda > 0$，此时称作**岭回归**(ridge regression)。 
- 如果使用 $L_1$ 范数正则化，优化目标就是 $min_{\vec{w}} \sum^N_{i=1}(\breve{y_i}-\vec{w^T}\vec{x_i})^2+ \lambda|| \vec{w}||_1 , \lambda > 0$，此时称作**LASSO(Least Absolute Shrinkage and Selection Operator)回归**。 

3. 引入 $L_1$ 范数除了**降低过拟合风险**之外，还有一个好处：它求得的 $\vec{w}$ 会有较多的分量为零。即：**它更容易获得稀疏解**。

于是基于 $L_1$ 正则化的学习方法就是一种嵌入式特征选择方法，其特征选择过程与学习器训练过程融为一体，二者同时完成。

4. 常见的嵌入式选择模型：

- 在 `Lasso` 中，$\lambda$ 参数控制了稀疏性：
  - 如果 $\lambda$ 越小，则稀疏性越小，被选择的特征越多；
  - 相反，$\lambda$ 越大，则稀疏性越大，被选择的特征越少；
- 在 `SVM` 和 逻辑回归中，参数 `C` 控制了稀疏性：
  - 如果 `C` 越小，则稀疏性越大，被选择的特征越少；
  - 如果 `C` 越大， 则稀疏性越小，被选择的特征越多。



### 特征提取

特征提取一般是在特征选择之前，它提取的对象是**原始数据**，目的就是自动地构建新的特征，**将原始数据转换为一组具有明显物理意义（比如 Gabor、几何特征、纹理特征）或者统计意义的特征**。

一般常用的方法包括降维（PCA、ICA、LDA等）、图像方面的SIFT、Gabor、HOG等、文本方面的词袋模型、词嵌入模型等，这里简单介绍这几种方法的一些基本概念。

#### 降维

1.**PCA**(Principal Component Analysis，主成分分析)

PCA 是降维最经典的方法，它旨在是找到**数据中的主成分，并利用这些主成分来表征原始数据，从而达到降维的目的**。

PCA 的思想是通过坐标轴转换，寻找数据分布的最优子空间。

比如，在三维空间中有一系列数据点，它们分布在过原点的平面上，如果采用自然坐标系的 x，y，z 三个轴表示数据，需要三个维度，但实际上这些数据点都在同一个二维平面上，如果我们可以通过坐标轴转换使得数据所在平面和 x，y 平面重合，我们就可以通过新的 x'、y' 轴来表示原始数据，并且没有任何损失，这就完成了降维的目的，而且这两个新的轴就是我们需要找的主成分。

因此，PCA 的解法一般分为以下几个步骤：

1. 对样本数据进行中心化处理；
2. 求样本协方差矩阵；
3. 对协方差矩阵进行特征值分解，将特征值从大到小排列；
4. 取特征值前 n 个最大的对应的特征向量 `W1, W2, ..., Wn` ，这样将原来 m 维的样本降低到 n 维。

通过 PCA ，就可以将方差较小的特征给抛弃，这里，特征向量可以理解为坐标转换中新坐标轴的方向，特征值表示在对应特征向量上的方差，**特征值越大，方差越大，信息量也就越大**。这也是为什么选择前 n 个最大的特征值对应的特征向量，因为这些特征包含更多重要的信息。

**PCA 是一种线性降维方法，这也是它的一个局限性**。不过也有很多解决方法，比如采用核映射对 PCA 进行拓展得到核主成分分析(KPCA)，或者是采用流形映射的降维方法，比如等距映射、局部线性嵌入、拉普拉斯特征映射等，对一些 PCA 效果不好的复杂数据集进行非线性降维操作。

2.**LDA**(Linear Discriminant Analysis，线性判别分析)

LDA 是一种有监督学习算法，相比较 PCA，它考虑到数据的类别信息，而 PCA 没有考虑，只是将数据映射到方差比较大的方向上而已。

因为考虑数据类别信息，所以 LDA 的目的不仅仅是降维，还需要找到一个投影方向，使得投影后的样本尽可能按照原始类别分开，即寻找一个**可以最大化类间距离以及最小化类内距离**的方向。

LDA 的主要步骤如下：

1. 分别计算每个类别 i 的原始中心点：$m_i = \frac{1}{n_i}\sum{x}$
2. 类别 i 投影后的中心点为: $m_i = w^Tm_i$
3. 衡量类别 i 投影后，类间的分散程度，用方差来表示：$s_i = \sum{(y-m_i)^2}$
4. 使用下面的式子表示 LDA 投影到 w 后的损失函数，最大化 J(w) 可以求出最优的 w , $J(w) = \frac{|m_1-m_2|^2}{s_1^2 + s_2^2}$，具体的解法参考[机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)](https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html)。

LDA 的优点如下：

- 相比较 PCA，LDA 更加擅长处理带有类别信息的数据；
- 线性模型对噪声的鲁棒性比较好，LDA 是一种有效的降维方法。

相应的，也有如下缺点：

- LDA 对**数据的分布做出了很强的假设**，比如每个类别数据都是高斯分布、各个类的协方差相等。这些假设在实际中不一定完全满足。
- **LDA 模型简单，表达能力有一定局限性**。但这可以通过引入**核函数**拓展 LDA 来处理分布比较复杂的数据。

3.**ICA**(Independent Component Analysis，独立成分分析)

PCA特征转换降维，提取的是**不相关**的部分，ICA独立成分分析，获得的是**相互独立**的属性。ICA算法本质寻找一个线性变换 `z = Wx`，使得 z 的**各个特征分量之间的独立性最大**。

通常先采用 PCA 对数据进行降维，然后再用 ICA 来从多个维度分离出有用数据。PCA 是 ICA 的数据预处理方法。

具体可以查看知乎上的这个问题和回答 [独立成分分析 ( ICA ) 与主成分分析 ( PCA ) 的区别在哪里？](https://www.zhihu.com/question/28845451)。

#### 图像特征提取

图像的特征提取，在深度学习火起来之前，是有很多传统的特征提取方法，比较常见的包括以下几种。

1.**SIFT** 特征

SIFT 是图像特征提取中非常广泛应用的特征。它包含以下几种优点：

- 具有旋转、尺度、平移、视角及亮度不变性，有利于对目标特征信息进行有效表达；
- SIFT 特征对参数调整鲁棒性好，可以根据场景需要调整适宜的特征点数量进行特征描述，以便进行特征分析。

SIFT 对图像局部特征点的提取主要包括四个步骤：

1. 疑似特征点检测
2. 去除伪特征点
3. 特征点梯度与方向匹配
4. 特征描述向量的生成

SIFT 的缺点是不借助硬件加速或者专门的图像处理器很难实现。

2.**SURF** 特征

SURF 特征是对 SIFT 算法的改进，降低了时间复杂度，并且提高了鲁棒性。

它主要是简化了 SIFT 的一些运算，如将 SIFT 中的高斯二阶微分的模型进行了简化，使得卷积平滑操作仅需要转换成加减运算。并且最终生成的特征向量维度从 128 维减少为 64 维。

3.**HOG** 特征

方向梯度直方图(HOG)特征是 2005 年针对行人检测问题提出的直方图特征，它通过计算和统计图像局部区域的梯度方向直方图来实现特征描述。

HOG 特征提取步骤如下：

1. **归一化处理**。先将图像转为灰度图像，再利用伽马校正实现。这一步骤是为了提高图像特征描述对光照及环境变化的鲁棒性，降低图像局部的阴影、局部曝光过多和纹理失真，尽可能抵制噪声干扰；
2. **计算图像梯度**；
3. **统计梯度方向**；
4. **特征向量归一化**；为克服光照不均匀变化及前景与背景的对比差异，需要对块内的特征向量进行归一化处理。
5. **生成特征向量**。



4.**LBP** 特征

局部二值模式（LBP）是一种描述图像局部纹理的特征算子，它具有旋转不变性和灰度不变性的优点。

LBP 特征描述的是一种灰度范围内的图像处理操作技术，针对的是**输入为 8 位或者 16 位的灰度图像**。

LBP 特征通过对**窗口中心点与邻域点的关系进行比较**，重新编码形成新特征以消除对外界场景对图像的影响，因此一定程度上解决了**复杂场景下（光照变换）特征**描述问题。

根据窗口领域的不同分为两种，经典 LBP 和圆形 LBP。前者的窗口是 3×3 的正方形窗口，后者将窗口从正方形拓展为任意圆形领域。

更详细的可以参考这篇文章--[图像特征检测描述(一):SIFT、SURF、ORB、HOG、LBP特征的原理概述及OpenCV代码实现](https://blog.csdn.net/wenhao_ir/article/details/52046569)

当然上述特征都是比较传统的图像特征提取方法了，现在图像基本都直接利用 CNN（卷积神经网络）来进行特征提取以及分类。

#### 文本特征提取

1.**词袋模型**

最基础的文本表示模型是词袋模型。

具体地说，就是将整段文本以词为单位切分开，然后每篇文章可以表示成一个长向量，向量的每一个维度代表一个单词，而该维度的权重反映了该单词在原来文章中的重要程度。

通常采用 **TF-IDF** 计算权重，公式为 `TF-IDF(t, d) = TF(t,d) × IDF(t)`

其中 TF(t, d) 表示单词 t 在文档 d 中出现的频率，IDF(t) 是逆文档频率，用来衡量单词 t 对表达语义所起的重要性，其表示为：
$$
IDF(t)=log\frac{文章总数}{包含单词 t 的文章总数+1}
$$
直观的解释就是，如果这个单词在多篇文章都出现过，那么它很可能是比较通用的词汇，对于区分文章的贡献比较小，自然其权重也就比较小，即 IDF(t) 会比较小。

2.**N-gram 模型**

词袋模型是以单词为单位进行划分，但有时候进行单词级别划分并不是很好的做法，毕竟有的单词组合起来才是其要表达的含义，比如说 `natural language processing(自然语言处理)`、`computer vision(计算机视觉)` 等。

因此可以将连续出现的 n 个词 (n <= N) 组成的词组(N-gram)作为一个单独的特征放到向量表示中，构成了 N-gram 模型。

另外，同一个词可能会有多种词性变化，但却具有相同含义，所以实际应用中还会对单词进行**词干抽取**(Word Stemming)处理，即将不同词性的单词统一为同一词干的形式。

3.**词嵌入模型**

词嵌入是一类将词向量化的模型的统称，核心思想是将**每个词都映射成低维空间（通常 K=50~300 维）上的一个稠密向量（Dense Vector）**。

常用的词嵌入模型是 **Word2Vec**。它是一种底层的神经网络模型，有两种网络结构，分别是 CBOW(Continues Bag of Words) 和 Skip-gram。

CBOW 是根据**上下文出现的词语预测当前词**的生成概率；Skip-gram 是根据**当前词来预测上下文中各个词的生成概率**。

词嵌入模型是将每个词都映射成一个 K 维的向量，如果一篇文档有 N 个单词，那么每篇文档就可以用一个 N×K 的矩阵进行表示，但这种表示过于底层。实际应用中，如果直接将该矩阵作为原文本的特征表示输入到模型中训练，通常很难得到满意的结果，一般还需要对该矩阵进行处理，提取和构造更高层的特征。

深度学习模型的出现正好提供了一种自动进行特征工程的方法，它的每个隐含层都相当于不同抽象层次的特征。卷积神经网络（CNN)和循环神经网络（RNN)在文本表示中都取得了很好的效果，这是因为它们可以很好地对文本进行建模，抽取出一些高层的语义特征。

#### 特征提取和特征选择的区别

特征提取与特征选择都是为了从原始特征中找出最有效的特征。

它们之间的区别是特征提取强调通过**特征转换**的方式得到一组具有明显物理或统计意义的特征；

而特征选择是从特征集合中挑选一组具有明显物理或统计意义的**特征子集**。

两者都能**帮助减少特征的维度、数据冗余**，特征提取有时能发现更有意义的特征属性，特征选择的过程经常能表示出每个特征的重要性对于模型构建的重要性。

### 特征构建

特征构建是指**从原始数据中人工的构建新的特征**。需要花时间去观察原始数据，思考问题的潜在形式和数据结构，对数据敏感性和机器学习实战经验能帮助特征构建。

特征构建需要很强的洞察力和分析能力，要求我们能够从原始数据中找出一些具有物理意义的特征。假设原始数据是表格数据，一般你可以使用**混合属性或者组合属性**来创建新的特征，或是**分解或切分原有的特征**来创建新的特征。

特征构建非常需要相关的领域知识或者丰富的实践经验才能很好构建出更好的有用的新特征，相比于特征提取，特征提取是通过一些现成的特征提取方法来将原始数据进行特征转换，而特征构建就需要我们自己人为的手工构建特征，比如组合两个特征，或者分解一个特征为多个新的特征。

------

### 参考

- 《hands-on-ml-with-sklearn-and-tf》第二节
- https://www.jiqizhixin.com/articles/091202
- https://blog.csdn.net/fendegao/article/details/79968994
- https://blog.csdn.net/xg123321123/article/details/80781611
- https://towardsdatascience.com/top-sources-for-machine-learning-datasets-bb6d0dc3378b
- 《百面机器学习》第一章 特征工程
- [机器学习之特征工程](https://blog.csdn.net/dream_angel_z/article/details/49388733#commentBox)
- [[数据预处理（方法总结）]](https://www.cnblogs.com/sherial/archive/2018/03/07/8522405.html)
- [Python数据分析（三）——数据预处理](https://gofisher.github.io/2018/06/22/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/)
- [Python数据分析（二）——数据探索](https://gofisher.github.io/2018/06/20/%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2/)
- [【Python数据分析基础】: 异常值检测和处理](https://juejin.im/post/5b6a44f55188251aa8294b8c)
- http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/8_feature_selection.html
- [Python机器学习-数据预处理技术（标准化处理、归一化、二值化、独热编码、标记编码）](https://blog.csdn.net/weixin_38168620/article/details/79233086)
- [[Scikit-learn介绍几种常用的特征选择方法](http://dataunion.org/14072.html)](http://dataunion.org/14072.html)
- [博客园--机器学习之特征工程](https://www.cnblogs.com/wxquare/p/5484636.html)
- [机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)](https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html)
- [独立成分分析 ( ICA ) 与主成分分析 ( PCA ) 的区别在哪里？](https://www.zhihu.com/question/28845451)
- [图像特征检测描述(一):SIFT、SURF、ORB、HOG、LBP特征的原理概述及OpenCV代码实现](https://blog.csdn.net/wenhao_ir/article/details/52046569)
- [Gabor特征提取](https://blog.csdn.net/xidianzhimeng/article/details/19493019)



## 模型评估

在机器学习领域中，对模型的评估非常重要，只有选择和问题相匹配的评估方法，才能快速发现算法模型或者训练过程的问题，迭代地对模型进行优化。

模型评估主要分为离线评估和在线评估两个阶段。并且针对分类、回归、排序、序列预测等不同类型的机器学习问题，评估指标的选择也有所不同。

模型评估这部分会介绍以下几方面的内容：

- 性能度量
- 模型评估方法
- 泛化能力
- 过拟合、欠拟合
- 超参数调优



### 1. 性能度量

性能度量就是指对模型泛化能力衡量的评价标准。

#### 1.1 准确率和错误率

分类问题中最常用的两个性能度量标准--准确率和错误率。

**准确率**：指的是分类正确的样本数量占样本总数的比例，定义如下：
$$
Accuracy = \frac{n_{correct}}{N}
$$
**错误率**：指分类错误的样本占样本总数的比例，定义如下：
$$
Error = \frac{n_{error}}{N}
$$
错误率也是损失函数为 0-1 损失时的误差。

这两种评价标准是分类问题中最简单也是最直观的评价指标。但它们都存在一个问题，在类别不平衡的情况下，它们都无法有效评价模型的泛化能力。即如果此时有 99% 的负样本，那么模型预测所有样本都是负样本的时候，可以得到 99% 的准确率。

这种情况就是**在类别不平衡的时候，占比大的类别往往成为影响准确率的最主要因素**！

这种时候，其中一种解决方法就是更换评价指标，比如采用更为有效的平均准确率(**每个类别的样本准确率的算术平均**)，即：
$$
A_{mean}=\frac{a_1+a_2+\dots+a_m}{m}
$$
其中 m 是类别的数量。

对于准确率和错误率，用 Python 代码实现如下图所示：

```python
def accuracy(y_true, y_pred):
    return sum(y == y_p for y, y_p in zip(y_true, y_pred)) / len(y_true)

def error(y_true, y_pred):
    return sum(y != y_p for y, y_p in zip(y_true, y_pred)) / len(y_true)
```

一个简单的二分类测试样例：

```python
y_true = [1, 0, 1, 0, 1]
y_pred = [0, 0, 1, 1, 0]

acc = accuracy(y_true, y_pred)
err = error(y_true, y_pred)
print('accuracy=', acc)
print('error=', err)
```

输出结果如下：

```
accuracy= 0.4
error= 0.6
```



#### 1.2 精确率、召回率、P-R 曲线和 F1

##### 1.2.1 精确率和召回率

精确率，也被称作查准率，是指**所有预测为正类的结果中，真正的正类的比例**。公式如下：
$$
P = \frac{TP}{TP+FP}
$$
召回率，也被称作查全率，是指所有正类中，被分类器找出来的比例。公式如下：
$$
R = \frac{TP}{TP+FN}
$$
对于上述两个公式的符号定义，是在二分类问题中，我们将关注的类别作为正类，其他类别作为负类别，因此，定义：

- `TP(True Positive)`：真正正类的数量，即分类为正类，实际也是正类的样本数量；
- `FP`(False Positive)：假正类的数量，即分类为正类，但实际是负类的样本数量；
- `FN(False Negative)`：假负类的数量，即分类为负类，但实际是正类的样本数量；
- `TN(True Negative)`：真负类的数量，即分类是负类，实际也负类的样本数量。

更形象的说明，可以参考下表，也是**混淆矩阵**的定义：

|            | 预测：正类 | 预测：负类 |
| :--------: | :--------: | :--------: |
| 实际：正类 |     TP     |     FN     |
| 实际：负类 |     FP     |     TN     |

精确率和召回率是一对矛盾的度量，通常精确率高时，召回率往往会比较低；而召回率高时，精确率则会比较低，原因如下：

- 精确率越高，代表预测为正类的比例更高，而要做到这点，通常就是**只选择有把握的样本**。最简单的就是只挑选最有把握的一个样本，此时 `FP=0`，`P=1`，但 `FN` 必然非常大(没把握的都判定为负类)，召回率就非常低了；
- 召回率要高，就是需要找到所有正类出来，要做到这点，最简单的就是**所有类别都判定为正类**，那么 `FN=0` ，但 `FP` 也很大，所有精确率就很低了。

而且不同的问题，侧重的评价指标也不同，比如：

- **对于推荐系统，侧重的是精确率**。也就是希望推荐的结果都是用户感兴趣的结果，即用户感兴趣的信息比例要高，因为通常给用户展示的窗口有限，一般只能展示 5 个，或者 10 个，所以更要求推荐给用户真正感兴趣的信息；
- **对于医学诊断系统，侧重的是召回率**。即希望不漏检任何疾病患者，如果漏检了，就可能耽搁患者治疗，导致病情恶化。

精确率和召回率的代码简单实现如下，这是基于二分类的情况

```python
def precision(y_true, y_pred):
    true_positive = sum(y and y_p for y, y_p in zip(y_true, y_pred))
    predicted_positive = sum(y_pred)
    return true_positive / predicted_positive
def recall(y_true, y_pred):
    true_positive = sum(y and y_p for y, y_p in zip(y_true, y_pred))
    real_positive = sum(y_true)
    return true_positive / real_positive
```

简单的测试样例以及输出如下

```python
y_true = [1, 0, 1, 0, 1]
y_pred = [0, 0, 1, 1, 0]

precisions = precision(y_true, y_pred)
recalls = recall(y_true, y_pred)

print('precisions=', precisions) # 输出为0.5
print('recalls=', recalls)       # 输出为 0.3333
```



##### 1.2.2 P-R 曲线和 F1

很多时候，我们都可以根据分类器的预测结果对样本进行排序，越靠前的是分类器越有把握是正类的样本，而最后面的自然就是分类器觉得最不可能是正类的样本了。

一般来说，这个预测结果其实就是分类器对样本判断为某个类别的置信度，我们可以选择不同的阈值来调整分类器对某个样本的输出结果，比如设置阈值是 0.9，那么只有置信度是大于等于 0.9 的样本才会最终判定为正类，其余的都是负类。

我们设置不同的阈值，自然就会得到不同的正类数量和负类数量，依次计算不同情况的精确率和召回率，然后我们可以**以精确率为纵轴，召回率为横轴，绘制一条“P-R曲线”**，如下图所示：

![来自西瓜书](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/P-R_plot.png)

当然，以上这个曲线是比较理想情况下的，未来绘图方便和美观，实际情况如下图所示：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/P_R.png)

对于 P-R 曲线，有：

1.**曲线从左上角 `(0,1)` 到右下角 `(1,0)` 的走势，正好反映了精确率和召回率是一对矛盾的度量**，一个高另一个低的特点：

- **开始是精确率高**，因为设置阈值很高，只有第一个样本（分类器最有把握是正类）被预测为正类，其他都是负类，所以精确率高，几乎是 1，而召回率几乎是 0，仅仅找到 1 个正类。
- **右下角时候就是召回率很高，精确率很低**，此时设置阈值就是 0，所以类别都被预测为正类，所有正类都被找到了，召回率很高，而精确率非常低，因为大量负类被预测为正类。

2.`P-R` 曲线可以非常直观显示出分类器在样本总体上的精确率和召回率。所以可以对比两个分类器在同个测试集上的 `P-R` 曲线来比较它们的分类能力：

- 如果分类器 `B` 的 `P-R` 曲线被分类器 `A` 的曲线完全包住，如下左图所示，则可以说，`A` 的性能优于 `B`;
- 如果是下面的右图，两者的曲线有交叉，则很难直接判断两个分类器的优劣，只能根据具体的精确率和召回率进行比较：
  - 一个合理的依据是**比较 `P-R` 曲线下方的面积大小**，它在一定程度上表征了分类器在精确率和召回率上取得“双高”的比例，但这个数值不容易计算；
  - 另一个比较就是**平衡点**(Break-Event Point, BEP)，它是**精确率等于召回率时的取值**，如下右图所示，而且可以判定，**平衡点较远的曲线更好**。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/P_R_AB.JPG)

当然了，平衡点还是过于简化，于是有了 **F1 值**这个新的评价标准，它是**精确率和召回率的调和平均值**，定义为：
$$
F1 = \frac{2 \times P \times R}{P+R}=\frac{2\times TP}{样本总数+TP-TN}
$$
F1 还有一个更一般的形式：$F_{\beta}$，能让我们表达出对精确率和召回率的不同偏好，定义如下：
$$
F_{\beta}=\frac{(1+\beta^2)\times P\times R}{(\beta^2 \times P)+R}
$$
其中$\beta > 0$ 度量了召回率对精确率的相对重要性，当 $\beta = 1$，就是 F1；如果 $\beta > 1$，召回率更加重要；如果 $\beta < 1$，则是精确率更加重要。

##### 1.2.3 宏精确率/微精确率、宏召回率/微召回率以及宏 F1 / 微 F1

很多时候，我们会得到不止一个二分类的混淆矩阵，比如多次训练/测试得到多个混淆矩阵，在多个数据集上进行训练/测试来估计算法的“全局”性能，或者是执行多分类任务时对类别两两组合得到多个混淆矩阵。

总之，我们希望在 n 个二分类混淆矩阵上综合考察精确率和召回率。这里一般有两种方法来进行考察：

1.第一种是直接在**各个混淆矩阵上分别计算出精确率和召回率**，记为 $(P_1, R_1), (P_2, R_2), \cdots, (P_n, R_n)$，接着**计算平均值**，就得到宏精确率(macro-P)、宏召回率(macro-R)以及宏 F1(macro-F1) , 定义如下：
$$
macro-P = \frac{1}{n}\sum_{i=1}^n P_i,\\
macro-R = \frac{1}{n}\sum_{i=1}^n R_i,\\
macro-F1 = \frac{2\times macro-P\times macro-R}{marco-P+macro-R}
$$
2.第二种则是**对每个混淆矩阵的对应元素进行平均**，**得到 TP、FP、TN、FN 的平均值**，再基于这些平均值就就得到微精确率(micro-P)、微召回率(micro-R)以及微 F1(micro-F1) , 定义如下：
$$
micro-P = \frac{\overline{TP}}{\overline{TP}+\overline{FP}},\\
micro-R = \frac{\overline{TP}}{\overline{TP}+\overline{FN}},\\
micro-F1 = \frac{2\times micro-P\times micro-R}{micro-P + micro-R}
$$

#### 1.3 ROC 与 AUC

##### 1.3.1 ROC 曲线

ROC 曲线的 Receiver Operating Characteristic 曲线的简称，中文名是“受试者工作特征”，起源于军事领域，后广泛应用于医学领域。

它的横坐标是**假正例率(False Positive Rate, FPR)**，纵坐标是**真正例率(True Positive Rate, TPR)**，两者的定义分别如下：
$$
TPR = \frac{TP}{TP+FN},\\
FPR = \frac{FP}{FP+TN}
$$
TPR 表示**正类中被分类器预测为正类的概率**，刚好就等于正类的召回率；

FPR 表示**负类中被分类器预测为正类的概率**，它等于 1 减去负类的召回率，负类的召回率如下，**称为真反例率(True Negative Rate, TNR)**, 也被称为特异性，表示负类被正确分类的比例。
$$
TNR =\frac{TN}{FP+TN}
$$

跟 P-R 曲线的绘制一样，ROC 曲线其实也是通过**不断调整区分正负类结果的阈值**来绘制得到的，它的纵轴是 TPR，横轴是 FPR，这里借鉴《百面机器学习》上的示例来介绍，首先有下图所示的表格，表格是一个二分类模型的输出结果样例，包含 20 个样本，然后有对应的真实标签，其中 p 表示是正类别，而 n 表示是负类别。然后模型输出概率表示模型对判断该样本是正类的置信度。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/roc_table.jpg)

最开始如果设置阈值是无穷大的时候，那么模型会将所有样本判断为负类，TP 和 FP 都会是 0，也就是 TPR 和 FPR 必然也是 0，ROC 曲线的第一个坐标就是 (0, 0)。接着，阈值设置为 0.9，此时样本序号为 1 的样本会被判断为正样本，并且它确实是正样本，那么 TP = 1，而正类样本的个数是有 10 个，所有 TPR = 0.1；然后没有预测错误的正类，即 FP = 0，FPR = 0，这个时候曲线的第二个坐标就是 (0, 0.1)。

通过不断调整阈值，就可以得到曲线的不同坐标，最终得到下图所示的 ROC 曲线。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/roc_2.jpg)

第二种更直观地绘制 ROC 曲线的方法，首先统计出正负样本的数量，假设分别是 P 和 N，接着，将横轴的刻度间隔设置为 1/N，纵轴的刻度间隔设置为 1/P。然后根据模型输出的概率对样本排序，并按顺序遍历样本，从零点开始绘制 ROC 曲线，**每次遇到一个正样本就沿纵轴方向绘制一个刻度间隔的曲线**，**遇到一个负样本就沿横轴绘制一个刻度间隔的曲线**，直到遍历完所有样本，曲线最终停留在 (1,1) 这个点，此时就完成了 ROC 曲线的绘制了。

当然，更一般的 ROC 曲线是如下图所示的，会更加的平滑，上图是由于样本数量有限才导致的。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/ROC.png)

对于 ROC 曲线，有以下几点特性：

1.ROC 曲线通常都是从左下角 (0,0) 开始，到右上角 (1,1) 结束。

- 开始时候，**第一个样本被预测为正类**，其他都是预测为负类别；
  - TPR 会很低，几乎是 0，上述例子就是 0.1，此时大量正类没有被分类器找出来；
  - FPR 也很低，可能就是0，上述例子就是 0，这时候被预测为正类的样本可能实际也是正类，所以几乎没有预测错误的正类样本。
- 结束时候，**所有样本都预测为正类**。
  - TPR 几乎就是 1，因为所有样本都预测为正类，那肯定就找出所有的正类样本了；
  - FPR 也是几乎为 1，因为所有负样本都被错误判断为正类。

2.ROC 曲线中：

- **对角线对应于随机猜想模型**，即概率为 0.5；
- **点 `(0,1)` 是理想模型**，因为此时 `TPR=1`，`FPR=0`，也就是正类都预测出来，并且没有预测错误；
- 通常，**ROC 曲线越接近点 `(0, 1)` 越好。**

3.同样可以根据 ROC 曲线来判断两个分类器的性能：

- 如果**分类器 `A` 的 `ROC` 曲线被分类器 `B` 的曲线完全包住，可以说 `B` 的性能好过 `A**`，这对应于上一条说的 ROC 曲线越接近点 `(0, 1)越好；
- 如果两个分类器的 `ROC` 曲线发生了交叉，则同样很难直接判断两者的性能优劣，需要借助 `ROC` 曲线下面积大小来做判断，而这个面积被称为 **`AUC:Area Under ROC Curve`**。

简单的代码实现如下：

```python
def true_negative_rate(y_true, y_pred):
    true_negative = sum(1 - (yi or yi_hat) for yi, yi_hat in zip(y_true, y_pred))
    actual_negative = len(y_true) - sum(y_true)
    return true_negative / actual_negative


def roc(y, y_hat_prob):
    thresholds = sorted(set(y_hat_prob), reverse=True)
    ret = [[0, 0]]
    for threshold in thresholds:
        y_hat = [int(yi_hat_prob >= threshold) for yi_hat_prob in y_hat_prob]
        ret.append([recall(y, y_hat), 1 - true_negative_rate(y, y_hat)])
    return ret
```

简单的测试例子如下：

```python
y_true = [1, 0, 1, 0, 1]
y_hat_prob = [0.9, 0.85, 0.8, 0.7, 0.6]

roc_list = roc(y_true, y_hat_prob)
print('roc_list:', roc_list)
# 输出结果是 roc_list: [[0, 0], [0.3333333333333333, 0.0], [0.3333333333333333, 0.5], [0.6666666666666666, 0.5], [0.6666666666666666, 1.0], [1.0, 1.0]]
```



##### 1.3.2 **ROC 和 P-R 曲线的对比**

**相同点**

1.**两者刻画的都是阈值的选择对分类度量指标的影响**。虽然每个分类器对每个样本都会输出一个概率，也就是置信度，但通常我们都会人为设置一个阈值来影响分类器最终判断的结果，比如设置一个很高的阈值--0.95，或者比较低的阈值--0.3。

- **如果是偏向于精确率，则提高阈值**，保证只把有把握的样本判断为正类，此时可以设置阈值为 0.9，或者更高；
- **如果偏向于召回率，那么降低阈值**，保证将更多的样本判断为正类，更容易找出所有真正的正样本，此时设置阈值是 0.5，或者更低。

2.两个曲线的每个点都是**对应某个阈值的选择，该点是在该阈值下的 `(精确率，召回率)` / `(TPR, FPR)`**。然后沿着横轴方向对应阈值的下降。

**不同**

相比较 `P-R` 曲线，`ROC` 曲线有一个特点，就是**正负样本的分布发生变化时，它的曲线形状能够基本保持不变**。如下图所示：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/ROC_PR_compare.jpg)

分别比较了增加十倍的负样本后， `P-R` 和 `ROC` 曲线的变化，可以看到 `ROC` 曲线的形状基本不变，但 `P-R` 曲线发生了明显的变化。

所以 `ROC` 曲线的这个特点**可以降低不同测试集带来的干扰**，**更加客观地评估模型本身的性能**，因此它适用的场景更多，比如排序、推荐、广告等领域。

这也是由于**现实场景中很多问题都会存在正负样本数量不平衡**的情况，比如计算广告领域经常涉及转化率模型，正样本的数量往往是负样本数量的千分之一甚至万分之一，这时候选择 `ROC` 曲线更加考验反映模型本身的好坏。

当然，如果希望看到模型在特定数据集上的表现，`P-R` 曲线会更直观地反映其性能。所以还是需要具体问题具体分析。

##### 1.3.3 AUC 曲线

`AUC` 是 `ROC` 曲线的面积，其物理意义是：从所有正样本中随机挑选一个样本，模型将其预测为正样本的概率是 $p_1$；从所有负样本中随机挑选一个样本，模型将其预测为正样本的概率是 $p_0$。**$p_1 > p_0$ 的概率就是 `AUC`**。

`AUC` 曲线有以下几个特点：

- 如果完全随机地对样本进行分类，那么 $p_1 > p_0$ 的概率是 0.5，则 `AUC=0.5`；

- **`AUC` 在样本不平衡的条件下依然适用**。

  如：在反欺诈场景下，假设正常用户为正类（设占比 99.9%），欺诈用户为负类（设占比 0.1%）。

  如果使用准确率评估，则将所有用户预测为正类即可获得 99.9%的准确率。很明显这并不是一个很好的预测结果，因为欺诈用户全部未能找出。

  如果使用 `AUC` 评估，则此时 `FPR=1,TPR=1`，对应的 `AUC=0.5` 。因此 `AUC` 成功的指出了这并不是一个很好的预测结果。

- `AUC` 反应的是**模型对于样本的排序能力**（根据样本预测为正类的概率来排序）。如：`AUC=0.8` 表示：给定一个正样本和一个负样本，在 `80%` 的情况下，模型对正样本预测为正类的概率大于对负样本预测为正类的概率。

- **`AUC` 对于均匀采样不敏感**。如：上述反欺诈场景中，假设对正常用户进行均匀的降采样。任意给定一个负样本 n，设模型对其预测为正类的概率为 Pn 。降采样前后，由于是均匀采样，因此预测为正类的概率大于 Pn 和小于 Pn  的真正样本的比例没有发生变化。因此 `AUC` 保持不变。

  但是如果是非均匀的降采样，则预测为正类的概率大于 Pn  和小于 Pn 的真正样本的比例会发生变化，这也会导致 `AUC` 发生变化。

- **正负样本之间的预测为正类概率之间的差距越大，则 `AUC` 越高**。因为这表明正负样本之间排序的把握越大，区分度越高。

  如：在电商场景中，点击率模型的 `AUC` 要低于购买转化模型的 `AUC` 。因为点击行为的成本低于购买行为的成本，所以点击率模型中正负样本的差别要小于购买转化模型中正负样本的差别。

`AUC` 的计算可以通过对 `ROC` 曲线下各部分的面积求和而得。假设 `ROC` 曲线是由坐标为下列这些点按顺序连接而成的： 
$$
{(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m)}, 其中\ x_1=0, x_m=1
$$
那么 `AUC` 可以这样估算：
$$
AUC = \frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)\times (y_i+y_{i+1})
$$

代码实现如下：

```python
def get_auc(y, y_hat_prob):
    roc_val = iter(roc(y, y_hat_prob))
    tpr_pre, fpr_pre = next(roc_val)
    auc = 0
    for tpr, fpr in roc_val:
        auc += (tpr + tpr_pre) * (fpr - fpr_pre) / 2
        tpr_pre = tpr
        fpr_pre = fpr
    return auc
```

简单的测试样例如下：

```python
y_true = [1, 0, 1, 0, 1]
y_hat_prob = [0.9, 0.85, 0.8, 0.7, 0.6]

auc_val = get_auc(y_true, y_hat_prob)
print('auc_val:', auc_val) # 输出是 0.5
```

#### 1.4 代价矩阵

前面介绍的性能指标都有一个隐式的前提，错误都是**均等代价**。但实际应用过程中，不同类型的错误所造成的后果是不同的。比如将健康人判断为患者，与患者被判断为健康人，代价肯定是不一样的，前者可能就是需要再次进行检查，而后者可能错过治疗的最佳时机。

因此，为了衡量不同类型所造成的不同损失，可以为错误赋予**非均等代价(unequal cost)**。

对于一个二类分类问题，可以设定一个**代价矩阵(cost matrix)**，其中 $cost_{ij}$ 表示将第 `i` 类样本预测为第 `j` 类样本的代价，而预测正确的代价是 0 。如下表所示：

|                | 预测：第 0 类 | 预测：第 1 类 |
| :------------: | :-----------: | :-----------: |
| 真实：第 0 类  |       0       |  $cost_{01}$  |
| 真实： 第 1 类 |  $cost_{10}$  |       0       |

1. 在非均等代价下，希望找到的不再是简单地最小化错误率的模型，而是希望找到**最小化总体代价 `total cost` 的模型**。

2. 在非均等代价下，`ROC` 曲线不能直接反映出分类器的期望总体代价，此时需要使用代价曲线 `cost curve`

   - 代价曲线的横轴是**正例概率代价**，如下所示，其中 p 是正例(第 0 类)的概率

   $$
   P_{+cost} = \frac{p\times cost_{01}}{p\times cost_{01}+(1-p)\times cost_{10}}
   $$





   - 代价曲线的纵轴是归一化代价，如下所示：
     $$
     cost_{norm} = \frac{FNR\times p\times cost_{01}+FPR\times (1-p)\times cost_{10}}{p\times cost_{01}+(1-p)\times cost_{10}}
     $$
     其中，假正例率 `FPR` 表示模型将负样本预测为正类的概率，定义如下：
     $$
     FPR = \frac{FP}{TN+FP}
     $$
     假负例率 `FNR` 表示将正样本预测为负类的概率，定义如下：
     $$
     FNR = 1 - TPR = \frac{FN}{TP+FN}
     $$
     代价曲线如下图所示：

     ![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/cost_curve.png)



#### 1.5 回归问题的性能度量

对于回归问题，常用的性能度量标准有：

1.均方误差(Mean Square Error, MSE)，定义如下：
$$
MSE=\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y_i})^2
$$
2.均方根误差(Root Mean Squared Error, RMSE)，定义如下：
$$
RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y_i})^2}
$$
3.均方根对数误差(Root Mean Squared Logarithmic Error, RMSLE)，定义如下
$$
RMSLE=\sqrt{\frac{1}{N}\sum_{i=1}^N[log(y_i+1)- log(\hat{y_i}+1)]^2}
$$
4.平均绝对误差(Mean Absolute Error, MAE)，定义如下：
$$
MAE = \frac{1}{N}\sum_{i=1}^N |y_i-\hat{y_i}|
$$
这四个标准中，比较常用的第一个和第二个，即 `MSE` 和 `RMSE`，这两个标准一般都可以很好反映回归模型预测值和真实值的偏离程度，但如果遇到**个别偏离程度非常大的离群点**时，即便数量很少，也会让这两个指标变得很差。

遇到这种情况，有三种解决思路：

- 将离群点作为噪声点来处理，即数据预处理部分需要过滤掉这些噪声点；
- 从模型性能入手，提高模型的预测能力，将这些离群点产生的机制建模到模型中，但这个方法会比较困难；
- 采用其他指标，比如第三个指标 `RMSLE`，它关注的是预测误差的比例，即便存在离群点，也可以降低这些离群点的影响；或者是 `MAPE`，平均绝对百分比误差(Mean Absolute Percent Error)，定义为：

$$
MAPE = \sum_{i=1}^n |\frac{y_i-\hat{y_i}}{y_i}|\times\frac{100}{n}
$$

`RMSE` 的简单代码实现如下所示：

```python
def rmse(predictions, targets):
    # 真实值和预测值的误差
    differences = predictions - targets
    differences_squared = differences ** 2
    mean_of_differences_squared = differences_squared.mean()
    # 取平方根
    rmse_val = np.sqrt(mean_of_differences_squared)
    return rmse_val
```






#### 1.6 其他评价指标

1. 计算速度：模型训练和预测需要的时间；
2. 鲁棒性：处理缺失值和异常值的能力；
3. 可拓展性：处理大数据集的能力；
4. 可解释性：模型预测标准的可理解性，比如决策树产生的规则就很容易理解，而神经网络被称为黑盒子的原因就是它的大量参数并不好理解。


### 2. 模型评估的方法

#### 2.1 泛化能力

1. **泛化能力**：指模型对**未知的、新鲜的数据的预测能力**，通常是根据**测试误差**来衡量模型的泛化能力，测试误差越小，模型能力越强；
2. 统计理论表明：如果训练集和测试集中的样本都是独立同分布产生的，则有 **模型的训练误差的期望等于模型的测试误差的期望** 。
3. 机器学习的“没有免费的午餐定理”表明：在所有可能的数据生成分布上，没有一个机器学习算法总是比其他的要好。
   - 该结论仅在考虑所有可能的数据分布时才成立。
   - 现实中特定任务的数据分布往往满足某类假设，从而可以设计在这类分布上效果更好的学习算法。
   - 这意味着机器学习并不需要寻找一个通用的学习算法，而是寻找一个在关心的数据分布上效果最好的算法。
4. 正则化是对学习算法做的一个修改，这种修改趋向于降低泛化误差（而不是降低训练误差）。
   - 正则化是机器学习领域的中心问题之一。
   - 没有免费的午餐定理说明了没有最优的学习算法，因此也没有最优的正则化形式。

#### 2.2 泛化能力的评估

常用的对模型泛化能力的评估方法有以下几种，主要区别就是如何划分测试集。

- **留出法**(Holdout)
- **`k-fold` 交叉验证**(Cross Validation)
- **留一法**(Leave One Out, LOO)
- **自助法**(bootstrapping)

##### 2.2.1 **留出法**(Holdout)

留出法是最简单也是最直接的验证方法，它就是将**数据集随机划分为两个互斥的集合**，即训练集和测试集，比如按照 7:3 的比例划分，70% 的数据作为训练集，30% 的数据作为测试集。**也可以划分为三个互斥的集合，此时就增加一个验证集，用于调试参数和选择模型**。

直接采用 `sklearn` 库的 `train_test_split` 函数即可实现，一个简单的示例代码如下，这里简单调用 `knn` 算法，采用 `Iris` 数据集。

```python
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier

# 加载 Iris 数据集
dataset = load_iris()
# 划分训练集和测试集
(trainX, testX, trainY, testY) = train_test_split(dataset.data, dataset.target, random_state=3, test_size=0.3)
# 建立模型
knn = KNeighborsClassifier()
# 训练模型
knn.fit(trainX, trainY)
# 将准确率打印
print('hold_out, score:', knn.score(testX, testY))
```

留出法的使用需要注意：

1. **数据集的划分要尽可能保持数据分布的一致性，避免因为数据划分过程引入额外的偏差而对最终结果产生影响**。比如训练、验证和测试集的类别比例差别很大，则误差估计将由于三个集合数据分布的差异而产生偏差。

   因此，**分类任务中必须保持每个集合中的类别比例相似**。从采样的角度看数据集的划分过程，这种保留类别比例的采样方式称为“分层采样”。

2. 即便确定了训练、验证、测试集的比例，还是有多种划分方式，比如排序后划分、随机划分等等，这些不同的划分方式导致**单次留出法得到的估计结果往往不够稳定可靠**。因此，使用留出法的时候，**往往采用若干次随机划分、重复进行实验后，取平均值作为最终评估结果**。

分层采样的简单代码实现如下所示，主要是调用了 `sklearn.model_selection`  中的 `StratifiedKFold`

```python
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone

def StratifiedKFold_method(n_splits=3):
    '''
    分层采样
    :return:
    '''
    # 加载 Iris 数据集
    dataset = load_iris()
    data = dataset.data
    label = dataset.target
    # 建立模型
    knn = KNeighborsClassifier()
    print('use StratifiedKFold')
    skfolds = StratifiedKFold(n_splits=n_splits, random_state=42)
    scores = 0.
    for train_index, test_index in skfolds.split(data, label):
        clone_clf = clone(knn)
        X_train_folds = data[train_index]
        y_train_folds = (label[train_index])
        X_test_fold = data[test_index]
        y_test_fold = (label[test_index])
        clone_clf.fit(X_train_folds, y_train_folds)
        y_pred = clone_clf.predict(X_test_fold)
        n_correct = sum(y_pred == y_test_fold)
        print(n_correct / len(y_pred))
        scores += n_correct / len(y_pred)
    print('mean scores:', scores / n_splits)
```

留出法也存在以下的缺点：

1. **在验证集或者测试集上的评估结果和划分方式有关系**，这也就是为什么需要多次实验，取平均值；
2. 我们希望评估的是在原始数据集上训练得到的模型的能力，但留出法在划分两个或者三个集合后，训练模型仅使用了原始数据集的一部分，这会降低评估结果的保真性。但这个问题没有完美的解决方法，常见做法是将大约 `2/3 ~ 4/5` 的样本作为训练集，剩余的作为验证集和测试集。

##### 2.2.2 **`k-fold` 交叉验证**(Cross Validation)

**`k-fold` 交叉验证** 的工作流程：

1. 将原始数据集划分为 `k` 个大小相等且互斥的子集；
2. 选择 `k-1` 个子集作为训练集，剩余作为验证集进行模型的训练和评估，重复 `k` 次(每次采用不同子集作为验证集)；
3. 将 `k` 次实验评估指标的平均值作为最终的评估结果。

通常，`k` 取 10。

但和留出法类似，同样存在多种划分 `k` 个子集的方法，所以依然需要随时使用不同方式划分 `p` 次，每次得到 `k` 个子集。

同样，采用 `sklearn.cross_validation` 的 `cross_val_score` 库可以快速实现 `k-fold` 交叉验证法，示例如下：

```python
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cross_validation import cross_val_score
# 加载 Iris 数据集
dataset = load_iris()
data = dataset.data
label = dataset.target
# 建立模型
knn = KNeighborsClassifier()
# 使用K折交叉验证模块
scores = cross_val_score(knn, data, label, cv=10, scoring='accuracy')
# 将每次的预测准确率打印出
print(scores)
# 将预测准确平均率打印出
print(scores.mean())
```

##### 2.2.3 留一法

留一法是 `k-fold` 交叉验证的一个特例情况，即让 `k=N`, 其中 `N` 是原始数据集的样本数量，这样**每个子集就只有一个样本，这就是留一法**。

留一法的优点就是**训练数据更接近原始数据集**了，仅仅相差一个样本而已，通过这种方法训练的模型，几乎可以认为就是在原始数据集上训练得到的模型 。

但缺点也比较明显，**计算速度会大大降低**，**特别是原始数据集非常大的时候**，训练 `N` 个模型的计算量和计算时间都很大，因此一般实际应用中很少采用这种方法。

##### 2.2.4 自助法

在留出法和 `k-fold` 交叉验证法中，由于保留了一部分样本用于测试，因此实际训练模型使用的训练集比初始数据集小，**这必然会引入一些因为训练样本规模不同而导致的估计偏差**。

留一法受训练样本规模变化的影响较小，但是**计算复杂度太高**。

自助法是一个以**自助采样法(`bootstrap sampling`)为基础的比较好的解决方案**。同时，它也是随机森林算法中用到的方法。

它的做法就是**对样本数量为 `N` 的数据集进行 `N` 次有放回的随机采样**，得到一个大小是 `N` 的训练集。

在这个过程中将会有一部分数据是没有被采样得到的，一个样本始终没有被采样出来的概率是 $(1-\frac{1}{N})^N$，根据极限可以计算得到:
$$
lim_{N\rightarrow +\infty}(1-\frac{1}{N})^N=\frac{1}{e}\approx0.368
$$
也就是采用自助法，会有 **36.8%** 的样本不会出现在训练集中，使用这部分样本作为测试集。这种方法也被称为包外估计。

自助法的优点有：

- 在**数据集比较小、难以有效划分训练/测试集**时很有用：

- 能从**初始数据集中产生多个不同的训练集**，这对集成学习等方法而言有很大好处。

但也存在如下缺点：

- 产生的数据集**改变了初始数据集的分布，这会引入估计偏差**。因此在**初始数据量足够时，留出法和折交叉验证法更常用**。

#### 2.3 训练集、验证集、测试集

简单介绍下训练集、验证集和测试集各自的作用：

1. **训练集**：主要就是训练模型，理论上越大越好；
2. **验证集**：用于模型调试超参数。通常要求验证集比较大，避免模型会对验证集过拟合；
3. **测试集**：用于评估模型的泛化能力。理论上，测试集越大，评估结果就约精准。另外，测试集必须不包含训练样本，否则会影响对模型泛化能力的评估。

验证集和测试集的对比：

- 测试集通常用于对模型的预测能力进行评估，它是**提供模型预测能力的无偏估计**；如果不需要对模型预测能力的无偏估计，可以不需要测试集；
- 验证集主要是用于超参数的选择。

#### 2.4 划分数据集的比例选择方法

那么一般如何选择划分训练、验证和测试集的比例呢？通常可以按照如下做法：

1. 对于**小批量数据**，数据的拆分的常见比例为：
   - **如果未设置验证集，则将数据三七分**：70% 的数据用作训练集、30% 的数据用作测试集。
   - **如果设置验证集，则将数据划分为**：60% 的数据用作训练集、20%的数据用过验证集、20% 的数据用作测试集。
2. **对于大批量数据，验证集和测试集占总数据的比例会更小**。
   - 对于百万级别的数据，其中 1 万条作为验证集、1 万条作为测试集即可。
   - 验证集的目的就是验证不同的超参数；测试集的目的就是比较不同的模型。
     - 一方面它们要足够大，才足够评估超参数、模型。
     - 另一方面，如果它们太大，则会浪费数据（验证集和训练集的数据无法用于训练）。
3. 在 `k-fold` 交叉验证中：先将所有数据拆分成 `k` 份，然后其中 `1` 份作为测试集，其他 `k-1` 份作为训练集。
   - 这里并没有验证集来做超参数的选择。所有测试集的测试误差的均值作为模型的预测能力的一个估计。
   - **使用 `k-fold` 交叉的原因是：样本集太小**。如果选择一部分数据来训练，则有两个问题：
     - **训练数据的分布可能与真实的分布有偏离**。`k-fold` 交叉让所有的数据参与训练，会使得这种偏离得到一定程度的修正。
     - **训练数据太少，容易陷入过拟合**。`k`-fold 交叉让所有数据参与训练，会一定程度上缓解过拟合。

#### 2.5 分布不匹配

深度学习时代，经常会发生：**训练集和验证集、测试集的数据分布不同**。

如：训练集的数据可能是从网上下载的高清图片，测试集的数据可能是用户上传的、低像素的手机照片。

- **必须保证验证集、测试集的分布一致**，它们都要很好的代表你的真实应用场景中的数据分布。
- **训练数据可以与真实应用场景中的数据分布不一致**，因为最终关心的是在模型真实应用场景中的表现。

如果发生了数据不匹配问题，则可以想办法**让训练集的分布更接近验证集**。

- 一种做法是：**收集更多的、分布接近验证集的数据作为训练集合**。
- 另一种做法是：**人工合成训练数据，使得它更接近验证集**。该策略有一个潜在问题：你可能只是模拟了全部数据空间中的一小部分。导致你的模型对这一小部分过拟合。

当训练集和验证集、测试集的数据分布不同时，有以下经验原则：

- **确保验证集和测试集的数据来自同一分布**。

  因为需要使用验证集来优化超参数，而优化的最终目标是希望模型在测试集上表现更好。

- **确保验证集和测试集能够反映未来得到的数据，或者最关注的数据**。

- **确保数据被随机分配到验证集和测试集上**。

当训练集和验证集、测试集的数据分布不同时，**分析偏差和方差的方式有所不同**。

- 如果**训练集和验证集的分布一致**，那么当训练误差和验证误差相差较大时，我们认为**存在很大的方差问题**。

- 如果训练集和验证集的分布不一致，那么当训练误差和验证误差相差较大时，有两种原因：

  - 第一个原因：**模型只见过训练集数据，没有见过验证集的数据导致的，是数据不匹配的问题**。
  - 第二个原因：**模型本来就存在较大的方差**。

  为了弄清楚原因，需要将训练集再随机划分为：`训练-训练集`、`训练-验证集`。这时候，`训练-训练集`、`训练-验证集` 是同一分布的。

  - **模型在`训练-训练集` 和 `训练-验证集` 上的误差的差距代表了模型的方差**。
  - **模型在`训练-验证集` 和 验证集上的误差的差距代表了数据不匹配问题的程度**。

### 3. 过拟合、欠拟合

机器学习的两个主要挑战是**过拟合和欠拟合**。

**过拟合(overfitting)**：**指算法模型在训练集上的性能非常好，但是泛化能力很差，泛化误差很大，即在测试集上的效果却很糟糕的情况**。

- 过拟合的原因：将**训练样本本身的一些特点当作了所有潜在样本都具有的一般性质**，这会造成泛化能力下降；另一个原因是**模型可能学到训练集中的噪声，并基于噪声进行了预测**；
- **过拟合无法避免，只能缓解**。因为**机器学习的问题通常是 `NP` 难甚至更难的，而有效的学习算法必然是在多项式时间内运行完成**。如果可以避免过拟合，这就意味着构造性的证明了 `P=NP` 。

**欠拟合(underfitting)**：**模型的性能非常差，在训练数据和测试数据上的性能都不好，训练误差和泛化误差都很大**。其原因就是模型的学习能力比较差。

一般可以通过挑战模型的容量来缓解过拟合和欠拟合问题。**模型的容量是指其拟合各种函数的能力**。

- 容量低的模型容易发生欠拟合，模型拟合能力太弱。
- 容量高的模型容易发生过拟合，模型拟合能力太强。

一般解决过拟合的方法有：

- **简化模型**，这包括了采用简单点的模型、减少特征数量，比如神经网络中减少网络层数或者权重参数，决策树模型中降低树的深度、采用剪枝等；
- **增加训练数据**，采用数据增强的方法，比如人工合成训练数据等；
- **早停**，当验证集上的误差没有进一步改善，训练提前终止；
- **正则化**，常用 L1 或者 L2 正则化。
- **集成学习方法**，训练多个模型，并以每个模型的平均输出作为结果，降低单一模型的过拟合风险，常用方法有 `bagging` 、`boosting`、`dropout`(深度学习中的方法)等；
- **噪声注入**：包括输入噪声注入、输出噪声注入、权重噪声注入。将噪声分别注入到输入/输出/权重参数中，虽然噪声可能是模型过拟合的一个原因，但第一可以通过交叉验证来避免；第二就是没有噪声的完美数据也是很有可能发生过拟合；第三可以选择在特征、权值参数加入噪声，而非直接在数据加入噪声。



解决欠拟合的方法有：

- 选择一个**更强大的模型**，带有更多参数
- 用**更好的特征**训练学习算法（特征工程）
- **减小对模型的限制**（比如，减小正则化超参数）



### 4. 超参数调优

超参数调优是一件非常头疼的事情，很多时候都需要一些先验知识来选择合理的参数值，但如果没有这部分先验知识，要找到最优的参数值是很困难，非常耗费时间和精力。但超参数调优确实又可以让模型性能变得更加的好。

在选择超参数调优算法前，需要明确以下几个要素：

- **目标函数**。算法需要最大化/最小化的目标；
- **搜索范围**。一般通过上下限来确定；
- **算法的其他参数**，比如搜索步长。

#### 4.1 搜索策略

常用的几种超参数搜索策略如下：

- **手动搜索**：需要较好的先验知识经验
- **网格搜索**：超参数的数据相对较少的时候，这个方法比较实用
- **随机搜索**：通常推荐这种方式
- **贝叶斯优化算法**：基于模型的搜索方法，利用了历史搜索结果

##### 4.1.1 手动搜索

1. 手动选择超参数需要了解超参数做了些什么，以及机器学习模型如何才能取得良好的泛化。

2. 手动搜索超参数的任务是：**在给定运行时间和内存预算范围的条件下，最小化泛化误差**。

3. 手动调整超参数时不要忘记最终目标：提升测试集性能。

   - 加入正则化只是实现这个目标的一种方法。

   - 如果训练误差很低，也可以通过收集更多的训练数据来减少泛化误差。

     如果训练误差太大，则收集更多的训练数据就没有意义。

   - 实践中的一种暴力方法是：不断提高模型容量和训练集的大小。

     这种方法增加了计算代价，只有在拥有充足的计算资源时才可行

##### 4.1.2 网格搜索

网格搜索可能是最简单也是应用最广泛的超参数搜索算法了。它的几种做法如下：

- **采用较大的搜索范围和较小的搜索步长**，很大概率会搜索到全局最优值，但十分耗费计算资源和时间，特别是超参数比较多的时候；
- **先采用较大搜索范围和较大步长**，寻找全局最优的可能位置，**然后逐渐缩小搜索范围和步长**，来确定更精确的最优值。可以降低所需要的计算时间和计算量，但由于目标函数一般都是非凸的，可能会错过全局最优值。

网格搜索也可以借助 `sklearn` 实现，简单的示例代码如下：

```python
from sklearn.model_selection import	GridSearchCV
from sklearn.ensemble import RandomForestClassifier
param_grid = [
    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
]
forest_reg = RandomForestRegressor()
grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                           scoring='neg_mean_squared_error')
grid_search.fit(data, labels)
```



##### 4.1.3 随机搜索

随机搜索是一种可以替代网格搜索的方法，它编程简单、使用方便、能更快收敛到超参数的良好取值。

- 首先为每个超参数定义一个边缘分布，如伯努利分布（对应着二元超参数）或者对数尺度上的均匀分布（对应着正实值超参数）。
- 然后假设超参数之间相互独立，从各分布中抽样出一组超参数。
- 使用这组超参数训练模型。
- 经过多次抽样 -> 训练过程，挑选验证集误差最小的超参数作为最好的超参数。

随机搜索的优点如下：

- **不需要离散化超参数的值，也不需要限定超参数的取值范围**。这允许我们在一个更大的集合上进行搜索。
- 当某些超参数对于性能没有显著影响时，随机搜索相比于网格搜索指数级地高效，它能**更快的减小验证集误差**。

**随机搜索比网格搜索更快的找到良好超参数的原因是：没有浪费的实验**。

- 在网格搜索中，**两次实验之间只会改变一个超参数** （假设为 `m`）的值，而其他超参数的值保持不变。如果这个超参数 `m` 的值对于验证集误差没有明显区别，那么网格搜索相当于进行了两个重复的实验。
- 在随机搜索中，**两次实验之间，所有的超参数值都不会相等，因为每个超参数的值都是从它们的分布函数中随机采样而来**。因此不大可能会出现两个重复的实验。
- 如果  `m`  超参数与泛化误差无关，那么不同的 `m`  值：
  - 在网格搜索中，**不同 `m` 值、相同的其他超参数值，会导致大量的重复实验**。
  - 在随机搜索中，其他超参数值每次也都不同，因此不大可能出现两个重复的实验（除非所有的超参数都与泛化误差无关）。

随机搜索可以采用 `sklearn.model_selection` 中的 `RandomizedSearchCV ` 方法。

##### 4.1.4 贝叶斯优化方法

贝叶斯优化方法是基于模型的参数搜索算法的一种比较常见的算法。它相比于前面的网格搜索和随机搜索，**最大的不同就是利用历史的搜索结果进行优化搜索**。主要是由四部分组成的：

1. 目标函数。大部分情况是模型验证集上的损失；
2. 搜索空间。各类待搜索的超参数；
3. 优化策略。建立的概率模型和选择超参数的方式；
4. 历史的搜索结果。

贝叶斯优化算法的步骤如下：

1. 根据先验分布，假设一个搜索函数；
2. 然后，每一次采用新的采样点来测试目标函数时，利用这个信息更新目标函数的先验分布；
3. 最后，算法测试由后验分布给出的全局最优最可能出现的位置的点。

需要特别注意的是，贝叶斯优化算法容易**陷入局部最优值**：它在找到一个局部最优值后，会不断在该区域进行采样。

因此，贝叶斯优化算法会在探索和利用之间找到一个平衡点，探索是在还未取样的区域获取采样点，利用则是根据后验分布在最可能出现全局最优的区域进行采样。



#### 4.2 调整原则

1. 通常**先对超参数进行粗调，然后在粗调中表现良好的超参数区域进行精调**。

2. 超参数随机搜索，并不意味着是在有效范围内随机均匀取值。需要选择合适的缩放来进行随机选取。

   - 对于学习率，假设其取值范围为 `0.000001~1`。

     如果进行均匀取值，取 10 个，那么有  90% 的随机值都位于区间 `[0.1,1]`。则 `[0.000001,0.1]` 之间没有足够的探索。这种做法明显不合理。

     此时需要使用对数缩放，在对数轴上均匀随机取点。

   - 对于指数加权移动平均的超参数 β  。假设其取值范围为 `0.9~0.9999`。

     由于 $\frac{1}{1-\beta}$ 刻画了结果使用过去多少个周期的数据来加权平均。因此如果进行均匀取值，则：

     -  β  在`0.9~0.9005` 之间取值时， $\frac{1}{1-\beta}$ 变化不大。
     -  β  在`0.9990~0.9995` 之间取值时，$\frac{1}{1-\beta}$  变化非常大。

     β 越接近 1，$\frac{1}{1-\beta}$ 对于它的变化越敏感。此时，需要对 1-β 使用对数缩放，在对数轴上均匀随机取点。

   - 如果选择了错误的缩放，如果取值的总量足够大，也可以得到不错的结果。

     尤其当配合了`粗调 -> 精调` 策略时，最终还是会聚焦到合适的超参数范围上。

3. 通常情况下，**建议至少每隔几个月重新评估或者修改超参数**。因为随着时间的变化，真实场景的数据会逐渐发生改变：

   - 可能是由于用户的行为、偏好发生了改变。
   - 可能是采样的方式发生了改变。
   - 也可能仅仅是由于数据中心更新了服务器。

   由于这些变化，原来设定的超参数可能不再适用。

4. 有两种超参数调整策略：

   - 如果数据足够大且没有足够的计算资源，此时只能一次完成一个试验。

     则**可以每天观察模型的表现，实时的、动态的调整超参数**。

   - 如果数据不大，有足够的计算资源可以同一时间完成大量的试验，则**可以设置多组超参数设定，然后选择其中表现最好的那个**。





------

### 参考

- 《机器学习》--周志华
- 《百面机器学习》
- 《hands-on-ml-with-sklearn-and-tf》
- [9. 模型评估](http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/9_model_selection.html)
- [分类模型评估的方法及Python实现](https://mp.weixin.qq.com/s/fhVlM8L4dyvMopf3pvau0A)
- [机器学习中用来防止过拟合的方法有哪些？](https://www.zhihu.com/question/59201590)



------

# 知识点

---

## 1. 如何处理类别不平衡问题？




参考：

- [How to handle Imbalanced Classification Problems in machine learning?](https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/)
- [在分类中如何处理训练集中不平衡问题](https://blog.csdn.net/heyongluoyao8/article/details/49408131)

---

## 2. 什么是过拟合和欠拟合？如何解决？





---

## 3. 常见的损失函数有哪些？各自适用的场景。

参考文章：

- [交叉熵代价函数（作用及公式推导）](https://blog.csdn.net/u014313009/article/details/51043064)
- [机器学习中常用的损失函数你知多少？](https://www.jiqizhixin.com/articles/091202)
- [理解Hinge Loss (折页损失函数、铰链损失函数)](https://blog.csdn.net/fendegao/article/details/79968994)
- [简单谈谈Cross Entropy Loss](https://blog.csdn.net/xg123321123/article/details/80781611)







---
## 4. ROC 曲线、AUC、precision、recall等关系，如何计算 AUC

参考文章：

- [ROC和AUC介绍以及如何计算AUC](http://alexkong.net/2013/06/introduction-to-auc-and-roc/)
- [准确率(Accuracy), 精确率(Precision), 召回率(Recall)和F1-Measure](https://blog.argcv.com/articles/1036.c)