# 机器学习笔记合集

机器学习基础的学习笔记。



# 1. 基本概念

## 1.1 定义

机器学习算法是一种能够从数据中学习的算法。那么这里的学习的定义是什么呢？这里有一个简单的定义：

> 对于某类任务 T 和性能度量 P，一个计算机程序通过经验 E 改进后，在任务 T 上由性能度量 P 衡量的性能有所提升，这称为学习。

举例来说这个定义，比如对于图像分类这个任务，一般的性能度量 P 就是分类的准确率，而经验 E 其实就是图片数据集，当我们采用的算法，比如 CNN，在给定的训练集上训练后，然后在测试集上的准确率有所提升，这就是学习了。

这里的任务 T、经验 E 和性能 P 其实指代的内容非常的多，这里简单的介绍一下。

首先，对于任务 T，在机器学习领域里，可以是这些方向的任务：

- 分类：在该任务中计算机程序需要判断输入数据是属于给的 k 类中的哪一类，最常见的就是人脸识别，也是图像分类的一个子方向，另外还有语音识别、文本识别等；
- 回归：在该任务中需要对给定的输入预测数值，比如预测房价或者证券未来的价格等；
- 转录：将一些相对非结构化表示的数据，转录为离散的文本形式。比如 OCR（光学字符识别）、语音识别等；
- 机器翻译：将一种语言的序列转化为另一种语言。比如英语翻译为中文；
- 异常检测：查找不正常或者非典型的个体；
- 去噪

- 等等

对于性能度量 P，在不同的任务中会采用不同的性能指标，比如：

- 准确率和错误率
- 召回率、精准率、F1
- ROC 和 AUC
- 均方误差（MSE）、均方根误差（RMSE）
- 交并比 IoU

而经验 E，一般就是指数据集了，不同的任务对数据集的要求也不一样，比如图片分类一般就是图片和图片的标签，但目标检测、图像分割，需要的除了图片、标签，有的还需要图片中物体的标注框或者坐标信息等。



## 1.2 各种常见算法图示

如下图所示，这些是常见的机器学习算法的图示。

|                           回归算法                           |                           聚类算法                           |                          正则化方法                          |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/常见算法图示/1.jpg" style="zoom:24%;" /> | <img src="/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/常见算法图示/2.jpg" style="zoom:24%;" /> | <img src="/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/常见算法图示/3.jpg" style="zoom:24%;" /> |

|                          决策树学习                          |                          贝叶斯方法                          |                         基于核的算法                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/常见算法图示/2.2.4.png" style="zoom:25%;" /> | <img src="/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/常见算法图示/5.jpg" style="zoom:25%;" /> | <img src="/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/常见算法图示/6.jpg" style="zoom:25%;" /> |

|                           聚类算法                           |                         关联规则学习                         |                         人工神经网络                         |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/常见算法图示/7.jpg" style="zoom:25%;" /> | <img src="/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/常见算法图示/2.2.8.png" style="zoom:36%;" /> | <img src="/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/常见算法图示/2.2.09.png" style="zoom:36%;" /> |

|                           深度学习                           |                         降低维度算法                         |                           集成算法                           |
| :----------------------------------------------------------: | :----------------------------------------------------------: | :----------------------------------------------------------: |
| <img src="/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/常见算法图示/2.2.10.png" style="zoom:50%;" /> | <img src="/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/常见算法图示/2.2.11.png" style="zoom:36%;" /> | <img src="/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/常见算法图示/2.2.12.png" style="zoom:50%;" /> |



## 1.3 计算图的导数计算

计算图导数计算是反向传播，利用链式法则和隐式函数求导。
	
假设 $z = f(u,v)$ 在点 $(u,v)$ 处偏导连续，$(u,v)$是关于 $t$ 的函数，在 $t$ 点可导，求 $z$ 在 $t$ 点的导数。

根据链式法则有
$$
\frac{dz}{dt}=\frac{\partial z}{\partial u}.\frac{du}{dt}+\frac{\partial z}{\partial v}
				.\frac{dv}{dt}
$$

链式法则用文字描述:“由两个函数凑起来的复合函数，其导数等于里边函数代入外边函数的值之导数，乘以里边函数的导数。 

为了便于理解，下面举例说明：
$$
f(x)=x^2,g(x)=2x+1
$$

则:
$$
{f[g(x)]}'=2[g(x)] \times g'(x)=2[2x+1] \times 2=8x+4
$$

## 1.4 局部最优和全局最优

优化问题一般分为局部最优和全局最优。其中，

1. 局部最优，就是在函数值空间的一个**有限区域内寻找最小值**；而全局最优，是在函数值空间**整个区域寻找最小值**问题。
2. 函数局部最小点是它的函数值小于或等于附近点的点，但是有可能大于较远距离的点。
3. 全局最小点是那种它的函数值小于或等于所有的可行点。



### 1.4.1 如何区分局部最小点和鞍点

参考知乎回答：

- https://www.zhihu.com/question/358632429/answer/919562000

- https://www.zhihu.com/question/68109802/answer/263503269



通常一阶导数为 0 的点称为稳定点，可以分为三类：

- 局部最小点
- 局部最大点
- 鞍点



鞍点如下所示：

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/%E9%9E%8D%E7%82%B9%E5%9B%BE%E7%A4%BA.png" style="zoom:50%;" />

一般区分鞍点和局部最优的方法是使用神经网络 loss surface 的 Hessian 矩阵，通过计算 Hessian 矩阵的特征值，进行判断：

- 当 Hessian 矩阵的**特征值有正有负**的时候，神经网络的一阶导数为 0 的点是**鞍点**；
- 当 Hessian 矩阵的**特征值是非负**的时候，神经网络的一阶导数为 0 的点是**局部极小值点**；
- 当 Hessian 矩阵**最小特征值小于零**，则为**严格鞍点**（包含了局部最大）



根据文章：[Geometry of Neural Network Loss Surfaces via Random Matrix Theory](https://link.zhihu.com/?target=http%3A//proceedings.mlr.press/v70/pennington17a/pennington17a.pdf)，可以看到神经网络的 Hessian 矩阵的特征值分布如下:

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/Geometry%20of%20Neural%20Networks.png" style="zoom:100%;" />

其中 $\phi$ 表示参数数目和数据量之比，其值越大表示数量相对较少，$\lambda$ 是特征值，$\epsilon$ 表示 loss 值，所以从上图可以得到：

- 当 loss 很大的时候，特征值有正有负，**表明鞍点是困扰优化的主要原因**；
- 当 loss 很小的时候，特征值慢慢都是非负数，**也就是说这个时候基本是局部最小点**。



另外一种判断是否是鞍点的方法：**若某个一阶导数为0的点在至少一个方向上的二阶导数小于0，那它就是鞍点**。

最优点和鞍点的区别在于**其在各个维度是否都是最低点**。

只要某个一阶导数为0的点在某个维度上是最高点而不是最低点，那它就是鞍点。**而区分最高点和最低点当然就是用二阶导数**，斜率从负变正的过程当然就是“下凸”，即斜率的导数大于0，即二阶导数大于0。反之则为“上凹”，二阶导数小于0。





### 1.4.2 如何避免陷入局部最小值或者鞍点

实际上，我们并不需要害怕陷入局部最小值，原因有这几个：

- 第一个，很直观的解释来自于上面特征值的分布信息：当loss很小的时候，我们才会遇到局部最小值问题，**也就是说这时候的loss已经足够小，我们对这时候的loss已经足够满意了，不太需要花更大力气去找全局最优值**。

- 第二个，**在一定假设条件下，很多研究表明深度学习中局部最小值很接近于全局最小值**。

另外，根据https://www.zhihu.com/question/68109802的回答：

> 实际上我们可能并没有找到过”局部最优“，更别说全局最优了；
>
> ”局部最优是神经网络优化的主要难点“，这其实是来自于一维优化问题的直观想象，单变量的情况下，优化问题最直观的困难就是有很多局部极值。但在多变量的情况下，就不一定能找到局部最优了；





而对于鞍点，逃离鞍点的做法有这几种：

1. 利用严格鞍点负特征值对应的方向，采用矩阵向量乘积的形式找到下降方向；
2. 利用扰动梯度方法逃离鞍点，在梯度的模小于某个数的时候，在梯度上加个动量。





## 1.5 机器学习、深度学习、数据挖掘、大数据之间的关系

首先来看这四者简单的定义：

- **大数据**通常被定义为“超出常用软件工具捕获，管理和处理能力”的数据集，一般是在数据量、数据速度和数据类别三个维度上都大的问题。 
- **机器学习**关心的问题是如何构建计算机程序使用经验自动改进。
- **数据挖掘**是从数据中提取模式的特定算法的应用，在数据挖掘中，重点在于算法的应用，而不是算法本身。
- **深度学习**是机器学习的一个子类，一般特指学习层数较高的网络结构，这个结构通常会结合线性和非线性的关系。

关于这四个的关系，可以如下图所示：

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/%E5%A4%A7%E6%95%B0%E6%8D%AE%E3%80%81%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E3%80%81%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%85%B3%E7%B3%BB%E5%9B%BE.jpg" style="zoom:50%;" />



**机器学习和数据挖掘**之间的关系如下：

> 数据挖掘是一个过程，在此过程中机器学习算法被用作提取数据集中的潜在有价值模式的工具。



大数据与深度学习关系总结如下：

（1）**深度学习是一种模拟大脑的行为**。可以从所学习对象的机制以及行为等等很多相关联的方面进行学习，模仿类型行为以及思维。

（2）**深度学习对于大数据的发展有帮助**。深度学习对于大数据技术开发的每一个阶段均有帮助，不管是数据的分析还是挖掘还是建模，只有深度学习，这些工作才会有可能一一得到实现。

（3）**深度学习转变了解决问题的思维**。很多时候发现问题到解决问题，走一步看一步不是一个主要的解决问题的方式了，在深度学习的基础上，要求我们从开始到最后都要基于一个目标，为了需要优化的那个最终目标去进行处理数据以及将数据放入到数据应用平台上去，这就是端到端（End to End）。

（4）**大数据的深度学习需要一个框架**。在大数据方面的深度学习都是从基础的角度出发的，深度学习需要一个框架或者一个系统。总而言之，将你的大数据通过深度分析变为现实，这就是深度学习和大数据的最直接关系。



机器学习和深度学习的关系：

- 深度学习是机器学习的一个子类，是机器学习的一类算法，相比传统的机器学习方法，深度学习有这几个特点：
  - **对硬件要求更高**。经常需要 GPU 才能快速完成任务，单纯用 CPU 执行任务，那耗时是非常的惊人；
  - **对数据量要求更高**。传统的机器学习一般可能只需要几百上千的数据量，但是对于深度学习的任务，至少也需要上万甚至几百万数据量，否则很容易过拟合；
  - **具有更强的特征提取能力**。深度学习可以从数据中学习到不同等级的特征，从低级的边缘特征，到高级的语义特征，这也是越来越多的机器学习方向都采用深度学习算法来解决问题的一个原因，性能更加强；
  - **可解释性差**。因为抽象层次较高，所以深度学习也经常被称为是一个黑匣子。
- 机器学习的核心是数学，是用一个数学模型，然后输入数据来调节数学模型的参数，从而让数学模型可以解决特定的某类问题。简单说就是希望训练得到一个可以解决特定问题的数学函数。



## 1.6 为什么要使用机器学习

原因如下：

- 需要进行大量手工调整或需要拥有长串规则才能解决的问题：机器学习算法通常可以**简化代码、提高性能**。
- 问题复杂，传统方法难以解决：最好的机器学习方法可以找到解决方案。
- 环境有波动：机器学习算法可以**适应新数据**。
- 洞察复杂问题和大量数据


一些机器学习的应用例子：

- 数据挖掘
- 一些无法通过手动编程来编写的应用：如自然语言处理，计算机视觉
- 一些自助式的程序：如推荐系统
- 理解人类是如何学习的





# 2. 机器学习系统的类型

机器学习有多种类型，可以根据如下规则进行分类：

- 是否在人类监督下进行训练（监督，非监督，半监督和强化学习）
- 是否可以动态渐进学习（在线学习 vs批量学习）
- 它们是否只是通过简单地比较新的数据点和已知的数据点，或者在训练数据中进行模式识别，以建立一个预测模型，就像科学家所做的那样（基于实例学习 vs基于模型学习）



## 2.1 是否有监督

第一种分类机器学习的方法是可以根据训练时监督的量和类型进行分类。主要有四类：监督学习、非监督学习、半监督学习和强化学习。

### 2.1.1 监督学习

监督学习，顾名思义就是带有监督的学习，**而监督就是体现在训练数据都是有标签的**，所有在训练模型的时候可以根据数据的真实标签不断调整模型，从而得到一个性能更好的模型。

监督学习主要有两个常见的典型的任务--**分类和回归**。

#### 分类

分类问题主要就是预测新数据的类别问题。例如上文提到的垃圾邮件过滤器就是一个二分类问题，将邮件分为垃圾邮件还是正常的邮件，如下图所示。

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/%E5%88%86%E7%B1%BB%E7%A4%BA%E4%BE%8B.png)





#### 回归

回归问题主要是预测目标数值。比如给定预测房价的问题，给定一些特征，如房子大小、房间数量、地理位置等等，然后预测房子的价格。如下图所示：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/%E5%9B%9E%E5%BD%92%E7%A4%BA%E4%BE%8B.png)

注意，一些回归算法也可以用来进行分类，反之亦然。例如，逻辑回归通常用来进行分类，它可以生成一属于每个类别的概率值，然后选择最大概率的类别作为预测的类别。

常用的监督学习算法有：

- K近邻算法
- 线性回归
- 逻辑回归
- 支持向量机（SVM）
- 决策树和随机森林
- 深度学习方法



### 2.1.2 非监督学习

和监督学习相反，非监督学习就是采用没有标签的数据集。

非监督主要有四个典型的任务，分别是聚类、降维、异常检测和关联规则学习。



#### 聚类

聚类就是将数据根据一定的规则分成多个类，通常是采用相似性。比如对于博客访客的聚类，通过聚类算法，检测相似性访客的分组，如下图所示。不需要告诉算法访客是哪个类别，它会自动根据访客的属性找到相互间的关系，比如它可能找出访客的职业关系，将访客分为有 40% 的是上班族，有 50% 的是学生，或者对于技术博客，可能就是根据开发方向，划分为前端、后台、移动开发、人工智能等等。甚至，如果采用层次聚类分析算法，还可以继续对上述的分类进行更加详细的划分。这种做法可以帮助博主知道自己博客的主要群体是谁，更好规划自己博客发表的文章应该以什么方向为主。

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/cluster_example.png)

可视化算法也是极佳的非监督学习案例：**给算法大量复杂的且不加标签的数据，算法输出数据的2D或3D图像**。如下图所示，算法会试图保留数据的结构（即尝试保留输入的独立聚类，避免在图像中重叠），这样就可以明白数据是如何组织起来的，也许还能发现隐藏的规律。

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/visualize_example.png)

#### 降维

**降维的目的是简化数据、但是不能失去大部分信息**。做法之一是**合并若干相关的特征**。例如，汽车的里程数与车龄高度相关，降维算法就会将它们合并成一个，表示汽车的磨损。这叫做特征提取。

此外，在采用机器学习算法训练的时候，可以对训练集进行降维，这样有助于提高训练速度，降低占用的硬盘和内存空间，有时候也能提高算法的性能，但必须选择合适的降维算法，否则性能实际上是很有可能会下降的。



#### 异常检测

另一个重要的非监督任务是异常检测（anomaly detection）。例如，检测异常的信用卡转账以防欺诈，检测制造缺陷，或者在训练之前自动从训练数据集去除异常值。异常检测的系统使用正常值训练的，当它碰到一个新实例，它可以判断这个新实例是像正常值还是异常值。

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/anomaly%20detection.png)

#### 关联规则学习

最后，另一个常见的非监督任务是关联规则学习，它的目标是挖掘大量数据以发现属性间有趣的关系。例如，假设你拥有一个超市。在销售日志上运行关联规则，可能发现买了烧烤酱和薯片的人也会买牛排。因此，你可以将这些商品放在一起。




下面是一些最重要的非监督学习算法：

1. 聚类
   - K 均值（k-means）
   - 层次聚类分析（Hierarchical Cluster Analysis, HCA）
   - 期望最大值
2. 可视化和降维
   - 主成分分析（Principal	Component Analysis, PCA）
   - 核主成分分析
   - 局部线性嵌入（Locally-Linear Embedding, LLE）
   - t-分布邻域嵌入算法（t-distributed Stochastic Neighbor Embedding, t-SNE）
3. 关联性规则学习
   - Apriori 算法
   - Eclat算法





### 2.1.3 半监督学习

一些算法可以处理部分带标签的训练数据，通常是大量不带标签数据加上小部分带标签数据。这称作半监督学习。如下图所示，图中灰色圆点表示没有标签的数据，仅有几个三角形和正方形点表示带标签的数据。

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/semi-supriviesed_learning.png)

**多数半监督学习算法是非监督和监督算法的结合**。

例如，深度信念网络（deep belief networks）是基于被称为互相叠加的受限玻尔兹曼机（restricted Boltzmann machines，RBM）的非监督组件。RBM 是先用非监督方法进行训练，再用监督学习方法进行整个系统微调。

半监督学习的示例，如一些图片存储服务，比如 Google Photos，是半监督学习的好例子。一旦你上传了所有家庭相片，它就能自动识别相同的人 A 出现了相片1、5、11	中，另一个人	B 出现在了相片 2、5、7 中。这是算法的非监督部分（聚类）。现在系统需要的就是你告诉这两个人是谁。只要给每个人一个标签，算法就可以命名每张照片中的每个人，特别适合搜索照片。



常见应用场景：应用场景包括分类和回归，算法包括一些对常用监督式学习算法的延伸，通过对已标记数据建模，在此基础上，对未标记数据进行预测。



算法举例：常见算法如图论推理算法（Graph Inference）或者拉普拉斯支持向量机（Laplacian SVM）等。





### 2.1.4 强化学习

强化学习和上述三种学习问题是非常不同的。学习系统在这里被称为**智能体**（ agent），可以对环境进行观察，选择和执行动作，获得**奖励**（负奖励是惩罚，见下图）。然后它必须自己学习哪个是最佳方法（称为**策略**，policy），以得到长久的最大奖励。策略决定了智能体在给定情况下应该采取的行动 。

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/Reinforcement_learning.png)



目前强化学习的应用还不算非常广，特别是结合了深度学习的强化学习，主要是应用在机器人方面，当然最著名的一个应用就是 DeepMind 的 AlphaGo 了，它是通过分析数百万盘棋局学习制胜策略，然后自己和自己下棋。要注意，在比赛中机器学习是关闭的；AlphaGo	只是使用它学会的策略。 



## 2.2 是否可以动态渐进学习

第二种分类机器学习的准则是，它是否能从导入的数据流进行持续学习。也就是如果导入的是持续的数据流，机器学习算法能否在不断采用新数据来训练已经训练好的模型，并且新的模型对新旧数据都还有很好的性能。



### 2.2.1 批量学习

在批量学习中，**系统不能进行持续学习：必须用所有可用数据进行训练**。这通常会占用大量时间和计算资源，所以一般是线下做的。

首先是进行训练，然后部署在生产环境且停止学习，它只是使用已经学到的策略。**这称为离线学习**。

对于批量学习算法来说，当获取到新数据的时候，就需要重新重头训练整个数据集，然后更新模型，如果是应用该算法系统，那就相当于需要更新系统，需要停掉旧版本的系统，重新上线新版本的系统。

当然，一般训练、评估、部署一套机器学习的系统的整个过程可以自动进行，所以即便是批量学习也可以适应改变。只要有需要，就可以方便地更新数据、训练一个新版本。并且对于更新周期，可以选择每 24 小时或者每周更新一次。

但是，批量学习还是存在下面的缺点：

1. **实时性差**，即对于需要快速适应变化的系统，比如预测股票变化、电商推荐系统等，就不适合采用批量学习算法；
2. **耗费大量计算资源**，用全部数据训练需要大量计算资源（CPU、内存空间、磁盘空间、磁盘 I/O、网络 I/O 等等），特别是训练集特别大的情况，更加凸显这个问题的严峻性；
3. **无法应用在资源有限的设备上**，比如需要自动学习的系统，但是如果采用智能手机，每次采用大量训练数据重新训练几个小时是非常不实际的。



### 2.2.2 在线学习

批量学习的缺陷和问题可以通过采用在线学习算法来解决。

在在线学习中，是用数据实例持续地进行训练，可以一次一个或一次几个实例（称为小批量）。每个学习步骤都很快且廉价，所以系统可以动态地学习到达的新数据。

在线学习虽然名字带着在线两个字，但是实际上它的训练过程也是离线的，因此应该说是持续学习或者增量学习。

在线学习有下面几个优点：

1. **实时性好**。在线学习算法非常适合接收连续流的数据，然后自动更新模型，实时性比批量学习更好；
2. **可以节省大量计算资源**。在线学习算法在学习新数据后，可以扔掉训练数据，从而节省大量存储空间；此外，训练得过程不需要加载所有训练数据，对于内存、CPU 等资源的要求也大大减少；
3. **实现核外学习**(out-of-core learning)。当内存不足以加载训练集的时候，可以采用在线学习算法多次训练，每次加载一部分训练集，即将一部分训练集当做新数据不断加载，直到训练完所有数据。

在线学习也存在两个挑战：

1. **学习速率问题**。学习速率是在线学习的一个重要参数，它反映了在线学习算法有多快地适应数据的改变，必须选择一个合适的学习速率，因为学习速率过大，系统可以很快适应新数据，但是也容易遗忘旧数据，比如图像分类问题，训练了一个 50 类分类器后，增加新的 10 类数据，一旦学习速率过快，系统只会记住新的 10 个类别，忘记了前面的 50 个类别的数据。相反的，如果你设定的学习速率低，系统的惰性就会强：即，它学的更慢，但对新数据中的噪声或没有代表性的数据点结果不那么敏感。
2. **坏数据的影响**。如果采用坏数据训练，会破坏系统的性能。要减小这种风险，你需要密集监测，如果检测到性能下降，要快速关闭（或是滚回到一个之前的状态）。你可能还要监测输入数据，对反常数据做出反应（比如，使用异常检测算法）。



## 2.3 实例学习 vs 模型学习

第三种分类机器学习的方法是判断它们是如何进行归纳推广的。大多机器学习任务是关于预测的。这意味着给定一定数量的训练样本，系统需要能推广到之前没见到过的样本。对训练数据集有很好的性能还不够，真正的目标是对新实例预测的性能。

有两种主要的归纳方法：基于实例学习和基于模型学习。

### 2.3.1 实例学习

基于实例学习是**系统先用记忆学习案例，然后使用相似度测量推广到新的例子**，如下图所示：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/BaseOnInstanceLearning.png)

这种学习算法可以说是机器学习中最简单的算法了，**它实际上就是采用存储的数据集进行分类或者回归**。

**典型的算法就是 KNN 算法**，即 K 近邻算法，它就是将新的输入数据和已经保存的训练数据采用相似性度量（一般采用欧式距离）得到最近的 K 个训练样本，并采用 K 个训练样本中类别出现次数最多的类别作为预测的结果。

所以，这种算法的缺点就比较明显了：

- **对存储空间的需求很大**，需要占用的空间直接取决于实例数量的大小；
- **运行时间比较慢**，因为需要需要与已知的实例进行比对。



### 2.3.2 模型学习

和基于实例学习相反的就是基于模型学习：建立这些样本的模型，然后使用这个模型进行预测。如下图所示：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/BaseOnModelLearning.png)


基于模型学习算法的流程一般如下所示：

- 研究数据。先对数据进行分析，这可能包含清洗数据、特征筛选、特征组合等等
- 选择模型。选择合适的模型，从简单的线性回归、逻辑回归，到慢慢复杂的随机森林、集成学习，甚至深度学习的卷积神经网络模型等等
- 用训练数据进行训练。也就是寻找最适合算法模型的参数，使得代价函数取得最小值。
- 使用模型对新案例进行预测（这称作推断）。预测结果非常好，就能上线系统；如果不好，就需要进行错误分析，问题出现在哪里，是数据问题还是模型问题，找到问题，然后继续重复这个流程。





# 3. 如何构建一个机器学习项目

一般我们构建一个机器学习项目，常用的还是监督学习的算法，所以这里按照监督学习算法来简单说明如何构造一个机器学习项目。

通常的步骤是这样的：

1. **项目概述**。明确项目的目标，需要用什么性能度量，然后确定问题的类别（监督学习、非监督学习或者需要批量学习还是在线学习方法），大概确定采用的算法；

2. **获取数据**。获取公开的相关领域的数据集，准备开发环境，测试集一般就是采用特定的数据集，比如业务给出的实际应用场景的数据。

3. **发现并可视化数据，发现规律**。对数据进行探索，查找数据间存在的一些关联和规律。

4. **为机器学习算法准备数据**。这一步其实就是做好特征工程，对原始数据进行预处理、特征提取、特征选择等步骤。

5. **模型训练**。选择好合适的机器学习模型，然后进行训练，通常可能会考虑选择多个模型进行对比，然后挑选合适的算法模型。

6. **微调模型**。对选择好的模型进行调参，尽量得到最佳的性能。

7. **给出解决方案**。这一步主要是展示给你的上级，你要采用什么算法模型，实验结果如何等等。

8. **部署、监控、维护系统**。最后就是部署模型，上线，然后监控服务的运行，并进行维护。

   

# 4. 机器学习的主要挑战

在介绍基于模型学习算法的流程的时候，对于预测结果不好的问题分析，主要说了是数据问题还是模型问题，这同时也就是机器学习的效果不好的两个主要原因，即错误的数据和错误的算法。

## 4.1 训练数据量不足

第一个问题就是**训练数据的数量问题**，这是非常重要的问题。

因为即使是简单的问题，一般也需要数千的样本，这还是因为简单的问题一般采用简单的算法就可以解决，对于复杂的图像或语音问题，通常需要数百万的样本，特别是如果采用现在非常热门的深度学习算法，比如卷积神经网络模型，这些复杂的模型如果没有足够的数据量支持，非常容易陷入过拟合的情况。

实际上更多数量的训练集也是为了获得更有代表性的数据，能够学习到这类数据的所有特征。

但是，应该注意到，**小型和中型的数据集仍然是非常常见的，获得额外的训练数据并不总是轻易和廉价的，所以不要抛弃算法**。



## 4.2 没有代表性的训练数据

无论采用基于实例还是基于模型的学习，让训练数据对新数据具有代表性是非常重要的。如果训练集没有代表性，那么训练得到的模型就是不可能得到准确性的模型，比如人脸识别中，模型没有学习到某个人最明显的代表性的特征，比如高鼻梁或者没有眉毛等突出特征，那么模型对这个人的识别率就不会很高。

使用具有代表性的训练集对于推广到新案例是非常重要的。但是做起来比说起来要难：**如果样本太小，就会有样本噪声（即会有一定概率包含没有代表性的数据），但是即使是非常大的样本也可能没有代表性，如果取样方法错误的话。这叫做样本偏差。**




## 4.3 低质量的数据

低质量的数据指的是**数据有错误、带有过多噪声或者是出现异常值等的数据，这种数据会影响系统整体的性能**，因此，**数据清洗**对于构建一个机器学习系统或者一个机器学习项目来说都是必不可少的步骤。

对于这些低质量的数据，通常可以按照如下做法处理：

- 如果一些实例是明显的异常值，最好删掉它们或尝试手工修改错误；
- 如果一些实例缺少特征（比如，你的 5% 的顾客没有说明年龄），你必须决定是否忽略这个属性、忽略这些实例、填入缺失值（比如，年龄中位数），或者训练一个含有这个特征的模型和一个不含有这个特征的模型，等等。



## 4.4 不相关的特征

不相关的特征对于整个机器学习系统是有着反作用的效果，训练数据必须包含足够多的相关特征且非相关特征不多的情况下，才能训练出一个性能不错的模型。机器学习项目成功的关键之一是用好的特征进行训练。这个过程称作**特征工程**，包括：

- 特征选择：在所有存在的特征中选取最有用的特征进行训练。
- 特征提取：组合存在的特征，生成一个更有用的特征（如前面看到的，可以使用降维算法）。
- 收集新数据创建新特征。



## 4.5 过拟合

上述四种情况都是坏数据的情况，接下来是两种算法问题，也是机器学习最常见的两种算法方面的问题，**过拟合和欠拟合**。

**过拟合就是指算法模型在训练集上的性能非常好，但是泛化能力很差，即在测试集上的效果却很糟糕的情况**。比如下图，采用一个高阶多项式回归模型来预测生活满意度和人均 GDP 的关系，很明显看出来，这个模型过拟合了训练数据，其预测效果并不会达到在训练数据上这么好的效果。

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/OverfittingExample.png)

通常对于比较复杂的模型，比如深度神经网络，它能够检测和识别到数据中比较细微的规律和特征，但是如果训练集包含噪声，或者训练集数量太少（数量太少会引入样本噪声），这种情况下，模型同样会学习这种噪声，从而导致模型的泛化能力的下降。

一般解决过拟合的方法有：

- 简化模型，这包括了采用简单点的模型、减少特征数量以及限制模型，即采用正则化；
- 增加训练数据
- 减小训练数据的噪声，即数据清洗，比如修正数据错误和去除异常值等

其中**正则化方法是比较常用的方法，它的作用就是限制模型，不让模型过于复杂，从而降低过拟合的风险或者是缓和过拟合的程度**。常用的正则化方法是 **L2 和 L1 正则化**。正则化方法通常会采用一个超参数来控制其限制模型的强度。超参数是一个学习算法的参数（而不是模型的）。这样，它是不会被学习算法本身影响的，它优于训练，在训练中是保持不变的。如何调节超参数也是构建一个机器学习算法模型非常重要的一个步骤，也是让性能能够进一步提升的做法。



## 4.6 欠拟合

欠拟合和过拟合刚好相反，**它就是模型的性能非常差，在训练数据和测试数据上的性能都不好。**

通常也是因为模型过于简单，没有能够很好学习到数据的有效的相关的特征，解决方法有：

- 选择一个更强大的模型，带有更多参数
- 用更好的特征训练学习算法（特征工程）
- 减小对模型的限制（比如，减小正则化超参数）



# 5. 常用的分类算法优缺点

这里给出常用的分类算法的优缺点：

|            算法             | 优点                                                         | 缺点                                                         |
| :-------------------------: | :----------------------------------------------------------- | :----------------------------------------------------------- |
|          线性回归           | 1. 结果易于理解；<br/>2. 容易实现，计算不复杂                | 对非线性数据的拟合效果不好                                   |
| Logistic Regression逻辑回归 | 1. 速度快。<br />2. 简单易于理解，直接看到各个特征的权重。<br />3. 能容易地更新模型吸收新的数据。<br />4. 如果想要一个概率框架，动态调整分类阀值。<br/>5. 实现简单，广泛应用于工业问题上 | 1. 特征处理复杂。<br/>2. 只能处理二分类问题（需要 softmax 才可以处理多分类），且必须线性可分。<br />3. 容易欠拟合，一般准确率不太高。<br />4. 不能很好处理大量多类特征或者变量。<br />5. 特征空间很大时，性能一般。 |
|     Decision Tree决策树     | 1. 不需要任何领域知识或参数假设。<br />2. 适合高维数据。<br />3. 可解释性强，简单易于理解。<br />4. 效率高，短时间内可以处理大量数据，得到可行且效果较好的结果。<br />5. 能够同时处理数据型和常规性属性。 | 1. 对于各类别样本数量不一致数据，信息增益偏向于那些具有更多数值的特征。<br />2. 易于过拟合。<br />3. 忽略属性之间的相关性。<br />4. 不支持在线学习。<br />5. 单棵决策树分类能力弱，并且对连续值变量难以处理。 |
|    随机森林 RandomForest    | 1. 在数据集上表现良好，在当前的很多数据集上，相对其他算法有着很大的优势。<br  /> 2. 它能够处理很高维度（特征很多）的数据，并且不用做特征选择。<br />3. 可以评估特征的重要性<br  />4. 在创建随机森林的时候，对 generlization error 使用的是无偏估计<br  />5. 训练速度快，容易做成并行化方法<br  />6. 在训练过程中，能够检测到特征间的互相影响 <br  />7. 实现比较简单<br  />8. 对于不平衡的数据集来说，它可以平衡误差<br  /> 9. 可以应用在特征缺失的数据集上，并仍然有不错的性能 | 1. 随机森林已经被证明在某些**噪音较大**的分类或回归问题上会过拟合。<br />2. 对于有不同取值的属性的数据，**取值划分较多的属性会对随机森林产生更大的影响**，所以随机森林在这种数据上产出的属性权值是不可信的。 |
|        SVM支持向量机        | 1. 可以解决小样本下机器学习的问题。<br />2. 提高泛化性能。<br />3. 可以解决高维、非线性问题。超高维文本分类仍受欢迎。<br />4. 避免神经网络结构选择和局部极小的问题。 | 1. 对缺失数据或者噪音敏感。<br />2. 内存消耗大，难以解释。<br />3. 运行和调参略烦人。<br />4. 对大规模数据训练比较困难。<br />5. 无法直接支持多分类，但可以通过间接的方法来实现。 |
|     Bayes 贝叶斯分类法      | 1. 所需估计的参数少，对于缺失数据不敏感。<br />2.有着坚实的数学基础，以及稳定的分类效率。 | 1. 需要假设属性之间相互独立，这往往并不成立。（喜欢吃番茄、鸡蛋，却不喜欢吃番茄炒蛋）。<br />2. 需要知道先验概率。<br />3. 分类决策存在错误率。 |
|          KNN K近邻          | 1. 思想简单，理论成熟，既可以用来做分类也可以用来做回归； <br />2. 可用于非线性分类；<br />3.训练时间复杂度为O(n)； <br />4. 准确度高，对数据没有假设，对outlier不敏感； | 1. 计算量太大。<br />2. 对于样本分类不均衡的问题，会产生误判。<br />3. 需要大量的内存。<br />4. 输出的可解释性不强。 |
|   Neural Network 神经网络   | 1. 分类准确率高。<br />2. 并行处理能力强。<br />3. 分布式存储和学习能力强。<br />4. 鲁棒性较强，不易受噪声影响。 | 1. 需要大量参数（网络拓扑、阀值、阈值）。<br />2. 结果难以解释。<br />3. 训练时间过长。 |
|         Adaboosting         | 1. adaboost是一种有很高精度的分类器。<br />2. 可以使用各种方法构建子分类器，Adaboost算法提供的是框架。<br />3. 当使用简单分类器时，计算出的结果是可以理解的。而且弱分类器构造极其简单。<br />4. 简单，不用做特征筛选。<br />5. 不用担心overfitting。 | 对outlier比较敏感                                            |
|            GDBT             | 1. 精度高。<br />2. 能处理非线性数据和多特征类型数据。<br /> 3. 适合低维稠密数据。<br /> 4. 可解释性好。<br /> 5. 不需要对特征做归一化，可以自动选择特征。<br /> 6.能采用多少损失函数，比如均方误差和 LogLoss 等 | 1. boosting 是个串行过程，并行比较麻烦，需要考虑上下树之间的关系。<br /> 2. 计算复杂度高。<br /> 3. 不适合高位稀疏数据。 |



# 6. 目标函数、代价函数、损失函数

参考：

-  https://www.zhihu.com/question/358635266/answer/917582302
-  https://zhuanlan.zhihu.com/p/58883095



## 6.1 目标函数、代价函数、损失函数的定义

**损失函数（Loss Function）**：针对单个样本，**衡量单个样**本的预测值 ![[公式]](https://www.zhihu.com/equation?tex=%5Chat%7By%7D%5E%7B%28i%29%7D) 与真实值 ![[公式]](https://www.zhihu.com/equation?tex=y%5E%7B%28i%29%7D) 之间的差距。

用来衡量算法的运行情况，估量模型的预测值与真实值的不一致程度，是一个非负实值函数，通常使用$
L(Y, f(x))$来表示。

损失函数越小，模型的鲁棒性就越好。损失函数是经验风险函数的核心部分，也是结构风险函数重要组成部分。

**损失函数**分为**经验风险损失函数**和**结构风险损失函数**。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。

**代价函数/成本函数（Cost Function）**：针对多个样本，**衡量多个样本**的预测值 ![[公式]](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5E%7Bn%7D+%5Chat%7By%7D%5E%7B%28i%29%7D) 与真实值 ![[公式]](https://www.zhihu.com/equation?tex=%5Csum_%7Bi%3D1%7D%5E%7Bn%7Dy%5E%7B%28i%29%7D) 之间的差距。为什么用代价函数呢，这有两个原因：

1. 为了得到训练逻辑回归模型的参数，需要一个代价函数，通过训练代价函数来得到参数。
2. 用于找到最优解的目的函数。



**目标函数（Objective Function）：**梯度下降等优化算法就是针对目标函数来进行的。其实代价函数就可以是一种目标函数，换句话说，目标函数可以直接选用代价函数。但是，我们经常给代价函数添加一个正则化项，最终作为模型的目标函数。

一般设计的目标函数要有一个下界，在优化过程当中，如果优化算法能够使目标函数不断减小，根据单调有界准则，这个优化算法就能证明是收敛有效的。而代价函数非负更为方便。





## 6.2 常见的损失函数

### 6.2.1 平方损失函数(quadratic)

平方损失函数的公式如下所示：
$$
L(Y, f(x)) = \sum_N (Y-f(x))^2
$$

其中 Y 表示真实标签，f(x) 表示输出值。

一般常用于回归问题。



### 6.2.2 0-1 损失函数

如果预测值和目标值相等，值为0，如果不相等，值为1。
$$
L(Y, f(x)) =
\begin{cases}
1,& Y\ne f(x)\\
0,& Y = f(x)
\end{cases}
$$

特点：

1. 0-1损失函数直接对应分类判断错误的个数，但是它是一个非凸函数，不太适用；

2. **感知机**就是用的这种损失函数。但是相等这个条件太过严格，因此可以放宽条件，即满足 ![[公式]](https://www.zhihu.com/equation?tex=%7CY+-+f%28x%29%7C+%3C+T) 时认为相等：

$$
L(Y, f(x)) =
\begin{cases}
1,& |Y-f(x)|\geqslant T\\
0,& |Y-f(x)|< T
\end{cases}
$$





### 6.2.3 绝对值损失函数

和0-1损失函数相似，绝对值损失函数表示为：
$$
L(Y, f(x)) = |Y-f(x)|
$$



### 6.2.4 对数损失函数

公式如下所示：
$$
L(Y, P(Y|X)) = -\log{P(Y|X)}=-\frac{1}{N}\sum_{i=1}^N\sum_{j=1}^M y_{ij}log(p_{ij})
$$

其中, Y 为输出变量, X为输入变量, L 为损失函数. N为输入样本量, M为可能的类别数, $y_{ij}$ 是一个二值指标, 表示类别 j 是否是输入实例 xi 的真实类别. $p_{ij}$ 为模型或分类器预测输入实例 xi 属于类别 j 的概率.

**常见的逻辑回归使用的就是对数损失函数**。逻辑回归它假设样本服从伯努利分布（0-1分布），进而求得满足该分布的似然函数，接着取对数求极值等。

逻辑回归推导出的**经验风险函数是最小化负的似然函数**，从损失函数的角度看，就是对数损失函数。**形式上等价于二分类的交叉熵损失函数**。



### 6.2.5 指数损失函数

指数损失函数的标准形式为：
$$
L(Y, f(x)) = \exp(-yf(x))
$$

对离群点、噪声非常敏感。经常用在AdaBoost算法中。



### 6.2.6 Hinge损失函数

Hinge损失函数的标准形式如下：
$$
L(y) = \max{(0, 1-ty)}
$$

统一的形式：
$$
L(Y, f(x)) = \max{(0, Yf(x))}
$$

其中y是预测值，范围为(-1,1)，t为目标值，其为-1或1。

特点：

(1)hinge损失函数表示如果被分类正确，损失为0，否则损失就为 ![[公式]](https://www.zhihu.com/equation?tex=1-yf%28x%29) 。**SVM**就是使用这个损失函数。

(2)一般的 ![[公式]](https://www.zhihu.com/equation?tex=f%28x%29) 是预测值，在-1到1之间， ![[公式]](https://www.zhihu.com/equation?tex=y) 是目标值(-1或1)。其含义是， ![[公式]](https://www.zhihu.com/equation?tex=f%28x%29+) 的值在-1和+1之间就可以了，并不鼓励 ![[公式]](https://www.zhihu.com/equation?tex=%7Cf%28x%29%7C+%3E+1) ，即并不鼓励分类器过度自信，让某个正确分类的样本距离分割线超过1并不会有任何奖励，从而**使分类器可以更专注于整体的误差。**

(3) **健壮性相对较高，对异常点、噪声不敏感，但它没太好的概率解释**。



在线性支持向量机中，最优化问题可等价于

$$
\underset{\min}{w,b}\sum_{i=1}^N (1-y_i(wx_i+b))+\lambda\Vert w\Vert ^2
$$

上式相似于下式

$$
\frac{1}{m}\sum_{i=1}^{N}l(wx_i+by_i) + \Vert w\Vert ^2
$$

其中$l(wx_i+by_i)$是Hinge损失函数，$\Vert w\Vert ^2$可看做为正则化项。



### 6.2.7 **感知损失(perceptron loss)函数**

**感知损失函数**的标准形式如下：

![[公式]](https://www.zhihu.com/equation?tex=L%28y%2C+f%28x%29%29+%3D+max%280%2C+-f%28x%29%29++%5C%5C)

特点：

- 它是Hinge损失函数的一个变种，Hinge loss对判定边界附近的点(正确端)惩罚力度很高。而perceptron loss**只要样本的判定类别正确的话，它就满意，不管其判定边界的距离**。

- 它比Hinge loss简单，因为不是max-margin boundary，所以模**型的泛化能力没 hinge loss强**。





### 6.2.8 交叉熵损失函数

形式如下所示：
$$
L = \frac{1}{n}\sum_x[ylna+(1-y)ln(1-a)]
$$
其中 $x$表示样本，$ y$表示实际值，$a$ 表示输出值，$n$表示样本的总数。

特点如下：

- 本质上也是一种**对数似然函数**，可用于二分类和多分类任务中；
- 当预测输出和实际值大的时候，即误差大，损失 L 也越大，此时对模型的”惩罚“也越大，所以权重更新也快；而误差小的时候，则权重更新慢；
- 当使用sigmoid作为激活函数的时候，常用**交叉熵损失函数**而不用**均方误差损失函数**，因为它可以**完美解决平方损失函数权重更新过慢**的问题，具有“误差大的时候，权重更新快；误差小的时候，权重更新慢”的良好性质。



**优点**：

在用梯度下降法做参数更新的时候，模型学习的速度取决于两个值：一、**学习率**；二、**偏导值**。其中，学习率是我们需要设置的超参数，所以我们重点关注偏导值。从上面的式子中，我们发现，偏导值的大小取决于 ![[公式]](https://www.zhihu.com/equation?tex=x_i) 和 ![[公式]](https://www.zhihu.com/equation?tex=%5B%5Csigma%28s%29-y%5D) ，我们重点关注后者，后者的大小值反映了我们模型的错误程度，该值越大，说明模型效果越差，但是该值越大同时也会使得偏导值越大，从而模型学习速度更快。所以，使用逻辑函数得到概率，并结合交叉熵当损失函数时，在模型效果差的时候学习速度比较快，在模型效果好的时候学习速度变慢。



**缺点**：

Softmax Loss的两个缺点：

1. 随着分类数目的增大，分类层的线性变化矩阵参数也随着增大；
2. 对于封闭集分类问题，学习到的特征是可分离的，但对于开放集人脸识别问题，所学特征却没有足够的区分性

对于人脸识别问题，首先人脸数目(对应分类数目)是很多的，而且会不断有新的人脸进来，不是一个封闭集分类问题。

另外，sigmoid(softmax)+cross-entropy loss 擅长于学习类间的信息，因为它采用了类间竞争机制，它只关心对于正确标签预测概率的准确性，忽略了其他非正确标签的差异，**导致学习到的特征比较散**。基于这个问题的优化有很多，比如对softmax进行改进，如L-Softmax、SM-Softmax、AM-Softmax等。





更多关于交叉熵损失函数，可以看这几篇文章：

- https://zhuanlan.zhihu.com/p/38241764
- https://zhuanlan.zhihu.com/p/35709485



# 7. 梯度下降

## 7.1 机器学习中为什么需要梯度下降

梯度下降是机器学习中常见优化算法之一，梯度下降法有以下几个作用：

1. 梯度下降是一种求解函数局部极小值的迭代优化算法，可以用于求解最小二乘问题。
2. 在求解机器学习算法的模型参数，即无约束优化问题时，主要有梯度下降法（Gradient Descent）和最小二乘法。
3. 在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。
4. 如果我们需要求解损失函数的最大值，可通过梯度上升法来迭代。梯度下降法和梯度上升法可相互转换。
5. 在机器学习中，梯度下降法主要有随机梯度下降法和批量梯度下降法。





## 7.2 梯度下降法缺点

梯度下降法缺点有以下几点：

（1）靠近极小值时收敛速度减慢。

（2）直线搜索时可能会产生一些问题。

（3）可能会“之字形”地下降。

梯度概念也有需注意的地方：

（1）梯度是一个向量，即有方向有大小。 

（2）梯度的方向是最大方向导数的方向。 

（3）梯度的值是最大方向导数的值。



## 7.3 梯度下降法直观理解

梯度下降法经典图示如下图2.7所示：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%A4%BA%E4%BE%8B.png)

​								

形象化举例，由上图所示，假如最开始，我们在一座大山上的某处位置，因为到处都是陌生的，不知道下山的路，所以只能摸索着根据直觉，走一步算一步，在此过程中，每走到一个位置的时候，都会求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。不断循环求梯度，就这样一步步地走下去，一直走到我们觉得已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山势低处。


由此，从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部的最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。

**核心思想归纳**：

1. 初始化参数，随机选取取值范围内的任意数；

2. 迭代操作：
   - 计算当前梯度；
   - 修改新的变量；
   - 计算朝最陡的下坡方向走一步；
   - 判断是否需要终止，如否，重新开始计算梯度，然后继续重复这个步骤；

3. 得到全局最优解或者接近全局最优解。



## 7.4 梯度下降法算法描述

梯度下降法算法步骤如下：

（1）确定优化模型的假设函数及损失函数。
举例，对于线性回归，假设函数为：
$$
  h_\theta(x_1,x_2,...,x_n)=\theta_0+\theta_1x_1+...+\theta_nx_n
$$

其中，$\theta_i,x_i(i=0,1,2,...,n)$分别为模型参数、每个样本的特征值。

对于假设函数，损失函数为：
$$
  J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum^{m}_{j=0}(h_\theta (x^{(j)}_0
  	,x^{(j)}_1,...,x^{(j)}_n)-y_j)^2
$$

（2）相关参数初始化。

主要初始化${\theta}_i$、算法迭代步长${\alpha} $、终止距离${\zeta} $。初始化时可以根据经验初始化，即${\theta} $初始化为0，步长${\alpha} $初始化为1。当前步长记为${\varphi}_i $。当然，也可随机初始化。

（3）迭代计算。

1）计算当前位置时损失函数的梯度，对${\theta}_i $，其梯度表示为：
$$
\frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{2m}\sum^{m}_{j=0}(h_\theta (x^{(j)}_0
	,x^{(j)}_1,...,x^{(j)}_n)-y_j)^2
$$

2）计算当前位置下降的距离。
$$
{\varphi}_i={\alpha} \frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)
$$

3）判断是否终止。
确定是否所有${\theta}_i$梯度下降的距离${\varphi}_i$都小于终止距离${\zeta}$，如果都小于${\zeta}$，则算法终止，当然的值即为最终结果，否则进入下一步。

4）更新所有的${\theta}_i$，更新后的表达式为：
$$
{\theta}_i={\theta}_i-\alpha \frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)
$$

$$
\theta_i=\theta_i - \alpha \frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{(j)}_0
	,x^{(j)}_1,...,x^{(j)}_n)-y_j)x^{(j)}_i
$$

5）令上式$x^{(j)}_0=1$，更新完毕后转入1)。

由此，可看出，当前位置的梯度方向由所有样本决定，上式中 $\frac{1}{m}$、$\alpha \frac{1}{m}$ 的目的是为了便于理解。



## 7.5 如何对梯度下降法进行调优

实际使用梯度下降法时，各项参数指标不能一步就达到理想状态，对梯度下降法调优主要体现在以下几个方面：

### 7.5.1 学习率$\alpha$选择

在算法参数初始化时，有时根据经验将学习率初始化为1。实际取值取决于数据样本。可以从大到小，多取一些值，分别运行算法看迭代效果，如果损失函数在变小，则取值有效。如果取值无效，说明要增大步长。但步长太大，有时会导致迭代速度过快，错过最优解。步长太小，迭代速度慢，算法运行时间长。如下图所示分别是不同的学习率情况：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%8E%87.png)

- a 图的学习率最优，模型成功收敛到最小值；
- b 图的学习率太小，需要花费更多时间，但最终是收敛到最小值；
- c 图的学习率高于最优值，也是以较慢速度来收敛；
- d 图的学习率过大，所以会过度偏离，远离了最小值，无法收敛。

对于不同的学习率，随着训练次数的增加，loss 增加的情况如下所示：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%8E%872.png)







### 7.5.2 参数的初始值选择

初始值不同，获得的最小值也有可能不同，梯度下降有可能得到的是局部最小值。**如果损失函数是凸函数，则一定是最优解**。

由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。

一般对于不带约束条件的优化问题，可以将初始值设置为 0，或者随机数。而对于神经网络，则一般设置为随机数。



### 7.5.3 标准化处理

由于样本不同，特征取值范围也不同，导致迭代速度慢。为了减少特征取值的影响，可对特征数据标准化，使新期望为0，新方差为1，**可节省算法运行时间**。



## 7.6 随机梯度和批量梯度区别

随机梯度下降（SGD）和批量梯度下降（BGD）是两种主要梯度下降法，其目的是增加某些限制来加速运算求解。
下面通过介绍两种梯度下降法的求解思路，对其进行比较。
假设函数为：
$$
h_\theta (x_0,x_1,...,x_3) = \theta_0 x_0 + \theta_1 x_1 + ... + \theta_n x_n
$$
损失函数为：
$$
J(\theta_0, \theta_1, ... , \theta_n) = 
			\frac{1}{2m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
	,x^{j}_1,...,x^{j}_n)-y^j)^2
$$
其中，$m$为样本个数，$j$为参数个数。



1、**批量梯度下降的求解思路如下：**
a) 得到每个$ \theta $对应的梯度：
$$
\frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{m}\sum^{m}_{j=0}(h_\theta (x^{j}_0
	,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i
$$
b) 由于是求最小化风险函数，所以按每个参数 $ \theta $ 的梯度负方向更新 $ \theta_i $ ：
$$
\theta_i=\theta_i - \frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
	,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i
$$
c) 从上式可以注意到，它得到的虽然是一个全局最优解，但每迭代一步，都要用到训练集所有的数据，**如果样本数据很大，这种方法迭代速度就很慢**。

相比而言，随机梯度下降可避免这种问题。



2、**随机梯度下降的求解思路如下：**
a) 相比批量梯度下降对应所有的训练样本，随机梯度下降法中损失函数对应的是**训练集中每个样本的粒度**。

损失函数可以写成如下这种形式，
$$
J(\theta_0, \theta_1, ... , \theta_n) = 
			\frac{1}{m} \sum^{m}_{j=0}(y^j - h_\theta (x^{j}_0
			,x^{j}_1,...,x^{j}_n))^2 = 
			\frac{1}{m} \sum^{m}_{j=0} cost(\theta,(x^j,y^j))
$$
b）对每个参数 $ \theta$ 按梯度方向更新 $ \theta$：
$$
\theta_i = \theta_i + (y^j - h_\theta (x^{j}_0, x^{j}_1, ... ,x^{j}_n))
$$
c) 随机梯度下降是通过每个样本来迭代更新一次。

随机梯度下降伴随的一个问题是噪音较批量梯度下降要多，使得随机梯度下降并不是每次迭代都向着整体最优化方向。



**小结：**
随机梯度下降法、批量梯度下降法相对来说都比较极端，简单对比如下：

|     方法     | 特点                                                         |
| :----------: | :----------------------------------------------------------- |
| 批量梯度下降 | a）采用所有数据来梯度下降。<br/>b）批量梯度下降法在样本量很大的时候，训练速度慢。 |
| 随机梯度下降 | a）随机梯度下降用一个样本来梯度下降。<br/>b）训练速度很快。<br />c）随机梯度下降法仅仅用一个样本决定梯度方向，导致解有可能不是全局最优。<br />d）收敛速度来说，随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解，效率会比较低。 |



下面介绍能结合两种方法优点的小批量梯度下降法。

3、 **小批量（Mini-Batch）梯度下降的求解思路如下**

对于总数为$m$个样本的数据，根据样本的数据，选取其中的$n(1< n< m)$个子样本来迭代。其参数$\theta$按梯度方向更新$\theta_i$公式如下：
$$
\theta_i = \theta_i - \alpha \sum^{t+n-1}_{j=t}
		( h_\theta (x^{j}_{0}, x^{j}_{1}, ... , x^{j}_{n} ) - y^j ) x^{j}_{i}
$$



首先，如果训练集较小，直接使用 **batch** 梯度下降法，这里的少是说小于 2000 个样本。一般的 **mini-batch** 大小为 64 到 512，考虑到电脑内存设置和使用的方式，如果 **mini-batch** 大小是 2 的𝑛次方，代码会运行地快一些。

对于调节 Batch 大小对模型的影响：

- Batch_Size 太小，模型表现效果极其糟糕(error飙升)。
- 随着 Batch_Size 增大，处理相同数据量的速度越快。
- 随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。
- 由于上述两种因素的矛盾， Batch_Size 增大到某个时候，**达到时间上的最优**。
- 由于最终收敛精度会陷入**不同的局部极值**，因此 Batch_Size 增大到某些时候，**达到最终收敛精度上的最优**。



## 7.7 各种梯度下降法性能比较

下表简单对比随机梯度下降（SGD）、批量梯度下降（BGD）、小批量梯度下降（Mini-batch GD）、和Online GD的区别：

|                |    BGD     |   SGD    | Mini-batch GD |   Online GD    |
| :------------: | :--------: | :------: | :-----------: | :------------: |
|     训练集     |    固定    |   固定   |     固定      |    实时更新    |
| 单次迭代样本数 | 整个训练集 | 单个样本 | 训练集的子集  | 根据具体算法定 |
|   算法复杂度   |     高     |    低    |     一般      |       低       |
|     时效性     |     低     |   一般   |     一般      |       高       |
|     收敛性     |    稳定    |  不稳定  |    较稳定     |     不稳定     |

BGD、SGD、Mini-batch GD，前面均已讨论过，这里介绍一下Online GD。

Online GD 与 Mini-batch GD/SGD的区别在于，**所有训练数据只用一次，然后丢弃**。这样做的优点在于**可预测最终模型的变化趋势**。

Online GD 在互联网领域用的较多，比如搜索广告的点击率（CTR）预估模型，网民的点击行为会随着时间改变。用普通的BGD算法（每天更新一次）一方面耗时较长（需要对所有历史数据重新训练）；另一方面，无法及时反馈用户的点击行为迁移。而 Online GD 算法可以**实时的依据网民的点击行为进行迁移**。



## 7.8 面临的问题

梯度下降法在实现的时候也会遇到一些问题，比较典型的就是局部极小值和鞍点问题。

局部极小值是指有的函数可能有多个局部极小值，如下图所示：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/%E5%B1%80%E9%83%A8%E6%9E%81%E5%B0%8F%E5%80%BC.png)

上图存在 3 个局部极小值，但 A 才是全局极小值，但是梯度下降可能会在 B 或者 C 点就停止了。



鞍点是指梯度为0，Hessian矩阵既不是正定也不是负定，即不定的点。如下图所示：

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/%E9%9E%8D%E7%82%B9%E5%9B%BE%E7%A4%BA.png" style="zoom:50%;" />

在鞍点的位置，梯度下降法可能认为已经找到了全局最小值，然后停止了迭代。

怎么解决上述两个问题呢，在后续的优化算法中有了一些解决办法，比如 Adam、AdaDelta等方法。这部分在后续优化算法中会提到。





------

### 参考

1. 深度学习 500 问：https://github.com/scutan90/DeepLearning-500-questions
2. Andrew Ng 在 Coursea 上的 [机器学习课程](https://www.coursera.org/learn/machine-learning)
3. [梯度下降算法的工作原理](https://mp.weixin.qq.com/s/2hO5LHOOVgxRvmJpAF1lvA)
4. [推荐收藏 | Dropout、梯度消失/爆炸、Adam优化算法，神经网络优化算法看这一篇就够了](https://mp.weixin.qq.com/s/HbQXB4NajXAAfUeglDe8XA)
5. [一文读懂梯度下降法](https://mp.weixin.qq.com/s/vPraRVs5Y6qzIe8-YogNfQ)









# 8. 模型评估

在机器学习领域中，对模型的评估非常重要，只有选择和问题相匹配的评估方法，才能快速发现算法模型或者训练过程的问题，迭代地对模型进行优化。

模型评估主要分为离线评估和在线评估两个阶段。并且针对分类、回归、排序、序列预测等不同类型的机器学习问题，评估指标的选择也有所不同。

模型评估这部分会介绍以下几方面的内容：

- 性能度量
- 模型评估方法
- 泛化能力
- 过拟合、欠拟合
- 超参数调优



## 8.1 性能度量

性能度量就是指对模型泛化能力衡量的评价标准。

### 8.1.1 准确率和错误率

分类问题中最常用的两个性能度量标准--准确率和错误率。

**准确率**：指的是分类正确的样本数量占样本总数的比例，定义如下：
$$
Accuracy = \frac{n_{correct}}{N}
$$
**错误率**：指分类错误的样本占样本总数的比例，定义如下：
$$
Error = \frac{n_{error}}{N}
$$
错误率也是损失函数为 0-1 损失时的误差。

这两种评价标准是分类问题中最简单也是最直观的评价指标。但它们都存在一个问题，在类别不平衡的情况下，它们都无法有效评价模型的泛化能力。即如果此时有 99% 的负样本，那么模型预测所有样本都是负样本的时候，可以得到 99% 的准确率。

这种情况就是**在类别不平衡的时候，占比大的类别往往成为影响准确率的最主要因素**！

这种时候，其中一种解决方法就是更换评价指标，比如采用更为有效的平均准确率(**每个类别的样本准确率的算术平均**)，即：
$$
A_{mean}=\frac{a_1+a_2+\dots+a_m}{m}
$$
其中 m 是类别的数量。

对于准确率和错误率，用 Python 代码实现如下图所示：

```python
def accuracy(y_true, y_pred):
    return sum(y == y_p for y, y_p in zip(y_true, y_pred)) / len(y_true)

def error(y_true, y_pred):
    return sum(y != y_p for y, y_p in zip(y_true, y_pred)) / len(y_true)
```

一个简单的二分类测试样例：

```python
y_true = [1, 0, 1, 0, 1]
y_pred = [0, 0, 1, 1, 0]

acc = accuracy(y_true, y_pred)
err = error(y_true, y_pred)
print('accuracy=', acc)
print('error=', err)
```

输出结果如下：

```
accuracy= 0.4
error= 0.6
```



### 8.1.2 精确率、召回率、P-R 曲线和 F1

#### 8.1.2.1 精确率和召回率

精确率，也被称作查准率，是指**所有预测为正类的结果中，真正的正类的比例**。公式如下：
$$
P = \frac{TP}{TP+FP}
$$
召回率，也被称作查全率，是指所有正类中，被分类器找出来的比例。公式如下：
$$
R = \frac{TP}{TP+FN}
$$
对于上述两个公式的符号定义，是在二分类问题中，我们将关注的类别作为正类，其他类别作为负类别，因此，定义：

- `TP(True Positive)`：真正正类的数量，即分类为正类，实际也是正类的样本数量；
- `FP`(False Positive)：假正类的数量，即分类为正类，但实际是负类的样本数量；
- `FN(False Negative)`：假负类的数量，即分类为负类，但实际是正类的样本数量；
- `TN(True Negative)`：真负类的数量，即分类是负类，实际也负类的样本数量。

更形象的说明，可以参考下表，也是**混淆矩阵**的定义：

|            | 预测：正类 | 预测：负类 |
| :--------: | :--------: | :--------: |
| 实际：正类 |     TP     |     FN     |
| 实际：负类 |     FP     |     TN     |

精确率和召回率是一对矛盾的度量，通常精确率高时，召回率往往会比较低；而召回率高时，精确率则会比较低，原因如下：

- 精确率越高，代表预测为正类的比例更高，而要做到这点，通常就是**只选择有把握的样本**。最简单的就是只挑选最有把握的一个样本，此时 `FP=0`，`P=1`，但 `FN` 必然非常大(没把握的都判定为负类)，召回率就非常低了；
- 召回率要高，就是需要找到所有正类出来，要做到这点，最简单的就是**所有类别都判定为正类**，那么 `FN=0` ，但 `FP` 也很大，所有精确率就很低了。

而且不同的问题，侧重的评价指标也不同，比如：

- **对于推荐系统，侧重的是精确率**。也就是希望推荐的结果都是用户感兴趣的结果，即用户感兴趣的信息比例要高，因为通常给用户展示的窗口有限，一般只能展示 5 个，或者 10 个，所以更要求推荐给用户真正感兴趣的信息；
- **对于医学诊断系统，侧重的是召回率**。即希望不漏检任何疾病患者，如果漏检了，就可能耽搁患者治疗，导致病情恶化。

精确率和召回率的代码简单实现如下，这是基于二分类的情况

```python
def precision(y_true, y_pred):
    true_positive = sum(y and y_p for y, y_p in zip(y_true, y_pred))
    predicted_positive = sum(y_pred)
    return true_positive / predicted_positive
def recall(y_true, y_pred):
    true_positive = sum(y and y_p for y, y_p in zip(y_true, y_pred))
    real_positive = sum(y_true)
    return true_positive / real_positive
```

简单的测试样例以及输出如下

```python
y_true = [1, 0, 1, 0, 1]
y_pred = [0, 0, 1, 1, 0]

precisions = precision(y_true, y_pred)
recalls = recall(y_true, y_pred)

print('precisions=', precisions) # 输出为0.5
print('recalls=', recalls)       # 输出为 0.3333
```



#### 8.1.2.2 P-R 曲线和 F1

很多时候，我们都可以根据分类器的预测结果对样本进行排序，越靠前的是分类器越有把握是正类的样本，而最后面的自然就是分类器觉得最不可能是正类的样本了。

一般来说，这个预测结果其实就是分类器对样本判断为某个类别的置信度，我们可以选择不同的阈值来调整分类器对某个样本的输出结果，比如设置阈值是 0.9，那么只有置信度是大于等于 0.9 的样本才会最终判定为正类，其余的都是负类。

我们设置不同的阈值，自然就会得到不同的正类数量和负类数量，依次计算不同情况的精确率和召回率，然后我们可以**以精确率为纵轴，召回率为横轴，绘制一条“P-R曲线”**，如下图所示：

![来自西瓜书](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/P-R_plot.png)

当然，以上这个曲线是比较理想情况下的，未来绘图方便和美观，实际情况如下图所示：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/P_R.png)

对于 P-R 曲线，有：

1.**曲线从左上角 `(0,1)` 到右下角 `(1,0)` 的走势，正好反映了精确率和召回率是一对矛盾的度量**，一个高另一个低的特点：

- **开始是精确率高**，因为设置阈值很高，只有第一个样本（分类器最有把握是正类）被预测为正类，其他都是负类，所以精确率高，几乎是 1，而召回率几乎是 0，仅仅找到 1 个正类。
- **右下角时候就是召回率很高，精确率很低**，此时设置阈值就是 0，所以类别都被预测为正类，所有正类都被找到了，召回率很高，而精确率非常低，因为大量负类被预测为正类。

2.`P-R` 曲线可以非常直观显示出分类器在样本总体上的精确率和召回率。所以可以对比两个分类器在同个测试集上的 `P-R` 曲线来比较它们的分类能力：

- 如果分类器 `B` 的 `P-R` 曲线被分类器 `A` 的曲线完全包住，如下左图所示，则可以说，`A` 的性能优于 `B`;
- 如果是下面的右图，两者的曲线有交叉，则很难直接判断两个分类器的优劣，只能根据具体的精确率和召回率进行比较：
  - 一个合理的依据是**比较 `P-R` 曲线下方的面积大小**，它在一定程度上表征了分类器在精确率和召回率上取得“双高”的比例，但这个数值不容易计算；
  - 另一个比较就是**平衡点**(Break-Event Point, BEP)，它是**精确率等于召回率时的取值**，如下右图所示，而且可以判定，**平衡点较远的曲线更好**。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/P_R_AB.JPG)

当然了，平衡点还是过于简化，于是有了 **F1 值**这个新的评价标准，它是**精确率和召回率的调和平均值**，定义为：
$$
F1 = \frac{2 \times P \times R}{P+R}=\frac{2\times TP}{样本总数+TP-TN}
$$
F1 还有一个更一般的形式：$F_{\beta}$，能让我们表达出对精确率和召回率的不同偏好，定义如下：
$$
F_{\beta}=\frac{(1+\beta^2)\times P\times R}{(\beta^2 \times P)+R}
$$
其中$\beta > 0$ 度量了召回率对精确率的相对重要性，当 $\beta = 1$，就是 F1；如果 $\beta > 1$，召回率更加重要；如果 $\beta < 1$，则是精确率更加重要。

#### 8.1.2.3 宏精确率/微精确率、宏召回率/微召回率以及宏 F1 / 微 F1

很多时候，我们会得到不止一个二分类的混淆矩阵，比如多次训练/测试得到多个混淆矩阵，在多个数据集上进行训练/测试来估计算法的“全局”性能，或者是执行多分类任务时对类别两两组合得到多个混淆矩阵。

总之，我们希望在 n 个二分类混淆矩阵上综合考察精确率和召回率。这里一般有两种方法来进行考察：

1.第一种是直接在**各个混淆矩阵上分别计算出精确率和召回率**，记为 $(P_1, R_1), (P_2, R_2), \cdots, (P_n, R_n)$，接着**计算平均值**，就得到宏精确率(macro-P)、宏召回率(macro-R)以及宏 F1(macro-F1) , 定义如下：
$$
macro-P = \frac{1}{n}\sum_{i=1}^n P_i,\\
macro-R = \frac{1}{n}\sum_{i=1}^n R_i,\\
macro-F1 = \frac{2\times macro-P\times macro-R}{marco-P+macro-R}
$$
2.第二种则是**对每个混淆矩阵的对应元素进行平均**，**得到 TP、FP、TN、FN 的平均值**，再基于这些平均值就就得到微精确率(micro-P)、微召回率(micro-R)以及微 F1(micro-F1) , 定义如下：
$$
micro-P = \frac{\overline{TP}}{\overline{TP}+\overline{FP}},\\
micro-R = \frac{\overline{TP}}{\overline{TP}+\overline{FN}},\\
micro-F1 = \frac{2\times micro-P\times micro-R}{micro-P + micro-R}
$$

### 8.1.3 ROC 与 AUC

#### 8.1.3.1 ROC 曲线

ROC 曲线的 Receiver Operating Characteristic 曲线的简称，中文名是“受试者工作特征”，起源于军事领域，后广泛应用于医学领域。

它的横坐标是**假正例率(False Positive Rate, FPR)**，纵坐标是**真正例率(True Positive Rate, TPR)**，两者的定义分别如下：
$$
TPR = \frac{TP}{TP+FN},\\
FPR = \frac{FP}{FP+TN}
$$
TPR 表示**正类中被分类器预测为正类的概率**，刚好就等于正类的召回率；

FPR 表示**负类中被分类器预测为正类的概率**，它等于 1 减去负类的召回率，负类的召回率如下，**称为真反例率(True Negative Rate, TNR)**, 也被称为特异性，表示负类被正确分类的比例。
$$
TNR =\frac{TN}{FP+TN}
$$

跟 P-R 曲线的绘制一样，ROC 曲线其实也是通过**不断调整区分正负类结果的阈值**来绘制得到的，它的纵轴是 TPR，横轴是 FPR，这里借鉴《百面机器学习》上的示例来介绍，首先有下图所示的表格，表格是一个二分类模型的输出结果样例，包含 20 个样本，然后有对应的真实标签，其中 p 表示是正类别，而 n 表示是负类别。然后模型输出概率表示模型对判断该样本是正类的置信度。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/roc_table.jpg)

最开始如果设置阈值是无穷大的时候，那么模型会将所有样本判断为负类，TP 和 FP 都会是 0，也就是 TPR 和 FPR 必然也是 0，ROC 曲线的第一个坐标就是 (0, 0)。接着，阈值设置为 0.9，此时样本序号为 1 的样本会被判断为正样本，并且它确实是正样本，那么 TP = 1，而正类样本的个数是有 10 个，所有 TPR = 0.1；然后没有预测错误的正类，即 FP = 0，FPR = 0，这个时候曲线的第二个坐标就是 (0, 0.1)。

通过不断调整阈值，就可以得到曲线的不同坐标，最终得到下图所示的 ROC 曲线。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/roc_2.jpg)

第二种更直观地绘制 ROC 曲线的方法，首先统计出正负样本的数量，假设分别是 P 和 N，接着，将横轴的刻度间隔设置为 1/N，纵轴的刻度间隔设置为 1/P。然后根据模型输出的概率对样本排序，并按顺序遍历样本，从零点开始绘制 ROC 曲线，**每次遇到一个正样本就沿纵轴方向绘制一个刻度间隔的曲线**，**遇到一个负样本就沿横轴绘制一个刻度间隔的曲线**，直到遍历完所有样本，曲线最终停留在 (1,1) 这个点，此时就完成了 ROC 曲线的绘制了。

当然，更一般的 ROC 曲线是如下图所示的，会更加的平滑，上图是由于样本数量有限才导致的。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/ROC.png)

对于 ROC 曲线，有以下几点特性：

1.ROC 曲线通常都是从左下角 (0,0) 开始，到右上角 (1,1) 结束。

- 开始时候，**第一个样本被预测为正类**，其他都是预测为负类别；
  - TPR 会很低，几乎是 0，上述例子就是 0.1，此时大量正类没有被分类器找出来；
  - FPR 也很低，可能就是0，上述例子就是 0，这时候被预测为正类的样本可能实际也是正类，所以几乎没有预测错误的正类样本。
- 结束时候，**所有样本都预测为正类**。
  - TPR 几乎就是 1，因为所有样本都预测为正类，那肯定就找出所有的正类样本了；
  - FPR 也是几乎为 1，因为所有负样本都被错误判断为正类。

2.ROC 曲线中：

- **对角线对应于随机猜想模型**，即概率为 0.5；
- **点 `(0,1)` 是理想模型**，因为此时 `TPR=1`，`FPR=0`，也就是正类都预测出来，并且没有预测错误；
- 通常，**ROC 曲线越接近点 `(0, 1)` 越好。**

3.同样可以根据 ROC 曲线来判断两个分类器的性能：

- 如果**分类器 `A` 的 `ROC` 曲线被分类器 `B` 的曲线完全包住，可以说 `B` 的性能好过 `A**`，这对应于上一条说的 ROC 曲线越接近点 `(0, 1)越好；
- 如果两个分类器的 `ROC` 曲线发生了交叉，则同样很难直接判断两者的性能优劣，需要借助 `ROC` 曲线下面积大小来做判断，而这个面积被称为 **`AUC:Area Under ROC Curve`**。

简单的代码实现如下：

```python
def true_negative_rate(y_true, y_pred):
    true_negative = sum(1 - (yi or yi_hat) for yi, yi_hat in zip(y_true, y_pred))
    actual_negative = len(y_true) - sum(y_true)
    return true_negative / actual_negative


def roc(y, y_hat_prob):
    thresholds = sorted(set(y_hat_prob), reverse=True)
    ret = [[0, 0]]
    for threshold in thresholds:
        y_hat = [int(yi_hat_prob >= threshold) for yi_hat_prob in y_hat_prob]
        ret.append([recall(y, y_hat), 1 - true_negative_rate(y, y_hat)])
    return ret
```

简单的测试例子如下：

```python
y_true = [1, 0, 1, 0, 1]
y_hat_prob = [0.9, 0.85, 0.8, 0.7, 0.6]

roc_list = roc(y_true, y_hat_prob)
print('roc_list:', roc_list)
# 输出结果是 roc_list: [[0, 0], [0.3333333333333333, 0.0], [0.3333333333333333, 0.5], [0.6666666666666666, 0.5], [0.6666666666666666, 1.0], [1.0, 1.0]]
```



#### 8.1.3.2 **ROC 和 P-R 曲线的对比**

**相同点**

1.**两者刻画的都是阈值的选择对分类度量指标的影响**。虽然每个分类器对每个样本都会输出一个概率，也就是置信度，但通常我们都会人为设置一个阈值来影响分类器最终判断的结果，比如设置一个很高的阈值--0.95，或者比较低的阈值--0.3。

- **如果是偏向于精确率，则提高阈值**，保证只把有把握的样本判断为正类，此时可以设置阈值为 0.9，或者更高；
- **如果偏向于召回率，那么降低阈值**，保证将更多的样本判断为正类，更容易找出所有真正的正样本，此时设置阈值是 0.5，或者更低。

2.两个曲线的每个点都是**对应某个阈值的选择，该点是在该阈值下的 `(精确率，召回率)` / `(TPR, FPR)`**。然后沿着横轴方向对应阈值的下降。

**不同**

相比较 `P-R` 曲线，`ROC` 曲线有一个特点，就是**正负样本的分布发生变化时，它的曲线形状能够基本保持不变**。如下图所示：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/ROC_PR_compare.jpg)

分别比较了增加十倍的负样本后， `P-R` 和 `ROC` 曲线的变化，可以看到 `ROC` 曲线的形状基本不变，但 `P-R` 曲线发生了明显的变化。

所以 `ROC` 曲线的这个特点**可以降低不同测试集带来的干扰**，**更加客观地评估模型本身的性能**，因此它适用的场景更多，比如排序、推荐、广告等领域。

这也是由于**现实场景中很多问题都会存在正负样本数量不平衡**的情况，比如计算广告领域经常涉及转化率模型，正样本的数量往往是负样本数量的千分之一甚至万分之一，这时候选择 `ROC` 曲线更加考验反映模型本身的好坏。

当然，如果希望看到模型在特定数据集上的表现，`P-R` 曲线会更直观地反映其性能。所以还是需要具体问题具体分析。

#### 8.1.3.3 AUC 曲线

`AUC` 是 `ROC` 曲线的面积，其物理意义是：从所有正样本中随机挑选一个样本，模型将其预测为正样本的概率是 $p_1$；从所有负样本中随机挑选一个样本，模型将其预测为正样本的概率是 $p_0$。**$p_1 > p_0$ 的概率就是 `AUC`**。

`AUC` 曲线有以下几个特点：

- 如果完全随机地对样本进行分类，那么 $p_1 > p_0$ 的概率是 0.5，则 `AUC=0.5`；

- **`AUC` 在样本不平衡的条件下依然适用**。

  如：在反欺诈场景下，假设正常用户为正类（设占比 99.9%），欺诈用户为负类（设占比 0.1%）。

  如果使用准确率评估，则将所有用户预测为正类即可获得 99.9%的准确率。很明显这并不是一个很好的预测结果，因为欺诈用户全部未能找出。

  如果使用 `AUC` 评估，则此时 `FPR=1,TPR=1`，对应的 `AUC=0.5` 。因此 `AUC` 成功的指出了这并不是一个很好的预测结果。

- `AUC` 反应的是**模型对于样本的排序能力**（根据样本预测为正类的概率来排序）。如：`AUC=0.8` 表示：给定一个正样本和一个负样本，在 `80%` 的情况下，模型对正样本预测为正类的概率大于对负样本预测为正类的概率。

- **`AUC` 对于均匀采样不敏感**。如：上述反欺诈场景中，假设对正常用户进行均匀的降采样。任意给定一个负样本 n，设模型对其预测为正类的概率为 Pn 。降采样前后，由于是均匀采样，因此预测为正类的概率大于 Pn 和小于 Pn  的真正样本的比例没有发生变化。因此 `AUC` 保持不变。

  但是如果是非均匀的降采样，则预测为正类的概率大于 Pn  和小于 Pn 的真正样本的比例会发生变化，这也会导致 `AUC` 发生变化。

- **正负样本之间的预测为正类概率之间的差距越大，则 `AUC` 越高**。因为这表明正负样本之间排序的把握越大，区分度越高。

  如：在电商场景中，点击率模型的 `AUC` 要低于购买转化模型的 `AUC` 。因为点击行为的成本低于购买行为的成本，所以点击率模型中正负样本的差别要小于购买转化模型中正负样本的差别。

`AUC` 的计算可以通过对 `ROC` 曲线下各部分的面积求和而得。假设 `ROC` 曲线是由坐标为下列这些点按顺序连接而成的： 
$$
{(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m)}, 其中\ x_1=0, x_m=1
$$
那么 `AUC` 可以这样估算：
$$
AUC = \frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)\times (y_i+y_{i+1})
$$

代码实现如下：

```python
def get_auc(y, y_hat_prob):
    roc_val = iter(roc(y, y_hat_prob))
    tpr_pre, fpr_pre = next(roc_val)
    auc = 0
    for tpr, fpr in roc_val:
        auc += (tpr + tpr_pre) * (fpr - fpr_pre) / 2
        tpr_pre = tpr
        fpr_pre = fpr
    return auc
```

简单的测试样例如下：

```python
y_true = [1, 0, 1, 0, 1]
y_hat_prob = [0.9, 0.85, 0.8, 0.7, 0.6]

auc_val = get_auc(y_true, y_hat_prob)
print('auc_val:', auc_val) # 输出是 0.5
```



### 8.1.4 代价矩阵

前面介绍的性能指标都有一个隐式的前提，错误都是**均等代价**。但实际应用过程中，不同类型的错误所造成的后果是不同的。比如将健康人判断为患者，与患者被判断为健康人，代价肯定是不一样的，前者可能就是需要再次进行检查，而后者可能错过治疗的最佳时机。

因此，为了衡量不同类型所造成的不同损失，可以为错误赋予**非均等代价(unequal cost)**。

对于一个二类分类问题，可以设定一个**代价矩阵(cost matrix)**，其中 $cost_{ij}$ 表示将第 `i` 类样本预测为第 `j` 类样本的代价，而预测正确的代价是 0 。如下表所示：

|                | 预测：第 0 类 | 预测：第 1 类 |
| :------------: | :-----------: | :-----------: |
| 真实：第 0 类  |       0       |  $cost_{01}$  |
| 真实： 第 1 类 |  $cost_{10}$  |       0       |

1. 在非均等代价下，希望找到的不再是简单地最小化错误率的模型，而是希望找到**最小化总体代价 `total cost` 的模型**。

2. 在非均等代价下，`ROC` 曲线不能直接反映出分类器的期望总体代价，此时需要使用代价曲线 `cost curve`

   - 代价曲线的横轴是**正例概率代价**，如下所示，其中 p 是正例(第 0 类)的概率

   $$
   P_{+cost} = \frac{p\times cost_{01}}{p\times cost_{01}+(1-p)\times cost_{10}}
   $$





   - 代价曲线的纵轴是归一化代价，如下所示：
     $$
     cost_{norm} = \frac{FNR\times p\times cost_{01}+FPR\times (1-p)\times cost_{10}}{p\times cost_{01}+(1-p)\times cost_{10}}
     $$
     其中，假正例率 `FPR` 表示模型将负样本预测为正类的概率，定义如下：
     $$
     FPR = \frac{FP}{TN+FP}
     $$
     假负例率 `FNR` 表示将正样本预测为负类的概率，定义如下：
     $$
     FNR = 1 - TPR = \frac{FN}{TP+FN}
     $$
     代价曲线如下图所示：

     ![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/cost_curve.png)





### 8.1.5 回归问题的性能度量

对于回归问题，常用的性能度量标准有：

1.均方误差(Mean Square Error, MSE)，定义如下：
$$
MSE=\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y_i})^2
$$
2.均方根误差(Root Mean Squared Error, RMSE)，定义如下：
$$
RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y_i})^2}
$$
3.均方根对数误差(Root Mean Squared Logarithmic Error, RMSLE)，定义如下
$$
RMSLE=\sqrt{\frac{1}{N}\sum_{i=1}^N[log(y_i+1)- log(\hat{y_i}+1)]^2}
$$
4.平均绝对误差(Mean Absolute Error, MAE)，定义如下：
$$
MAE = \frac{1}{N}\sum_{i=1}^N |y_i-\hat{y_i}|
$$
这四个标准中，比较常用的第一个和第二个，即 `MSE` 和 `RMSE`，这两个标准一般都可以很好反映回归模型预测值和真实值的偏离程度，但如果遇到**个别偏离程度非常大的离群点**时，即便数量很少，也会让这两个指标变得很差。

遇到这种情况，有三种解决思路：

- 将离群点作为噪声点来处理，即数据预处理部分需要过滤掉这些噪声点；
- 从模型性能入手，提高模型的预测能力，将这些离群点产生的机制建模到模型中，但这个方法会比较困难；
- 采用其他指标，比如第三个指标 `RMSLE`，它关注的是预测误差的比例，即便存在离群点，也可以降低这些离群点的影响；或者是 `MAPE`，平均绝对百分比误差(Mean Absolute Percent Error)，定义为：

$$
MAPE = \sum_{i=1}^n |\frac{y_i-\hat{y_i}}{y_i}|\times\frac{100}{n}
$$

`RMSE` 的简单代码实现如下所示：

```python
def rmse(predictions, targets):
    # 真实值和预测值的误差
    differences = predictions - targets
    differences_squared = differences ** 2
    mean_of_differences_squared = differences_squared.mean()
    # 取平方根
    rmse_val = np.sqrt(mean_of_differences_squared)
    return rmse_val
```






### 1.6 其他评价指标

1. 计算速度：模型训练和预测需要的时间；
2. 鲁棒性：处理缺失值和异常值的能力；
3. 可拓展性：处理大数据集的能力；
4. 可解释性：模型预测标准的可理解性，比如决策树产生的规则就很容易理解，而神经网络被称为黑盒子的原因就是它的大量参数并不好理解。



## 8.2. 模型评估的方法

### 8.2.1 泛化能力

1. **泛化能力**：指模型对**未知的、新鲜的数据的预测能力**，通常是根据**测试误差**来衡量模型的泛化能力，测试误差越小，模型能力越强；
2. 统计理论表明：如果训练集和测试集中的样本都是独立同分布产生的，则有 **模型的训练误差的期望等于模型的测试误差的期望** 。
3. 机器学习的“没有免费的午餐定理”表明：在所有可能的数据生成分布上，没有一个机器学习算法总是比其他的要好。
   - 该结论仅在考虑所有可能的数据分布时才成立。
   - 现实中特定任务的数据分布往往满足某类假设，从而可以设计在这类分布上效果更好的学习算法。
   - 这意味着机器学习并不需要寻找一个通用的学习算法，而是寻找一个在关心的数据分布上效果最好的算法。
4. 正则化是对学习算法做的一个修改，这种修改趋向于降低泛化误差（而不是降低训练误差）。
   - 正则化是机器学习领域的中心问题之一。
   - 没有免费的午餐定理说明了没有最优的学习算法，因此也没有最优的正则化形式。

### 8.2.2 泛化能力的评估

常用的对模型泛化能力的评估方法有以下几种，主要区别就是如何划分测试集。

- **留出法**(Holdout)
- **`k-fold` 交叉验证**(Cross Validation)
- **留一法**(Leave One Out, LOO)
- **自助法**(bootstrapping)

#### 8.2.2.1 **留出法**(Holdout)

留出法是最简单也是最直接的验证方法，它就是将**数据集随机划分为两个互斥的集合**，即训练集和测试集，比如按照 7:3 的比例划分，70% 的数据作为训练集，30% 的数据作为测试集。**也可以划分为三个互斥的集合，此时就增加一个验证集，用于调试参数和选择模型**。

直接采用 `sklearn` 库的 `train_test_split` 函数即可实现，一个简单的示例代码如下，这里简单调用 `knn` 算法，采用 `Iris` 数据集。

```python
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier

# 加载 Iris 数据集
dataset = load_iris()
# 划分训练集和测试集
(trainX, testX, trainY, testY) = train_test_split(dataset.data, dataset.target, random_state=3, test_size=0.3)
# 建立模型
knn = KNeighborsClassifier()
# 训练模型
knn.fit(trainX, trainY)
# 将准确率打印
print('hold_out, score:', knn.score(testX, testY))
```

留出法的使用需要注意：

1. **数据集的划分要尽可能保持数据分布的一致性，避免因为数据划分过程引入额外的偏差而对最终结果产生影响**。比如训练、验证和测试集的类别比例差别很大，则误差估计将由于三个集合数据分布的差异而产生偏差。

   因此，**分类任务中必须保持每个集合中的类别比例相似**。从采样的角度看数据集的划分过程，这种保留类别比例的采样方式称为“分层采样”。

2. 即便确定了训练、验证、测试集的比例，还是有多种划分方式，比如排序后划分、随机划分等等，这些不同的划分方式导致**单次留出法得到的估计结果往往不够稳定可靠**。因此，使用留出法的时候，**往往采用若干次随机划分、重复进行实验后，取平均值作为最终评估结果**。

分层采样的简单代码实现如下所示，主要是调用了 `sklearn.model_selection`  中的 `StratifiedKFold`

```python
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone

def StratifiedKFold_method(n_splits=3):
    '''
    分层采样
    :return:
    '''
    # 加载 Iris 数据集
    dataset = load_iris()
    data = dataset.data
    label = dataset.target
    # 建立模型
    knn = KNeighborsClassifier()
    print('use StratifiedKFold')
    skfolds = StratifiedKFold(n_splits=n_splits, random_state=42)
    scores = 0.
    for train_index, test_index in skfolds.split(data, label):
        clone_clf = clone(knn)
        X_train_folds = data[train_index]
        y_train_folds = (label[train_index])
        X_test_fold = data[test_index]
        y_test_fold = (label[test_index])
        clone_clf.fit(X_train_folds, y_train_folds)
        y_pred = clone_clf.predict(X_test_fold)
        n_correct = sum(y_pred == y_test_fold)
        print(n_correct / len(y_pred))
        scores += n_correct / len(y_pred)
    print('mean scores:', scores / n_splits)
```

留出法也存在以下的缺点：

1. **在验证集或者测试集上的评估结果和划分方式有关系**，这也就是为什么需要多次实验，取平均值；
2. 我们希望评估的是在原始数据集上训练得到的模型的能力，但留出法在划分两个或者三个集合后，训练模型仅使用了原始数据集的一部分，这会降低评估结果的保真性。但这个问题没有完美的解决方法，常见做法是将大约 `2/3 ~ 4/5` 的样本作为训练集，剩余的作为验证集和测试集。



#### 8.2.2.2 **`k-fold` 交叉验证**(Cross Validation)

**`k-fold` 交叉验证** 的工作流程：

1. 将原始数据集划分为 `k` 个大小相等且互斥的子集；
2. 选择 `k-1` 个子集作为训练集，剩余作为验证集进行模型的训练和评估，重复 `k` 次(每次采用不同子集作为验证集)；
3. 将 `k` 次实验评估指标的平均值作为最终的评估结果。

通常，`k` 取 10。

但和留出法类似，同样存在多种划分 `k` 个子集的方法，所以依然需要随时使用不同方式划分 `p` 次，每次得到 `k` 个子集。

同样，采用 `sklearn.cross_validation` 的 `cross_val_score` 库可以快速实现 `k-fold` 交叉验证法，示例如下：

```python
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cross_validation import cross_val_score
# 加载 Iris 数据集
dataset = load_iris()
data = dataset.data
label = dataset.target
# 建立模型
knn = KNeighborsClassifier()
# 使用K折交叉验证模块
scores = cross_val_score(knn, data, label, cv=10, scoring='accuracy')
# 将每次的预测准确率打印出
print(scores)
# 将预测准确平均率打印出
print(scores.mean())
```



#### 8.2.2.3 留一法

留一法是 `k-fold` 交叉验证的一个特例情况，即让 `k=N`, 其中 `N` 是原始数据集的样本数量，这样**每个子集就只有一个样本，这就是留一法**。

留一法的优点就是**训练数据更接近原始数据集**了，仅仅相差一个样本而已，通过这种方法训练的模型，几乎可以认为就是在原始数据集上训练得到的模型 。

但缺点也比较明显，**计算速度会大大降低**，**特别是原始数据集非常大的时候**，训练 `N` 个模型的计算量和计算时间都很大，因此一般实际应用中很少采用这种方法。



#### 8.2.2.4 自助法

在留出法和 `k-fold` 交叉验证法中，由于保留了一部分样本用于测试，因此实际训练模型使用的训练集比初始数据集小，**这必然会引入一些因为训练样本规模不同而导致的估计偏差**。

留一法受训练样本规模变化的影响较小，但是**计算复杂度太高**。

自助法是一个以**自助采样法(`bootstrap sampling`)为基础的比较好的解决方案**。同时，它也是随机森林算法中用到的方法。

它的做法就是**对样本数量为 `N` 的数据集进行 `N` 次有放回的随机采样**，得到一个大小是 `N` 的训练集。

在这个过程中将会有一部分数据是没有被采样得到的，一个样本始终没有被采样出来的概率是 $(1-\frac{1}{N})^N$，根据极限可以计算得到:
$$
lim_{N\rightarrow +\infty}(1-\frac{1}{N})^N=\frac{1}{e}\approx0.368
$$
也就是采用自助法，会有 **36.8%** 的样本不会出现在训练集中，使用这部分样本作为测试集。这种方法也被称为包外估计。

自助法的优点有：

- 在**数据集比较小、难以有效划分训练/测试集**时很有用：

- 能从**初始数据集中产生多个不同的训练集**，这对集成学习等方法而言有很大好处。

但也存在如下缺点：

- 产生的数据集**改变了初始数据集的分布，这会引入估计偏差**。因此在**初始数据量足够时，留出法和折交叉验证法更常用**。



### 8.2.3 训练集、验证集、测试集

简单介绍下训练集、验证集和测试集各自的作用：

1. **训练集**：主要就是训练模型，理论上越大越好；
2. **验证集**：用于模型调试超参数。通常要求验证集比较大，避免模型会对验证集过拟合；
3. **测试集**：用于评估模型的泛化能力。理论上，测试集越大，评估结果就约精准。另外，测试集必须不包含训练样本，否则会影响对模型泛化能力的评估。

验证集和测试集的对比：

- 测试集通常用于对模型的预测能力进行评估，它是**提供模型预测能力的无偏估计**；如果不需要对模型预测能力的无偏估计，可以不需要测试集；
- 验证集主要是用于超参数的选择。



### 8.2.4 划分数据集的比例选择方法

那么一般如何选择划分训练、验证和测试集的比例呢？通常可以按照如下做法：

1. 对于**小批量数据**，数据的拆分的常见比例为：
   - **如果未设置验证集，则将数据三七分**：70% 的数据用作训练集、30% 的数据用作测试集。
   - **如果设置验证集，则将数据划分为**：60% 的数据用作训练集、20%的数据用过验证集、20% 的数据用作测试集。
2. **对于大批量数据，验证集和测试集占总数据的比例会更小**。
   - 对于百万级别的数据，其中 1 万条作为验证集、1 万条作为测试集即可。
   - 验证集的目的就是验证不同的超参数；测试集的目的就是比较不同的模型。
     - 一方面它们要足够大，才足够评估超参数、模型。
     - 另一方面，如果它们太大，则会浪费数据（验证集和训练集的数据无法用于训练）。
3. 在 `k-fold` 交叉验证中：先将所有数据拆分成 `k` 份，然后其中 `1` 份作为测试集，其他 `k-1` 份作为训练集。
   - 这里并没有验证集来做超参数的选择。所有测试集的测试误差的均值作为模型的预测能力的一个估计。
   - **使用 `k-fold` 交叉的原因是：样本集太小**。如果选择一部分数据来训练，则有两个问题：
     - **训练数据的分布可能与真实的分布有偏离**。`k-fold` 交叉让所有的数据参与训练，会使得这种偏离得到一定程度的修正。
     - **训练数据太少，容易陷入过拟合**。`k`-fold 交叉让所有数据参与训练，会一定程度上缓解过拟合。



### 8.2.5 分布不匹配

深度学习时代，经常会发生：**训练集和验证集、测试集的数据分布不同**。

如：训练集的数据可能是从网上下载的高清图片，测试集的数据可能是用户上传的、低像素的手机照片。

- **必须保证验证集、测试集的分布一致**，它们都要很好的代表你的真实应用场景中的数据分布。
- **训练数据可以与真实应用场景中的数据分布不一致**，因为最终关心的是在模型真实应用场景中的表现。

如果发生了数据不匹配问题，则可以想办法**让训练集的分布更接近验证集**。

- 一种做法是：**收集更多的、分布接近验证集的数据作为训练集合**。
- 另一种做法是：**人工合成训练数据，使得它更接近验证集**。该策略有一个潜在问题：你可能只是模拟了全部数据空间中的一小部分。导致你的模型对这一小部分过拟合。

当训练集和验证集、测试集的数据分布不同时，有以下经验原则：

- **确保验证集和测试集的数据来自同一分布**。

  因为需要使用验证集来优化超参数，而优化的最终目标是希望模型在测试集上表现更好。

- **确保验证集和测试集能够反映未来得到的数据，或者最关注的数据**。

- **确保数据被随机分配到验证集和测试集上**。

当训练集和验证集、测试集的数据分布不同时，**分析偏差和方差的方式有所不同**。

- 如果**训练集和验证集的分布一致**，那么当训练误差和验证误差相差较大时，我们认为**存在很大的方差问题**。

- 如果训练集和验证集的分布不一致，那么当训练误差和验证误差相差较大时，有两种原因：

  - 第一个原因：**模型只见过训练集数据，没有见过验证集的数据导致的，是数据不匹配的问题**。
  - 第二个原因：**模型本来就存在较大的方差**。

  为了弄清楚原因，需要将训练集再随机划分为：`训练-训练集`、`训练-验证集`。这时候，`训练-训练集`、`训练-验证集` 是同一分布的。

  - **模型在`训练-训练集` 和 `训练-验证集` 上的误差的差距代表了模型的方差**。
  - **模型在`训练-验证集` 和 验证集上的误差的差距代表了数据不匹配问题的程度**。



## 8.3 过拟合、欠拟合

机器学习的两个主要挑战是**过拟合和欠拟合**。

**过拟合(overfitting)**：**指算法模型在训练集上的性能非常好，但是泛化能力很差，泛化误差很大，即在测试集上的效果却很糟糕的情况**。

- 过拟合的原因：将**训练样本本身的一些特点当作了所有潜在样本都具有的一般性质**，这会造成泛化能力下降；另一个原因是**模型可能学到训练集中的噪声，并基于噪声进行了预测**；
- **过拟合无法避免，只能缓解**。因为**机器学习的问题通常是 `NP` 难甚至更难的，而有效的学习算法必然是在多项式时间内运行完成**。如果可以避免过拟合，这就意味着构造性的证明了 `P=NP` 。

**欠拟合(underfitting)**：**模型的性能非常差，在训练数据和测试数据上的性能都不好，训练误差和泛化误差都很大**。其原因就是模型的学习能力比较差。

一般可以通过挑战模型的容量来缓解过拟合和欠拟合问题。**模型的容量是指其拟合各种函数的能力**。

- 容量低的模型容易发生欠拟合，模型拟合能力太弱。
- 容量高的模型容易发生过拟合，模型拟合能力太强。

一般解决过拟合的方法有：

- **简化模型**，这包括了采用简单点的模型、减少特征数量，比如神经网络中减少网络层数或者权重参数，决策树模型中降低树的深度、采用剪枝等；
- **增加训练数据**，采用数据增强的方法，比如人工合成训练数据等；
- **早停**，当验证集上的误差没有进一步改善，训练提前终止；
- **正则化**，常用 L1 或者 L2 正则化。
- **集成学习方法**，训练多个模型，并以每个模型的平均输出作为结果，降低单一模型的过拟合风险，常用方法有 `bagging` 、`boosting`、`dropout`(深度学习中的方法)等；
- **噪声注入**：包括输入噪声注入、输出噪声注入、权重噪声注入。将噪声分别注入到输入/输出/权重参数中，虽然噪声可能是模型过拟合的一个原因，但第一可以通过交叉验证来避免；第二就是没有噪声的完美数据也是很有可能发生过拟合；第三可以选择在特征、权值参数加入噪声，而非直接在数据加入噪声。



解决欠拟合的方法有：

- 选择一个**更强大的模型**，带有更多参数
- 用**更好的特征**训练学习算法（特征工程）
- **减小对模型的限制**（比如，减小正则化超参数）



## 8.4 超参数调优

超参数调优是一件非常头疼的事情，很多时候都需要一些先验知识来选择合理的参数值，但如果没有这部分先验知识，要找到最优的参数值是很困难，非常耗费时间和精力。但超参数调优确实又可以让模型性能变得更加的好。

在选择超参数调优算法前，需要明确以下几个要素：

- **目标函数**。算法需要最大化/最小化的目标；
- **搜索范围**。一般通过上下限来确定；
- **算法的其他参数**，比如搜索步长。



### 8.4.1 搜索策略

常用的几种超参数搜索策略如下：

- **手动搜索**：需要较好的先验知识经验
- **网格搜索**：超参数的数据相对较少的时候，这个方法比较实用
- **随机搜索**：通常推荐这种方式
- **贝叶斯优化算法**：基于模型的搜索方法，利用了历史搜索结果



#### 8.4.1.1 手动搜索

1. 手动选择超参数需要了解超参数做了些什么，以及机器学习模型如何才能取得良好的泛化。

2. 手动搜索超参数的任务是：**在给定运行时间和内存预算范围的条件下，最小化泛化误差**。

3. 手动调整超参数时不要忘记最终目标：提升测试集性能。

   - 加入正则化只是实现这个目标的一种方法。

   - 如果训练误差很低，也可以通过收集更多的训练数据来减少泛化误差。

     如果训练误差太大，则收集更多的训练数据就没有意义。

   - 实践中的一种暴力方法是：不断提高模型容量和训练集的大小。

     这种方法增加了计算代价，只有在拥有充足的计算资源时才可行



#### 8.4.1.2 网格搜索

网格搜索可能是最简单也是应用最广泛的超参数搜索算法了。它的几种做法如下：

- **采用较大的搜索范围和较小的搜索步长**，很大概率会搜索到全局最优值，但十分耗费计算资源和时间，特别是超参数比较多的时候；
- **先采用较大搜索范围和较大步长**，寻找全局最优的可能位置，**然后逐渐缩小搜索范围和步长**，来确定更精确的最优值。可以降低所需要的计算时间和计算量，但由于目标函数一般都是非凸的，可能会错过全局最优值。

网格搜索也可以借助 `sklearn` 实现，简单的示例代码如下：

```python
from sklearn.model_selection import	GridSearchCV
from sklearn.ensemble import RandomForestClassifier
param_grid = [
    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
]
forest_reg = RandomForestRegressor()
grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                           scoring='neg_mean_squared_error')
grid_search.fit(data, labels)
```



#### 8.4.1.3 随机搜索

随机搜索是一种可以替代网格搜索的方法，它编程简单、使用方便、能更快收敛到超参数的良好取值。

- 首先为每个超参数定义一个边缘分布，如伯努利分布（对应着二元超参数）或者对数尺度上的均匀分布（对应着正实值超参数）。
- 然后假设超参数之间相互独立，从各分布中抽样出一组超参数。
- 使用这组超参数训练模型。
- 经过多次抽样 -> 训练过程，挑选验证集误差最小的超参数作为最好的超参数。

随机搜索的优点如下：

- **不需要离散化超参数的值，也不需要限定超参数的取值范围**。这允许我们在一个更大的集合上进行搜索。
- 当某些超参数对于性能没有显著影响时，随机搜索相比于网格搜索指数级地高效，它能**更快的减小验证集误差**。

**随机搜索比网格搜索更快的找到良好超参数的原因是：没有浪费的实验**。

- 在网格搜索中，**两次实验之间只会改变一个超参数** （假设为 `m`）的值，而其他超参数的值保持不变。如果这个超参数 `m` 的值对于验证集误差没有明显区别，那么网格搜索相当于进行了两个重复的实验。
- 在随机搜索中，**两次实验之间，所有的超参数值都不会相等，因为每个超参数的值都是从它们的分布函数中随机采样而来**。因此不大可能会出现两个重复的实验。
- 如果  `m`  超参数与泛化误差无关，那么不同的 `m`  值：
  - 在网格搜索中，**不同 `m` 值、相同的其他超参数值，会导致大量的重复实验**。
  - 在随机搜索中，其他超参数值每次也都不同，因此不大可能出现两个重复的实验（除非所有的超参数都与泛化误差无关）。

随机搜索可以采用 `sklearn.model_selection` 中的 `RandomizedSearchCV ` 方法。



#### 8.4.1.4 贝叶斯优化方法

贝叶斯优化方法是基于模型的参数搜索算法的一种比较常见的算法。它相比于前面的网格搜索和随机搜索，**最大的不同就是利用历史的搜索结果进行优化搜索**。主要是由四部分组成的：

1. 目标函数。大部分情况是模型验证集上的损失；
2. 搜索空间。各类待搜索的超参数；
3. 优化策略。建立的概率模型和选择超参数的方式；
4. 历史的搜索结果。

贝叶斯优化算法的步骤如下：

1. 根据先验分布，假设一个搜索函数；
2. 然后，每一次采用新的采样点来测试目标函数时，利用这个信息更新目标函数的先验分布；
3. 最后，算法测试由后验分布给出的全局最优最可能出现的位置的点。

需要特别注意的是，贝叶斯优化算法容易**陷入局部最优值**：它在找到一个局部最优值后，会不断在该区域进行采样。

因此，贝叶斯优化算法会在探索和利用之间找到一个平衡点，探索是在还未取样的区域获取采样点，利用则是根据后验分布在最可能出现全局最优的区域进行采样。



### 8.4.2 调整原则

1. 通常**先对超参数进行粗调，然后在粗调中表现良好的超参数区域进行精调**。

2. 超参数随机搜索，并不意味着是在有效范围内随机均匀取值。需要选择合适的缩放来进行随机选取。

   - 对于学习率，假设其取值范围为 `0.000001~1`。

     如果进行均匀取值，取 10 个，那么有  90% 的随机值都位于区间 `[0.1,1]`。则 `[0.000001,0.1]` 之间没有足够的探索。这种做法明显不合理。

     此时需要使用对数缩放，在对数轴上均匀随机取点。

   - 对于指数加权移动平均的超参数 β  。假设其取值范围为 `0.9~0.9999`。

     由于 $\frac{1}{1-\beta}$ 刻画了结果使用过去多少个周期的数据来加权平均。因此如果进行均匀取值，则：

     -  β  在`0.9~0.9005` 之间取值时， $\frac{1}{1-\beta}$ 变化不大。
     -  β  在`0.9990~0.9995` 之间取值时，$\frac{1}{1-\beta}$  变化非常大。

     β 越接近 1，$\frac{1}{1-\beta}$ 对于它的变化越敏感。此时，需要对 1-β 使用对数缩放，在对数轴上均匀随机取点。

   - 如果选择了错误的缩放，如果取值的总量足够大，也可以得到不错的结果。

     尤其当配合了`粗调 -> 精调` 策略时，最终还是会聚焦到合适的超参数范围上。

3. 通常情况下，**建议至少每隔几个月重新评估或者修改超参数**。因为随着时间的变化，真实场景的数据会逐渐发生改变：

   - 可能是由于用户的行为、偏好发生了改变。
   - 可能是采样的方式发生了改变。
   - 也可能仅仅是由于数据中心更新了服务器。

   由于这些变化，原来设定的超参数可能不再适用。

4. 有两种超参数调整策略：

   - 如果数据足够大且没有足够的计算资源，此时只能一次完成一个试验。

     则**可以每天观察模型的表现，实时的、动态的调整超参数**。

   - 如果数据不大，有足够的计算资源可以同一时间完成大量的试验，则**可以设置多组超参数设定，然后选择其中表现最好的那个**。





------

### 参考

- 《机器学习》--周志华
- 《百面机器学习》
- 《hands-on-ml-with-sklearn-and-tf》
- [9. 模型评估](http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/9_model_selection.html)
- [分类模型评估的方法及Python实现](https://mp.weixin.qq.com/s/fhVlM8L4dyvMopf3pvau0A)
- [机器学习中用来防止过拟合的方法有哪些？](







# 9. 生成模型和判别模型的区别

## 生成模型

由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)（贝叶斯概率）。

基本思想是首先建立样本的联合概率概率密度模型P(X,Y)，然后再得到后验概率P(Y|X)，再利用它进行分类。

典型的生成模型有**朴素贝叶斯，隐马尔科夫模型**等



## 判别模型

由数据直接学习决策函数Y=f(X)或者条件概率分布P(Y|X)作为预测的模型，即判别模型。

基本思想是有限样本条件下建立判别函数，不考虑样本的产生模型，直接研究预测模型。

典型的判别模型包括**k近邻，感知级，决策树，支持向量机**等。

这些**模型的特点都是输入属性X可以直接得到后验概率P(Y|X)，**输出条件概率最大的作为最终的类别（对于二分类任务来说，实际得到一个score，当score大于threshold时则为正类，否则为负类）。



## 举例

判别式模型举例：要确定一个羊是山羊还是绵羊，用判别模型的方法是从历史数据中学习到模型，然后通过提取这只羊的特征来预测出这只羊是山羊的概率，是绵羊的概率。



生成式模型举例：利用生成模型是根据山羊的特征首先学习出一个山羊的模型，然后根据绵羊的特征学习出一个绵羊的模型，然后从这只羊中提取特征，放到山羊模型中看概率是多少，在放到绵羊模型中看概率是多少，哪个大就是哪个。



## 联系和区别

生成方法的特点：

- 生成方法学习联合概率密度分布P(X,Y)，所以就可以从统计的角度表示数据的分布情况，能够反映同类数据本身的相似度。但它不关心到底划分各类的那个分类边界在哪。
- 生成方法可以还原出联合概率分布P(Y,X)，而判别方法不能。
- **生成方法的学习收敛速度更快**，即当样本容量增加的时候，学到的模型可以更快的收敛于真实模型，当存在隐变量时，仍可以用生成方法学习。此时判别方法就不能用。

 

判别方法的特点：

- 判别方法直接学习的是决策函数Y=f(X)或者条件概率分布P(Y|X）
- **不能反映训练数据本身的特性**。但它寻找不同类别之间的最优分类面，反映的是异类数据之间的差异。
- 直接面对预测，**往往学习的准确率更高**。由于直接学习P(Y|X)或P(X)，可以对数据进行各种程度上的抽象、定义特征并使用特征，因此可以简化学习问题。





# 10. 特征工程

## 10.1 特征缩放

特征缩放主要分为两种方法，归一化和正则化。

### 归一化

1. **归一化(Normalization)，也称为标准化**，这里不仅仅是对特征，实际上对于原始数据也可以进行归一化处理，它是将特征（或者数据）都**缩放到一个指定的大致相同的数值区间内**。
2. **归一化的两个原因**：

- 某些算法要求样本数据或特征的数值**具有零均值和单位方差**；
- 为了消除样本数据或者特征之间的**量纲影响，即消除数量级的影响**。如下图所示是包含两个属性的目标函数的等高线
  - **数量级的差异将导致量级较大的属性占据主导地位**。从下图左看到量级较大的属性会让椭圆的等高线压缩为直线，使得目标函数仅依赖于该属性。
  - **数量级的差异会导致迭代收敛速度减慢**。原始的特征进行梯度下降时，每一步梯度的方向会偏离最小值（等高线中心点）的方向，**迭代次数较多，且学习率必须非常小**，否则非常容易引起**宽幅震荡**。但经过标准化后，每一步梯度的方向都几乎指向最小值（等高线中心点）的方向，**迭代次数较少**。
  - **所有依赖于样本距离的算法对于数据的数量级都非常敏感**。比如 KNN 算法需要计算距离当前样本最近的 k 个样本，当属性的量级不同，选择的最近的 k 个样本也会不同。

![图来自《百面机器学习》](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/normalization_example.png)

3. 常用的两种归一化方法：

- **线性函数归一化(Min-Max Scaling)**。它对原始数据进行线性变换，使得结果映射到`[0,1]`的范围，实现对原始数据的等比缩放，公式如下：

$$
X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}}
$$

其中 X 是原始数据，$X_{max}, X_{min}$分别表示数据最大值和最小值。

- **零均值归一化(Z-Score Normalization)**。它会将原始数据映射到均值为 0，标准差为 1 的分布上。假设原始特征的均值是$\mu$、方差是$\sigma$，则公式如下：

$$
z = \frac{x-\mu}{\sigma}
$$

4. 如果数据集分为训练集、验证集、测试集，那么**三个数据集都采用相同的归一化参数，数值都是通过训练集计算得到**，即上述两种方法中分别需要的数据最大值、最小值，方差和均值都是通过训练集计算得到（这个做法类似于深度学习中批归一化，BN的实现做法）。
5. 归一化不是万能的，实际应用中，**通过梯度下降法求解的模型是需要归一化的，这包括线性回归、逻辑回归、支持向量机、神经网络等模型**。但**决策树模型不需要**，以 C4.5 算法为例，决策树在分裂结点时候主要依据数据集 D 关于特征 x 的信息增益比，而信息增益比和特征是否经过归一化是无关的，归一化不会改变样本在特征 x 上的信息增益。



### 正则化

1. 正则化是**将样本或者特征的某个范数（如 L1、L2 范数）缩放到单位 1**。

假设数据集为：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/20170417171702256.png)

对样本首先计算 Lp 范数，得到：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/20170417171715565.png)

正则化后的结果是：每个属性值除以其 Lp 范数

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/20170417171731785.png)

2. 正则化的过程是针对**单个样本**的，对每个样本将它缩放到单位范数。

   归一化是针对**单个属性**的，需要用到所有样本在该属性上的值。

3. 通常如果使用**二次型（如点积）或者其他核方法计算两个样本之间的相似性**时，该方法会很有用。



## 10.2 特征编码

### 序号编码(Ordinal Encoding)

**定义**：序号编码一般用于**处理类别间具有大小关系**的数据。

比如成绩，可以分为高、中、低三个档次，并且存在“高>中>低”的大小关系，那么序号编码可以对这三个档次进行如下编码：高表示为 3，中表示为 2，低表示为 1，这样转换后依然保留了大小关系。

### 独热编码(One-hot Encoding)

**定义**：独热编码通常用于处理类别间不具有大小关系的特征。

独热编码是采用 **N** 位状态位来对 **N** 个可能的取值进行编码。比如血型，一共有 4 个取值（A、B、AB 以及 O 型），那么独热编码会将血型转换为一个 4 维稀疏向量，分别表示上述四种血型为：

- A型：(1,0,0,0)
- B型：(0,1,0,0)
- AB型：(0,0,1,0)
- O型：(0,0,0,1)

独热编码的优点有以下几个：

- 能够处理**非数值属性**。比如血型、性别等
- 一定程度上扩充了特征。
- 编码后的向量是稀疏向量，只有一位是 1，其他都是 0，可以利用向量的稀疏来**节省存储空间**。
- **能够处理缺失值**。当所有位都是 0，表示发生了缺失。此时可以采用处理缺失值提到的**高维映射**方法，用第 **N+1** 位来表示缺失值。

当然，独热编码也存在一些缺点：

1.高维度特征会带来以下几个方面问题：

- KNN 算法中，**高维空间下两点之间的距离很难得到有效的衡量**；
- 逻辑回归模型中，参数的数量会随着维度的增高而增加，导致**模型复杂，出现过拟合问题**；
- 通常只有部分维度是对分类、预测有帮助，需要**借助特征选择来降低维度**。

2.决策树模型不推荐对离散特征进行独热编码，有以下两个主要原因：

- **产生样本切分不平衡问题，此时切分增益会非常小**。

  比如对血型做独热编码操作，那么对每个特征`是否 A 型、是否 B 型、是否 AB 型、是否 O 型`，会有少量样本是 1 ，大量样本是 0。

  这种划分的增益非常小，因为拆分之后：

  - 较小的那个拆分样本集，它占总样本的比例太小。无论增益多大，乘以该比例之后几乎可以忽略。

  - 较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零。

- **影响决策树的学习**。

  决策树依赖的是**数据的统计信息**。而独热码编码会把数据切分到零散的小空间上。在这些零散的小空间上，统计信息是不准确的，学习效果变差。

  本质是因为**独热编码之后的特征的表达能力较差**。该特征的预测能力被人为的拆分成多份，每一份与其他特征竞争最优划分点都失败。最终该特征得到的重要性会比实际值低。

### 二进制编码(Binary Encoding)

二进制编码主要分为两步：

1. 先采用序号编码给每个类别赋予一个类别 ID；
2. 接着将类别 ID 对应的二进制编码作为结果。

继续以血型为例子，如下表所示：

| 血型 | 类别 ID | 二进制表示 | 独热编码 |
| :--: | :-----: | :--------: | :------: |
|  A   |    1    |   0 0 1    | 1 0 0 0  |
|  B   |    2    |   0 1 0    | 0 1 0 0  |
|  AB  |    3    |   0 1 1    | 0 0 1 0  |
|  O   |    4    |   1 0 0    | 0 0 0 1  |

从上表可以知道，二进制编码本质上是利用**二进制对类别 ID 进行哈希映射，最终得到 0/1 特征向量，并且特征维度小于独热编码，更加节省存储空间**。

### 二元化

**定义**：特征二元化就是将数值型的属性转换为布尔型的属性。通常用于假设属性取值分布是伯努利分布的情形。

特征二元化的算法比较简单。对属性 `j` 指定一个阈值 `m`。

- 如果样本在属性 `j` 上的值大于等于 `m`, 则二元化后为 1；
- 如果样本在属性 `j` 上的值小于 `m`，则二元化为 0

根据上述定义，`m` 是一个关键的超参数，它的取值需要结合模型和具体的任务来选择。

### 离散化

**定义**：顾名思义，离散化就是将连续的数值属性转换为离散的数值属性。

那么什么时候需要采用特征离散化呢？

这背后就是需要采用“**海量离散特征+简单模型**”，还是“**少量连续特征+复杂模型**”的做法了。

- 对于线性模型，通常使用“海量离散特征+简单模型”。
  - 优点：模型简单
  - 缺点：特征工程比较困难，但一旦有成功的经验就可以推广，并且可以很多人并行研究。
- 对于非线性模型（比如深度学习），通常使用“少量连续特征+复杂模型”。
  - 优点：不需要复杂的特征工程
  - 缺点：模型复杂

**分桶**

1.离散化的常用方法是**分桶**：

- 将所有样本在连续的数值属性 `j` 的取值从小到大排列 ${a_0, a_1, ..., a_N}$ 。
- 然后从小到大依次选择分桶边界$b_1, b_2, ..., b_M$  。其中：
  - `M` 为分桶的数量，它是一个超参数，需要人工指定。
  - 每个桶的大小$b_{k+1}-b_k$  也是一个超参数，需要人工指定。
- 给定属性 `j` 的取值$a_i$，对其进行分桶：
  - 如果$a_i < b_1$，则分桶编号是 0。分桶后的属性的取值为 0；
  - 如果$b_k \le a_i \le b_{k+1}$，则分桶编号是 `k`。分桶后的属性取值是 `k`；
  - 如果 $a_i \ge b_M$, 则分桶编号是 `M`。分桶后的属性取值是 `M`。

2.分桶的数量和边界通常需要人工指定。一般有两种方法：

- **根据业务领域的经验来指定**。如：对年收入进行分桶时，根据 2017 年全国居民人均可支配收入约为 2.6 万元，可以选择桶的数量为5。其中：
  - 收入小于 1.3 万元（人均的 0.5 倍），则为分桶 0 。
  - 年收入在 1.3 万元 ～5.2 万元（人均的 0.5～2 倍），则为分桶 1 。
  - 年收入在 5.3 万元～26 万元（人均的 2 倍～10 倍），则为分桶 2 。
  - 年收入在 26 万元～260 万元（人均的 10 倍～100 倍），则为分桶 3 。
  - 年收入超过 260 万元，则为分桶 4 。
- **根据模型指定**。根据具体任务来训练分桶之后的数据集，通过超参数搜索来确定最优的分桶数量和分桶边界。

3.选择分桶大小时，有一些经验指导：

- **分桶大小必须足够小**，使得桶内的属性取值变化对样本标记的影响基本在一个不大的范围。

  即不能出现这样的情况：单个分桶的内部，样本标记输出变化很大。

- **分桶大小必须足够大，使每个桶内都有足够的样本**。

  如果桶内样本太少，则随机性太大，不具有统计意义上的说服力。

- 每个桶内的样本尽量**分布均匀**。

**特性**

1.在工业界很少直接将连续值作为逻辑回归模型的特征输入，而是**将连续特征离散化为一系列 0/1 的离散特征**。

其优势有：

- 离散化之后得到的稀疏向量，**内积乘法运算速度更快，计算结果方便存储**。

- 离散化之后的特征对于**异常数据具有很强的鲁棒性**。

  如：销售额作为特征，当销售额在 `[30,100)` 之间时，为1，否则为 0。如果未离散化，则一个异常值 10000 会给模型造成很大的干扰。由于其数值较大，它对权重的学习影响较大。

- 逻辑回归属于广义线性模型，表达能力受限，只能描述线性关系。特征离散化之后，相当于**引入了非线性，提升模型的表达能力，增强拟合能力**。

  假设某个连续特征 `j`  ，它离散化为 `M` 个 0/1 特征 $j_1, j_2, ..., j_M$  。则：$w_j * x_j -> w_{j1} * x_{j1}^` + w_{j2} * x_{j2}^` + ...+w_{jM} * x_{jM}^` $。其中 $x_{j1}^`，x_{j2}^`，..., x_{jM}^`$ 是离散化之后的新的特征，它们的取值空间都是  {0, 1}。

  上式右侧是一个分段线性映射，其表达能力更强。

- **离散化之后可以进行特征交叉**。假设有连续特征`j` ，离散化为 `N` 个 0/1 特征；连续特征 `k`，离散化为 `M` 个 0/1 特征，则分别进行离散化之后引入了 `N+M` 个特征。

  假设离散化时，并不是独立进行离散化，而是特征 `j,k`  联合进行离散化，则可以得到  `N*M` 个组合特征。**这会进一步引入非线性，提高模型表达能力**。

- **离散化之后，模型会更稳定**。

  如对销售额进行离散化，`[30,100)` 作为一个区间。当销售额在40左右浮动时，并不会影响它离散化后的特征的值。

  但是**处于区间连接处的值要小心处理，另外如何划分区间也是需要仔细处理**。

2.**特征离散化简化了逻辑回归模型，同时降低模型过拟合的风险**。

能够对抗过拟合的原因：**经过特征离散化之后，模型不再拟合特征的具体值，而是拟合特征的某个概念**。因此能够**对抗数据的扰动，更具有鲁棒性**。

另外它使得模型要**拟合的值大幅度降低，也降低了模型的复杂度**。

## 10.3 特征选择

**定义**：从给定的特征集合中选出相关特征子集的过程称为特征选择(feature selection)。

1.对于一个学习任务，给定了属性集，其中某些属性可能对于学习来说很关键，但有些属性意义就不大。

- 对当前学习任务有用的属性或者特征，称为**相关特征**(relevant feature)；
- 对当前学习任务没用的属性或者特征，称为**无关特征**(irrelevant feature)。

2.特征选择可能会降低模型的预测能力，因为被剔除的特征中可能包含了有效的信息，抛弃这部分信息一定程度上会降低模型的性能。但这也是计算复杂度和模型性能之间的取舍：

- 如果保留尽可能多的特征，模型的性能会提升，但同时模型就变复杂，计算复杂度也同样提升；
- 如果剔除尽可能多的特征，模型的性能会有所下降，但模型就变简单，也就降低计算复杂度。

3.常见的特征选择分为三类方法：

- 过滤式(filter)
- 包裹式(wrapper)
- 嵌入式(embedding)

### 特征选择原理

1.采用特征选择的原因：

- **维数灾难问题**。因为属性或者特征过多造成的问题，如果可以选择重要的特征，使得仅需要一部分特征就可以构建模型，可以大大减轻维数灾难问题，从这个意义上讲，特征选择和降维技术有相似的动机，事实上它们也是处理高维数据的两大主流技术。
- **去除无关特征可以降低学习任务的难度，也同样让模型变得简单，降低计算复杂度**。

2.特征选择最重要的是**确保不丢失重要的特征**，否则就会因为缺少重要的信息而无法得到一个性能很好的模型。

- 给定数据集，学习任务不同，相关的特征很可能也不相同，因此特征选择中的**不相关特征指的是与当前学习任务无关的特征**。
- 有一类特征称作**冗余特征(redundant feature)**，它们所包含的信息可以从其他特征中推演出来。
  - 冗余特征通常都不起作用，去除它们可以减轻模型训练的负担；
  - 但如果冗余特征恰好对应了完成学习任务所需要的某个中间概念，则它是有益的，可以降低学习任务的难度。

3.在没有任何先验知识，即领域知识的前提下，要想从初始特征集合中选择一个包含所有重要信息的特征子集，唯一做法就是遍历所有可能的特征组合。

但这种做法并不实际，也不可行，因为会遭遇组合爆炸，特征数量稍多就无法进行。

一个可选的方案是：

- 产生一个候选子集，评价出它的好坏。
- 基于评价结果产生下一个候选子集，再评价其好坏。
- 这个过程持续进行下去，直至无法找到更好的后续子集为止。

这里有两个问题：如何根据评价结果获取下一个候选特征子集？如何评价候选特征子集的好坏？

#### 子集搜索

1.**子集搜索**方法步骤如下：

- 给定特征集合$A={A_1,A_2,...,A_d}$  ，首先将每个特征看作一个候选子集（即每个子集中只有一个元素），然后对这 $d$ 个候选子集进行评价。

  假设 $A_2$ 最优，于是将  $A_2$  作为第一轮的选定子集。

- 然后在上一轮的选定子集中加入一个特征，构成了包含两个特征的候选子集。

  假定 $A_2$,A_5   最优，且优于  $A_2$  ，于是将  $A_2,A_5$  作为第二轮的选定子集。

- ....

- 假定在第 `k+1` 轮时，**本轮的最优的特征子集不如上一轮的最优的特征子集**，则停止生成候选子集，并将上一轮选定的特征子集作为特征选择的结果。

2.这种逐渐增加相关特征的策略称作**前向 `forward`搜索**

类似地，如果从完整的特征集合开始，每次尝试去掉一个无关特征，这种逐渐减小特征的策略称作**后向`backward`搜索**

3.也可以将前向和后向搜索结合起来，每一轮逐渐增加选定的相关特征（这些特征在后续迭代中确定不会被去除），同时减少无关特征，这样的策略被称作是**双向`bidirectional`搜索**。

4该策略是贪心的，因为它们仅仅考虑了使本轮选定集最优。但是除非进行穷举搜索，否则这样的问题无法避免。

#### 子集评价

1.子集评价的做法如下：

给定数据集 D，假设所有属性均为离散型。对属性子集 A，假定根据其取值将 D 分成了 V 个子集：${D_1, D_2, \cdots,  D_V}$

可以计算属性子集 A 的信息增益：
$$
g(D, A) = H(D) - H(D|A)=H(D)-\sum^V_{v=1}\frac{|D_v|}{|D|}H(D_v)
$$
其中，$|•|$表示集合大小，$H(•)$表示熵。

**信息增益越大，表明特征子集 A 包含的有助于分类的信息越多**。所以对于每个候选特征子集，可以基于训练集 D 来计算其信息增益作为评价准则。

2.更一般地，特征子集 A 实际上确定了对数据集 D 的一个划分规则。

- 每个划分区域对应着 A 上的一个取值，而样本标记信息 y 则对应着 D 的真实划分。
- 通过估算这两种划分之间的差异，就能对 A 进行评价：与 y 对应的划分的差异越小，则说明 A 越好。
- **信息熵仅仅是判断这个差异的一种方法，其他能判断这两个划分差异的机制都能够用于特征子集的评价**。

3.**将特征子集搜索机制与子集评价机制结合就能得到特征选择方法**。

- 事实上，决策树可以用于特征选择，所有树结点的划分属性所组成的集合就是选择出来的特征子集。
- 其他特征选择方法本质上都是显式或者隐式地结合了某些子集搜索机制和子集评价机制。

4.常见的特征选择方法分为以下三种，主要区别在于特征选择部分是否使用后续的学习器。

- **过滤式**(filter)：先对数据集进行特征选择，其过程与后续学习器无关，即设计一些统计量来过滤特征，并不考虑后续学习器问题
- **包裹式**(wrapper)：实际上就是一个分类器，它是将后续的学习器的性能作为特征子集的评价标准。
- **嵌入式**(embedding)：实际上是学习器自主选择特征。

5.最简单的特征选择方法是：**去掉取值变化小的特征**。

假如某特征只有 0 和 1 的两种取值，并且所有输入样本中，95% 的样本的该特征取值都是 1 ，那就可以认为该特征作用不大。

当然，该方法的一个前提是，特征值都是**离散型**才使用该方法；如果是连续型，需要离散化后再使用，并且实际上一般不会出现 95% 以上都取某个值的特征的存在。

所以，这个方法简单，但不太好用，可以作为特征选择的一个预处理，先去掉变化小的特征，然后再开始选择上述三种类型的特征选择方法。



### 过滤式选择

该方法**先对数据集进行特征选择，然后再训练学习器**。特征选择过程与后续学习器无关。

也就是先采用特征选择对初始特征进行过滤，然后用过滤后的特征训练模型。

- 优点是**计算时间上比较高效，而且对过拟合问题有较高的鲁棒性**；
- 缺点是**倾向于选择冗余特征**，即没有考虑到特征之间的相关性。

#### Relief 方法

1.`Relief:Relevant Features`是一种著名的过滤式特征选择方法。该方法设计了一个**相关统计量来度量特征的重要性**。

- 该统计量是一个向量，其中每个分量都对应于一个初始特征。特征子集的重要性则是由**该子集中每个特征所对应的相关统计量分量之和来决定的**。

- 最终只需要指定一个阈值 $\gamma$，然后**选择比 $\gamma$ 大的相关统计量分量所对应的特征**即可。

  也可以**指定特征个数 m** ，然后选择相关统计量分量最大的 m 个特征。

2.给定训练集$D={(\vec{x_1}, \breve{y_1}), \cdots, (\vec{x_N}, \breve{y_N})}, \breve{y_i}\in{0, 1}$。对于每个样本$\vec{x_i}$：

- `Relief` 先在$\vec{x_i}$同类样本中寻找其最近邻 $\vec{x_{nm_i}}$，称作猜中近邻 `near-hit` ；
- 然后从$\vec{x_i}$ 的异类样本寻找最近邻$\vec{x_{nm_i}}$，称作猜错近邻`near-miss`。
- 然后相关统计量对应于属性`j`的分量为：

$$
\delta_j = \sum^N_{i=1}(-diff(x_{i,j}, x_{nh_i,j})^2 + diff(x_{i,j},x_{nm_i,j})^2)
$$

其中$diff(x_{a,j},x_{b,j})$是两个样本在属性 `j` 上的差异值，其结果取决于该属性是离散的还是连续的：

- 如果是离散的，则有

$$
    diff(x{a,j},x{b,j})=
    \begin{cases}
    0,\quad if\; x_{a,j}=x_{b,j}\\
    1, \quad else
    \end{cases}
$$

- 如果是连续的，则有

$$
diff(x{a,j},x{b,j})=|x_{a,j} - x_{b,j}|
$$

     注意，此时需要对$x_{a,j},x_{b,j}$进行标准化到`[0,1]`区间。

3.根据 3 的公式可以知道，如果

- 如果 $\vec{x_i}$ 与其猜中近邻  $\vec{x_{nh_i}}$ 在属性 `j` 上的距离小于 $\vec{x_i}$ 与其猜错近邻 $\vec{x_{nm_i}}$ 的距离，则说明属性 `j` 对于区分同类与异类样本是有益的，于是增大属性 `j` 所对应的统计量分量。
- 如果 $\vec{x_i}$ 与其猜中近邻  $\vec{x_{nh_i}}$ 在属性 `j` 上的距离大于$\vec{x_i}$  与其猜错近邻 $\vec{x_{nm_i}}$ 的距离，则说明属性 `j` 对于区分同类与异类样本是起负作用的，于是减小属性 `j` 所对应的统计量分量。
- 最后对基于不同样本得到的估计结果进行平均，就得到各属性的相关统计量分量。**分量值越大，则对应属性的分类能力越强**。

4.`Relief` 是为二分类问题设计的，其拓展变体 `Relief-F` 可以处理多分类问题。

假定数据集 D 中的样本类别为：${c_1, c_2, \cdots, c_K}$  。对于样本 $\vec{x_i}$ ，假设 $\breve{y_i}=c_k$ 。

- `Relief-F` 先在类别 $c_k$ 的样本中寻找 $\vec{x_i}$ 的最近邻 $\vec{x_{nh_i}}$ 作为猜中近邻。
- 然后在 $c_k$  之外的每个类别中分别找到一个 $\vec{x_i}$的最近邻 $\vec{x_{nm_i^l}}, l=1,2,\cdots,K; l \neq k$$   作为猜错近邻。
- 于是相关统计量对应于属性  j 的分量为：

$$
\delta_j = \sum^N_{i=1}(-diff(x_{i,j},x_{nh_{i,j}})^2+\sum_{l\neq k}(p_l\times diff(x_{i,j},x_{nm_{i,j}^l})^2))
$$

其中 $p_l$ 作为第 $l$ 类的样本在数据集 D 中所占的比例。

#### 方差选择法

使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用 sklearn 的 feature_selection 库的 VarianceThreshold 类来选择特征的代码如下：

```python
from sklearn.feature_selection import VarianceThreshold

#方差选择法，返回值为特征选择后的数据
#参数threshold为方差的阈值
VarianceThreshold(threshold=3).fit_transform(data)
```



#### 相关系数法

使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的 P 值。用 feature_selection 库的 SelectKBest 类结合相关系数来选择特征的代码如下：

```python
from sklearn.feature_selection import SelectKBest
from scipy.stats import pearsonr

#选择 K 个最好的特征，返回选择特征后的数据
#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数
#参数k为选择的特征个数
SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
```



#### 卡方检验

经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有 N 种取值，因变量有 M 种取值，考虑自变量等于 i 且因变量等于 j 的样本频数的观察值与期望的差距，构建统计量：
$$
X^2 = \sum\frac{(A-E)^2}{E}
$$
不难发现，这个统计量的含义简而言之就是自变量对因变量的相关性。用 feature_selection 库的 SelectKBest 类结合卡方检验来选择特征的代码如下：

```python
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

#选择K个最好的特征，返回选择特征后的数据
SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)
```

#### 互信息法

经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：
$$
I(X;Y)=\sum_{x\in X}\sum_{y\in Y}p(x,y)log\frac{p(x,y)}{p(x)p(y)}
$$
为了处理定量数据，**最大信息系数法**被提出，使用 feature_selection 库的 SelectKBest 类结合最大信息系数法来选择特征的代码如下

```python
from sklearn.feature_selection import SelectKBest
 from minepy import MINE
 
 #由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5
 def mic(x, y):
     m = MINE()
     m.compute_score(x, y)
     return (m.mic(), 0.5)

#选择K个最好的特征，返回特征选择后的数据
SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
```



### 包裹式选择

1.相比于过滤式特征选择不考虑后续学习器，包裹式特征选择**直接把最终将要使用的学习器的性能作为特征子集的评价原则**。其目的就是为给定学习器选择最有利于其性能、量身定做的特征子集。

- 优点是直接针对特定学习器进行优化，考虑到特征之间的关联性，因此**通常包裹式特征选择比过滤式特征选择能训练得到一个更好性能的学习器**，
- 缺点是由于特征选择过程需要多次训练学习器，故计算**开销要比过滤式特征选择要大得多**。

2.`LVW:Las Vegas Wrapper`是一个典型的包裹式特征选择方法。它是` Las Vegas method`   框架下使用随机策略来进行子集搜索，并以最终分类器的误差作为特征子集的评价标准。

3. `LVW` 算法：

- 输入：数据集 D，特征集 A，学习器 estimator, 迭代停止条件 T

- 输出：最优特征子集 $A^*$

- 算法步骤：

  - 初始化：令候选的最优特征子集$\breve{A^*}=A$, 然后学习器 estimator 在特征子集 $\breve{A^*}$上使用交叉验证法进行学习，通过学习结果评估学习器的误差 $err^*$。
  - 迭代，停止条件为迭代次数达到 T。迭代过程是：
    - 随机产生特征子集 $A^\prime$
    - 学习器在特征子集 $A^\prime$ 上使用交叉验证法进行学习，通过学习评估学习器的误差 $err^\prime$
    - 如果 $err^\prime$ 小于 $err^*$，或者$err^\prime = err^*$，但是特征子集 $A^\prime$ 的特征数量少于候选的最优特征子集$\breve{A^*}$，则令$A^\prime$ 为候选的最优特征子集，即 $\breve{A^*} = A^\prime; err^* = err^\prime$

  - 最终有 $A^* = \breve{A^*}$

4.由于 `LVW` 算法中**每次特征子集评价都需要训练学习器，计算开销很大**，因此算法设计了停止条件控制参数 $T$。

但是如果初始特征数量很多、$T$ 设置较大、以及每一轮训练的时间较长， 则很可能算法运行很长时间都不会停止。即：**如果有运行时间限制，则有可能给不出解**。

5.**递归特征消除法**：使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用 feature_selection 库的 RFE 类来选择特征的代码如下：

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

#递归特征消除法，返回特征选择后的数据
#参数estimator为基模型
#参数n_features_to_select为选择的特征个数
RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target)
```



### 嵌入式选择

1.在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别。

嵌入式特征选择是将特征选择与学习器训练过程融为一体，两者在同一个优化过程中完成的。**即学习器训练过程中自动进行了特征选择**。

常用的方法包括：

- 利用正则化，如`L_1, L_2` 范数，主要应用于如线性回归、逻辑回归以及支持向量机(SVM)等算法；
- 使用决策树思想，包括决策树、随机森林、Gradient Boosting 等。

2.以线性回归模型为例。给定数据集 $D={(\vec{x_1}, \breve{y_1}), \cdots, (\vec{x_N}, \breve{y_N})}, \breve{y_i}\in R$。以平方差为损失函数，则优化目标是 $min_{\vec{w}} \sum^N_{i=1}(\breve{y_i}-\vec{w^T}\vec{x_i})^2$

- 如果使用 $L_2$ 范数正则化，优化目标就是 $min_{\vec{w}} \sum^N_{i=1}(\breve{y_i}-\vec{w^T}\vec{x_i})^2+ \lambda|| \vec{w}||^2_2 , \lambda > 0$，此时称作**岭回归**(ridge regression)。 
- 如果使用 $L_1$ 范数正则化，优化目标就是 $min_{\vec{w}} \sum^N_{i=1}(\breve{y_i}-\vec{w^T}\vec{x_i})^2+ \lambda|| \vec{w}||_1 , \lambda > 0$，此时称作**LASSO(Least Absolute Shrinkage and Selection Operator)回归**。 

3.引入 $L_1$ 范数除了**降低过拟合风险**之外，还有一个好处：它求得的 $\vec{w}$ 会有较多的分量为零。即：**它更容易获得稀疏解**。

于是基于 $L_1$ 正则化的学习方法就是一种嵌入式特征选择方法，其特征选择过程与学习器训练过程融为一体，二者同时完成。

4.常见的嵌入式选择模型：

- 在 `Lasso` 中，$\lambda$ 参数控制了稀疏性：
  - 如果 $\lambda$ 越小，则稀疏性越小，被选择的特征越多；
  - 相反，$\lambda$ 越大，则稀疏性越大，被选择的特征越少；
- 在 `SVM` 和 逻辑回归中，参数 `C` 控制了稀疏性：
  - 如果 `C` 越小，则稀疏性越大，被选择的特征越少；
  - 如果 `C` 越大， 则稀疏性越小，被选择的特征越多。



## 10.4特征提取

特征提取一般是在特征选择之前，它提取的对象是**原始数据**，目的就是自动地构建新的特征，**将原始数据转换为一组具有明显物理意义（比如 Gabor、几何特征、纹理特征）或者统计意义的特征**。

一般常用的方法包括降维（PCA、ICA、LDA等）、图像方面的SIFT、Gabor、HOG等、文本方面的词袋模型、词嵌入模型等，这里简单介绍这几种方法的一些基本概念。

### 降维

1.**PCA**(Principal Component Analysis，主成分分析)

PCA 是降维最经典的方法，它旨在是找到**数据中的主成分，并利用这些主成分来表征原始数据，从而达到降维的目的**。

PCA 的思想是通过坐标轴转换，寻找数据分布的最优子空间。

比如，在三维空间中有一系列数据点，它们分布在过原点的平面上，如果采用自然坐标系的 x，y，z 三个轴表示数据，需要三个维度，但实际上这些数据点都在同一个二维平面上，如果我们可以通过坐标轴转换使得数据所在平面和 x，y 平面重合，我们就可以通过新的 x'、y' 轴来表示原始数据，并且没有任何损失，这就完成了降维的目的，而且这两个新的轴就是我们需要找的主成分。

因此，PCA 的解法一般分为以下几个步骤：

1. 对样本数据进行中心化处理；
2. 求样本协方差矩阵；
3. 对协方差矩阵进行特征值分解，将特征值从大到小排列；
4. 取特征值前 n 个最大的对应的特征向量 `W1, W2, ..., Wn` ，这样将原来 m 维的样本降低到 n 维。

通过 PCA ，就可以将方差较小的特征给抛弃，这里，特征向量可以理解为坐标转换中新坐标轴的方向，特征值表示在对应特征向量上的方差，**特征值越大，方差越大，信息量也就越大**。这也是为什么选择前 n 个最大的特征值对应的特征向量，因为这些特征包含更多重要的信息。

**PCA 是一种线性降维方法，这也是它的一个局限性**。不过也有很多解决方法，比如采用核映射对 PCA 进行拓展得到核主成分分析(KPCA)，或者是采用流形映射的降维方法，比如等距映射、局部线性嵌入、拉普拉斯特征映射等，对一些 PCA 效果不好的复杂数据集进行非线性降维操作。

2.**LDA**(Linear Discriminant Analysis，线性判别分析)

LDA 是一种有监督学习算法，相比较 PCA，它考虑到数据的类别信息，而 PCA 没有考虑，只是将数据映射到方差比较大的方向上而已。

因为考虑数据类别信息，所以 LDA 的目的不仅仅是降维，还需要找到一个投影方向，使得投影后的样本尽可能按照原始类别分开，即寻找一个**可以最大化类间距离以及最小化类内距离**的方向。

LDA 的主要步骤如下：

1. 分别计算每个类别 i 的原始中心点：$m_i = \frac{1}{n_i}\sum{x}$
2. 类别 i 投影后的中心点为: $m_i = w^Tm_i$
3. 衡量类别 i 投影后，类间的分散程度，用方差来表示：$s_i = \sum{(y-m_i)^2}$
4. 使用下面的式子表示 LDA 投影到 w 后的损失函数，最大化 J(w) 可以求出最优的 w , $J(w) = \frac{|m_1-m_2|^2}{s_1^2 + s_2^2}$，具体的解法参考[机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)](https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html)。

LDA 的优点如下：

- 相比较 PCA，LDA 更加擅长处理带有类别信息的数据；
- 线性模型对噪声的鲁棒性比较好，LDA 是一种有效的降维方法。

相应的，也有如下缺点：

- LDA 对**数据的分布做出了很强的假设**，比如每个类别数据都是高斯分布、各个类的协方差相等。这些假设在实际中不一定完全满足。
- **LDA 模型简单，表达能力有一定局限性**。但这可以通过引入**核函数**拓展 LDA 来处理分布比较复杂的数据。

3.**ICA**(Independent Component Analysis，独立成分分析)

PCA特征转换降维，提取的是**不相关**的部分，ICA独立成分分析，获得的是**相互独立**的属性。ICA算法本质寻找一个线性变换 `z = Wx`，使得 z 的**各个特征分量之间的独立性最大**。

通常先采用 PCA 对数据进行降维，然后再用 ICA 来从多个维度分离出有用数据。PCA 是 ICA 的数据预处理方法。

具体可以查看知乎上的这个问题和回答 [独立成分分析 ( ICA ) 与主成分分析 ( PCA ) 的区别在哪里？](https://www.zhihu.com/question/28845451)。



### 图像特征提取

图像的特征提取，在深度学习火起来之前，是有很多传统的特征提取方法，比较常见的包括以下几种。

1.**SIFT** 特征

SIFT 是图像特征提取中非常广泛应用的特征。它包含以下几种优点：

- 具有旋转、尺度、平移、视角及亮度不变性，有利于对目标特征信息进行有效表达；
- SIFT 特征对参数调整鲁棒性好，可以根据场景需要调整适宜的特征点数量进行特征描述，以便进行特征分析。

SIFT 对图像局部特征点的提取主要包括四个步骤：

1. 疑似特征点检测
2. 去除伪特征点
3. 特征点梯度与方向匹配
4. 特征描述向量的生成

SIFT 的缺点是不借助硬件加速或者专门的图像处理器很难实现。

2.**SURF** 特征

SURF 特征是对 SIFT 算法的改进，降低了时间复杂度，并且提高了鲁棒性。

它主要是简化了 SIFT 的一些运算，如将 SIFT 中的高斯二阶微分的模型进行了简化，使得卷积平滑操作仅需要转换成加减运算。并且最终生成的特征向量维度从 128 维减少为 64 维。

3.**HOG** 特征

方向梯度直方图(HOG)特征是 2005 年针对行人检测问题提出的直方图特征，它通过计算和统计图像局部区域的梯度方向直方图来实现特征描述。

HOG 特征提取步骤如下：

1. **归一化处理**。先将图像转为灰度图像，再利用伽马校正实现。这一步骤是为了提高图像特征描述对光照及环境变化的鲁棒性，降低图像局部的阴影、局部曝光过多和纹理失真，尽可能抵制噪声干扰；
2. **计算图像梯度**；
3. **统计梯度方向**；
4. **特征向量归一化**；为克服光照不均匀变化及前景与背景的对比差异，需要对块内的特征向量进行归一化处理。
5. **生成特征向量**。



4.**LBP** 特征

局部二值模式（LBP）是一种描述图像局部纹理的特征算子，它具有旋转不变性和灰度不变性的优点。

LBP 特征描述的是一种灰度范围内的图像处理操作技术，针对的是**输入为 8 位或者 16 位的灰度图像**。

LBP 特征通过对**窗口中心点与邻域点的关系进行比较**，重新编码形成新特征以消除对外界场景对图像的影响，因此一定程度上解决了**复杂场景下（光照变换）特征**描述问题。

根据窗口领域的不同分为两种，经典 LBP 和圆形 LBP。前者的窗口是 3×3 的正方形窗口，后者将窗口从正方形拓展为任意圆形领域。

更详细的可以参考这篇文章--[图像特征检测描述(一):SIFT、SURF、ORB、HOG、LBP特征的原理概述及OpenCV代码实现](https://blog.csdn.net/wenhao_ir/article/details/52046569)

当然上述特征都是比较传统的图像特征提取方法了，现在图像基本都直接利用 CNN（卷积神经网络）来进行特征提取以及分类。



### 文本特征提取

1.**词袋模型**

最基础的文本表示模型是词袋模型。

具体地说，就是将整段文本以词为单位切分开，然后每篇文章可以表示成一个长向量，向量的每一个维度代表一个单词，而该维度的权重反映了该单词在原来文章中的重要程度。

通常采用 **TF-IDF** 计算权重，公式为 `TF-IDF(t, d) = TF(t,d) × IDF(t)`

其中 TF(t, d) 表示单词 t 在文档 d 中出现的频率，IDF(t) 是逆文档频率，用来衡量单词 t 对表达语义所起的重要性，其表示为：
$$
IDF(t)=log\frac{文章总数}{包含单词 t 的文章总数+1}
$$
直观的解释就是，如果这个单词在多篇文章都出现过，那么它很可能是比较通用的词汇，对于区分文章的贡献比较小，自然其权重也就比较小，即 IDF(t) 会比较小。

2.**N-gram 模型**

词袋模型是以单词为单位进行划分，但有时候进行单词级别划分并不是很好的做法，毕竟有的单词组合起来才是其要表达的含义，比如说 `natural language processing(自然语言处理)`、`computer vision(计算机视觉)` 等。

因此可以将连续出现的 n 个词 (n <= N) 组成的词组(N-gram)作为一个单独的特征放到向量表示中，构成了 N-gram 模型。

另外，同一个词可能会有多种词性变化，但却具有相同含义，所以实际应用中还会对单词进行**词干抽取**(Word Stemming)处理，即将不同词性的单词统一为同一词干的形式。

3.**词嵌入模型**

词嵌入是一类将词向量化的模型的统称，核心思想是将**每个词都映射成低维空间（通常 K=50~300 维）上的一个稠密向量（Dense Vector）**。

常用的词嵌入模型是 **Word2Vec**。它是一种底层的神经网络模型，有两种网络结构，分别是 CBOW(Continues Bag of Words) 和 Skip-gram。

CBOW 是根据**上下文出现的词语预测当前词**的生成概率；Skip-gram 是根据**当前词来预测上下文中各个词的生成概率**。

词嵌入模型是将每个词都映射成一个 K 维的向量，如果一篇文档有 N 个单词，那么每篇文档就可以用一个 N×K 的矩阵进行表示，但这种表示过于底层。实际应用中，如果直接将该矩阵作为原文本的特征表示输入到模型中训练，通常很难得到满意的结果，一般还需要对该矩阵进行处理，提取和构造更高层的特征。

深度学习模型的出现正好提供了一种自动进行特征工程的方法，它的每个隐含层都相当于不同抽象层次的特征。卷积神经网络（CNN)和循环神经网络（RNN)在文本表示中都取得了很好的效果，这是因为它们可以很好地对文本进行建模，抽取出一些高层的语义特征。



### 特征提取和特征选择的区别

特征提取与特征选择都是为了从原始特征中找出最有效的特征。

它们之间的区别是特征提取强调通过**特征转换**的方式得到一组具有明显物理或统计意义的特征；

而特征选择是从特征集合中挑选一组具有明显物理或统计意义的**特征子集**。

两者都能**帮助减少特征的维度、数据冗余**，特征提取有时能发现更有意义的特征属性，特征选择的过程经常能表示出每个特征的重要性对于模型构建的重要性。



## 10.5 特征构建

特征构建是指**从原始数据中人工的构建新的特征**。需要花时间去观察原始数据，思考问题的潜在形式和数据结构，对数据敏感性和机器学习实战经验能帮助特征构建。

特征构建需要很强的洞察力和分析能力，要求我们能够从原始数据中找出一些具有物理意义的特征。假设原始数据是表格数据，一般你可以使用**混合属性或者组合属性**来创建新的特征，或是**分解或切分原有的特征**来创建新的特征。

特征构建非常需要相关的领域知识或者丰富的实践经验才能很好构建出更好的有用的新特征，相比于特征提取，特征提取是通过一些现成的特征提取方法来将原始数据进行特征转换，而特征构建就需要我们自己人为的手工构建特征，比如组合两个特征，或者分解一个特征为多个新的特征。

------

## 参考

- 《hands-on-ml-with-sklearn-and-tf》第二节
- https://www.jiqizhixin.com/articles/091202
- https://blog.csdn.net/fendegao/article/details/79968994
- https://blog.csdn.net/xg123321123/article/details/80781611
- https://towardsdatascience.com/top-sources-for-machine-learning-datasets-bb6d0dc3378b
- 《百面机器学习》第一章 特征工程
- [机器学习之特征工程](https://blog.csdn.net/dream_angel_z/article/details/49388733#commentBox)
- [[数据预处理（方法总结）]](https://www.cnblogs.com/sherial/archive/2018/03/07/8522405.html)
- [Python数据分析（三）——数据预处理](https://gofisher.github.io/2018/06/22/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/)
- [Python数据分析（二）——数据探索](https://gofisher.github.io/2018/06/20/%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2/)
- [【Python数据分析基础】: 异常值检测和处理](https://juejin.im/post/5b6a44f55188251aa8294b8c)
- http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/8_feature_selection.html
- [Python机器学习-数据预处理技术（标准化处理、归一化、二值化、独热编码、标记编码）](https://blog.csdn.net/weixin_38168620/article/details/79233086)
- [[Scikit-learn介绍几种常用的特征选择方法](http://dataunion.org/14072.html)](http://dataunion.org/14072.html)
- [博客园--机器学习之特征工程](https://www.cnblogs.com/wxquare/p/5484636.html)
- [机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)](https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html)
- [独立成分分析 ( ICA ) 与主成分分析 ( PCA ) 的区别在哪里？](https://www.zhihu.com/question/28845451)
- [图像特征检测描述(一):SIFT、SURF、ORB、HOG、LBP特征的原理概述及OpenCV代码实现](https://blog.csdn.net/wenhao_ir/article/details/52046569)
- [Gabor特征提取](https://blog.csdn.net/xidianzhimeng/article/details/19493019)







# 11. 逻辑回归

## 简介

逻辑回归算法基于 Sigmoid 函数，或者说 Sigmoid 就是逻辑回归函数。Sigmoid 函数定义如下：
$\frac{1}{1+e^{-z}}$。函数值域范围(0,1)。

因此逻辑回归函数的表达式如下：
$$
h_\theta(x) =g(\theta^T X) = \frac{1}{1+e^{-\theta^TX}} \\
其中，g(z) = \frac{1}{1+e^{-z}}h_\theta(x) =g(\theta^T X) = \frac{1}{1+e^{-\theta^TX}} \\
其中，g(z) = \frac{1}{1+e^{-z}}
$$
其导数形式为：
$$
g\prime (z)  =  \frac{d}{dz} \frac{1}{1+e^{-z}} \\
		 = \frac{1}{(1+e^{-z})^2} (e^{-z}) \\
		 =  \frac{1}{1+e^{-z}} (1-  \frac{1}{1+e^{-z}}) \\
		 = g(z)(1-g(z))
$$



## 代价函数

逻辑回归方法主要是用**最大似然估计**来学习的，所以单个样本的后验概率为：
$$
p(y | x; \theta) = (h_\theta(x))^y(1-h_\theta(x))^{1-y}
$$
到整个样本的后验概率就是:
$$
L(\theta) = p(y | X;\theta) \\
	      = \prod_{i=1}^{m} p(y^{(i)} | x^{(i)};\theta)\\
	      = \prod_{i=1}^{m} (h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{1-y^{(i)}}
$$
其中，$P(y=1|x;\theta) = h_\theta(x), P(y=0|x;\theta)=1-h_\theta(x)$。

通过对数进一步简化有：$l(\theta) = logL(\theta) = \sum_{i=1}^{m}y^{(i)}logh(x^{(i)})+(1-y^{(i)})log(1-h(x^{(i)}))$.

而逻辑回归的代价函数就是$-l(\theta)$。也就是如下所示：
$$
J(\theta) = -\frac{1}{m} [\sum_{i=1}^my^{(i)}logh_\theta(x^{(i)})+(1-y^{(i)})log(1-h_\theta(x^{(i)}))]
$$

同样可以使用梯度下降算法来求解使得代价函数最小的参数。其梯度下降法公式为：

![](https://img-blog.csdn.net/20170212181541232?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

![](https://img-blog.csdn.net/20170212181600234?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)



## 优缺点

### 优点

1. 实现简单，广泛的应用于工业问题上；
2. 分类时计算量非常小，速度很快，存储资源低；
3. 便于观测样本概率分数
4. 对逻辑回归而言，多重共线性并不是问题，它可以结合L2正则化来解决该问题。

### 缺点

1. 容易**欠拟合，一般准确度不太高**
2. 只能处理**两分类**问题（在此基础上衍生出来的softmax可以用于多分类），且必须**线性可分**；
3. **特征空间很大**时，逻辑回归的性能不是很好；
4. 不能很好地处理**大量多类特征或变量**
5. 对于非线性特征，需要进行转换。





## 回归划分

广义线性模型家族里，依据因变量不同，可以有如下划分：

（1）如果是连续的，就是多重线性回归。

（2）如果是二项分布，就是逻辑回归。

（3）如果是泊松（Poisson）分布，就是泊松回归。

（4）如果是负二项分布，就是负二项回归。

（5）逻辑回归的因变量可以是二分类的，也可以是多分类的，但是二分类的更为常用，也更加容易解释。所以实际中最常用的就是二分类的逻辑回归。





## 逻辑回归的用途

逻辑回归可用于以下几个方面：

（1）**用于概率预测**。用于可能性预测时，得到的结果有可比性。比如根据模型进而预测在不同的自变量情况下，发生某病或某种情况的概率有多大。

（2）**用于分类**。实际上跟预测有些类似，也是根据模型，判断某人属于某病或属于某种情况的概率有多大，也就是看一下这个人有多大的可能性是属于某病。进行分类时，仅需要设定一个阈值即可，可能性高于阈值是一类，低于阈值是另一类。

（3）**寻找危险因素**。寻找某一疾病的危险因素等。

（4）**仅能用于线性问题**。只有当目标和特征是线性关系时，才能用逻辑回归。在应用逻辑回归时注意两点：一是当知道模型是非线性时，不适用逻辑回归；二是当使用逻辑回归时，应注意选择和目标为线性关系的特征。

（5）**各特征之间不需要满足条件独立假设，但各个特征的贡献独立计算**。



## 逻辑回归和朴素贝叶斯的区别

逻辑回归与朴素贝叶斯区别有以下几个方面：

1. 逻辑回归是判别模型， 朴素贝叶斯是生成模型，所以生成和判别的所有区别它们都有。
2. 朴素贝叶斯属于贝叶斯，逻辑回归是最大似然，两种概率哲学间的区别。
3. 朴素贝叶斯需要条件独立假设。
4. 逻辑回归需要求特征参数间是线性的。



## 逻辑回归和线性回归的区别

线性回归与逻辑回归的区别如下描述：

（1）线性回归的样本的输出，都是连续值，$ y\in (-\infty ,+\infty )$，而逻辑回归中$y\in (0,1)$，只能取0和1。

（2）对于**拟合函数也有本质上的差别**： 

线性回归：$f(x)=\theta ^{T}x=\theta _{1}x _{1}+\theta _{2}x _{2}+...+\theta _{n}x _{n}$
	
逻辑回归：$f(x)=P(y=1|x;\theta )=g(\theta ^{T}x)$，其中，$g(z)=\frac{1}{1+e^{-z}}$


可以看出，线性回归的拟合函数，是对f(x)的输出变量y的拟合，而逻辑回归的拟合函数是对为1类样本的概率的拟合。
	
那么，为什么要以1类样本的概率进行拟合呢，为什么可以这样拟合呢？ 
	
$\theta ^{T}x=0$就相当于是1类和0类的决策边界： 
	
当$\theta ^{T}x>0$，则y>0.5；若$\theta ^{T}x\rightarrow +\infty $，则$y \rightarrow  1 $，即y为1类; 


当$\theta ^{T}x<0$，则y<0.5；若$\theta ^{T}x\rightarrow -\infty $，则$y \rightarrow  0 $，即y为0类; 

这个时候就能看出区别，在线性回归中$\theta ^{T}x$为预测值的拟合函数；而在逻辑回归中$\theta ^{T}x$为决策边界。下表为线性回归和逻辑回归的区别。


|              |  线性回归  |   逻辑回归   |
| :----------: | :--------: | :----------: |
|     目的     |    预测    |     分类     |
|  $y^{(i)}$   |    未知    |   （0,1）    |
|     函数     |  拟合函数  |   预测函数   |
| 参数计算方式 | 最小二乘法 | 极大似然估计 |


下面具体解释一下： 

1. 拟合函数和预测函数什么关系呢？简单来说就是将拟合函数做了一个逻辑函数的转换，转换后使得$y^{(i)} \in (0,1)$;
2. 最小二乘和最大似然估计可以相互替代吗？回答当然是不行了。我们来看看两者依仗的原理：**最大似然估计是计算使得数据出现的可能性最大的参数**，依仗的自然是Probability。而**最小二乘是计算误差损失**。



## 逻辑回归和 SVM 的区别

### 相同点

1. 都是分类算法
2. 都是监督学习算法
3. 都是判别模型
4. 都能通过核函数方法针对非线性情况分类
5. 目标都是找一个分类超平面
6. 都能减少离群点的影响



### 不同点

1. 损失函数不同，逻辑回归是cross entropy loss，svm是hinge loss
2. 逻辑回归在优化参数时所有样本点都参与了贡献，svm则只取离分离超平面最近的支持向量样本。这也是为什么逻辑回归不用核函数，它需要计算的样本太多。并且由于逻辑回归受所有样本的影响，当样本不均衡时需要平衡一下每一类的样本个数。
3. 逻辑回归对概率建模，svm对分类超平面建模
4. 逻辑回归是处理经验风险最小化，svm是结构风险最小化。这点体现在svm自带L2正则化项，逻辑回归并没有
5. 逻辑回归通过非线性变换减弱分离平面较远的点的影响，svm则只取支持向量从而消去较远点的影响
6. 逻辑回归是统计方法，svm是几何方法
7. 线性SVM依赖数据表达的距离测度，所以需要对数据先做normalization，LR不受其影响



## 代码实现

首先是采用`sklearn`包中的逻辑回归算法代码：

```python
#Import Library
from sklearn.linear_model import LogisticRegression
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset

# Create logistic regression object

model = LogisticRegression()

# Train the model using the training sets and check score
model.fit(X, y)
model.score(X, y)

#Equation coefficient and Intercept
print('Coefficient: \n', model.coef_)
print('Intercept: \n', model.intercept_)

#Predict Output
predicted= model.predict(x_test)
```



接下来则是应用例子，如下所示：

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@Time    : 2016/10/19 21:35
@Author  : cai

实现多类的逻辑回归算法
"""
import os
import numpy as np
import pandas as pd
import matplotlib.pylab as plt
from scipy.optimize import minimize
from scipy.io import loadmat

# 定义Sigmoid函数
def sigmoid(z):
    return 1 / (1 + np.exp(-z))

# 定义 cost函数
def costReg(theta, X, y, lambdas):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)
    h = X * theta.T
    first = np.multiply(-y, np.log(sigmoid(h)))
    second = np.multiply((1-y), np.log(1 - sigmoid(h)))
    reg = (lambdas / 2 * len(X)) * np.sum(np.power(theta[:, 1:theta.shape[1]], 2))
    return np.sum(first - second) / (len(X)) + reg

# 梯度下降算法的实现, 输出梯度对权值的偏导数
def gradient(theta, X, y, lambdas):
    theta = np.matrix(theta)
    X = np.matrix(X)
    y = np.matrix(y)

    parameters = int(theta.ravel().shape[1])
    grad = np.zeros(parameters)
    # 计算误差
    error = sigmoid(X * theta.T) - y

    grad = ((X.T * error) / len(X)).T + ((lambdas / len(X)) * theta)

    grad[0, 0] = np.sum(np.multiply(error, X[:, 0])) / len(X)

    return np.array(grad).ravel()

# 实现一对多的分类方法
def one_vs_all(X, y, num_labels, lambdas):
    rows = X.shape[0]
    params = X.shape[1]

    # 每个分类器有一个 k * (n+1)大小的权值数组
    all_theta = np.zeros((num_labels, params + 1))

    # 增加一列，这是用于偏置值
    X = np.insert(X, 0, values=np.ones(rows), axis=1)

    # 标签的索引从1开始
    for i in range(1, num_labels + 1):
        theta = np.zeros(params + 1)
        y_i = np.array([1 if label == i else 0 for label in y])
        y_i = np.reshape(y_i, (rows, 1))

        # 最小化损失函数
        fmin = minimize(fun=costReg, x0=theta, args=(X, y_i, lambdas), method='TNC', jac=gradient)
        all_theta[i-1, :] = fmin.x

    return all_theta

def predict_all(X, all_theta):
    rows = X.shape[0]
    params = X.shape[1]
    num_labels = all_theta.shape[0]

    # 增加一列，这是用于偏置值
    X = np.insert(X, 0, values=np.ones(rows), axis=1)

    X = np.matrix(X)
    all_theta = np.matrix(all_theta)

    # 对每个训练样本计算其类的概率值
    h = sigmoid(X * all_theta.T)

    # 获取最大概率值的数组索引
    h_argmax = np.argmax(h, axis=1)
    # 数组是从0开始索引，而标签值是从1开始，所以需要加1
    h_argmax = h_argmax + 1

    return h_argmax

dataPath = os.path.join('data', 'ex3data1.mat')
# 载入数据
data = loadmat(dataPath)
print(data)
print(data['X'].shape, data['y'].shape)

# print(np.unique(data['y']))
# 测试
# rows = data['X'].shape[0]
# params = data['X'].shape[1]
#
# all_theta = np.zeros((10, params + 1))
#
# X = np.insert(data['X'], 0, values=np.ones(rows), axis=1)
#
# theta = np.zeros(params + 1)
#
# y_0 = np.array([1 if label == 0 else 0 for label in data['y']])
# y_0 = np.reshape(y_0, (rows, 1))
# print(X.shape, y_0.shape, theta.shape, all_theta.shape)

all_theta = one_vs_all(data['X'], data['y'], 10, 1)
print(all_theta)

# 计算分类准确率
y_pred = predict_all(data['X'], all_theta)
correct = [1 if a == b else 0 for (a, b) in zip(y_pred, data['y'])]
accuracy = (sum(map(int, correct)) / float(len(correct)))
print('accuracy = {0}%'.format(accuracy * 100))
```

实现代码来自[Part 4 - Multivariate Logistic Regression](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-4/)。具体可以查看[我的github](https://github.com/ccc013/CodingPractise/blob/master/Python/MachineLearning/mulLogisticRegressionPractise.py)。





# 12. 线性回归

## 简述

在统计学中，**线性回归（Linear Regression）是利用称为线性回归方程的最小平方函数对一个或多个自变量和因变量之间关系进行建模的一种回归分析**。这种函数是一个或多个称为回归系数的模型参数的线性组合（自变量都是一次方）。**只有一个自变量的情况称为简单回归，大于一个自变量情况的叫做多元回归**。

优点：结果易于理解，计算上不复杂。
缺点：对非线性数据拟合不好。

适用数据类型：数值型和标称型数据。
算法类型：回归算法

线性回归的模型函数如下：
$$
h_\theta = \theta ^T x
$$

它的损失函数如下：
$$
J(\theta) = {1\over {2m}} \sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})^2
$$
通过训练数据集寻找参数的最优解，即求解可以得到 $minJ(\theta)$ 的参数向量 $\theta$ ,其中这里的参数向量也可以分为参数 $w和b$ , 分别表示权重和偏置值。

求解最优解的方法有**最小二乘法和梯度下降法**。



## 梯度下降法

梯度下降算法的思想如下(这里以一元线性回归为例)：

> 首先，我们有一个代价函数，假设是$J(\theta_0,\theta_1)$，我们的目标是$min_{\theta_0,\theta_1}J(\theta_0,\theta_1)$。
> 接下来的做法是：
>
> - 首先是随机选择一个参数的组合$(\theta_0,\theta_1)$,一般是设$\theta_0 = 0,\theta_1 = 0$;
> - 然后是不断改变$(\theta_0,\theta_1)$，并计算代价函数，直到一个**局部最小值**。之所以是**局部最小值**，是因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是**全局最小值**，选择不同的初始参数组合，可能会找到不同的局部最小值。
>   下面给出梯度下降算法的公式：

> repeat until convergence{

$$
\theta_j := \theta_j - \alpha \frac{\partial}{\partial \theta_j} J(\theta_0,\theta_1)\quad (for\quad j=0 \quad and\quad j=1)
$$

> } 

也就是在梯度下降中，不断重复上述公式直到收敛，也就是找到$\color{red}{局部最小值}$。其中符号`:=`是赋值符号的意思。

而应用梯度下降法到线性回归，则公式如下：
$$
\theta_0 := \theta_0 - \alpha \frac{1}{m}\sum_{i=1}^m (h_\theta(x^{(i)}) - y^{(i)})\ \\
 \theta_1 := \theta_1 - \alpha \frac{1}{m}\sum_{i=1}^m ((h_\theta(x^{(i)}) - y^{(i)}) \cdot x^{(i)})
$$

公式中的$\alpha$称为**学习率(learning rate)**，它决定了我们沿着能让代价函数下降程度最大的方向向下迈进的步子有多大。

在梯度下降中，还涉及到一个参数更新的问题，即更新$(\theta_0,\theta_1)$，一般我们的做法是**同步更新。**

最后，上述梯度下降算法公式实际上是一个叫**批量梯度下降(batch gradient descent)**，即它在每次梯度下降中都是使用整个训练集的数据，所以公式中是带有$\sum_{i=1}^m$。



## 岭回归（ridge regression）

岭回归是一种专用于共线性数据分析的有偏估计回归方法，实质上是一种改良的最小二乘估计法，通过放弃最小二乘法的无偏性，以损失部分信息、降低精度为代价，获得回归系数更为符合实际、更可靠的回归方法，对病态数据的耐受性远远强于最小二乘法。

岭回归分析法是从根本上消除复共线性影响的统计方法。岭回归模型通过在相关矩阵中引入一个很小的岭参数K（1>K>0），并将它加到主对角线元素上，从而降低参数的最小二乘估计中复共线特征向量的影响，减小复共线变量系数最小二乘估计的方法，以保证参数估计更接近真实情况。岭回归分析将所有的变量引入模型中，比逐步回归分析提供更多的信息。

其他回归还可以参考 [各种回归全解：传统回归、逻辑回归、加权回归/核回归、岭回归、广义线性模型/指数族](http://blog.csdn.net/ownfed/article/details/41181665)。



## 代码实现

Python实现的代码如下：

```python
#Import Library
#Import other necessary libraries like pandas, numpy...
from sklearn import linear_model
#Load Train and Test datasets
#Identify feature and response variable(s) and values must be numeric and numpy arrays

x_train=input_variables_values_training_datasets
y_train=target_variables_values_training_datasets
x_test=input_variables_values_test_datasets

# Create linear regression object
linear = linear_model.LinearRegression()

# Train the model using the training sets and check score
linear.fit(x_train, y_train)
linear.score(x_train, y_train)

#Equation coefficient and Intercept
print('Coefficient: \n', linear.coef_)
print('Intercept: \n', linear.intercept_)

#Predict Output
predicted= linear.predict(x_test)
```

上述是使用`sklearn`包中的线性回归算法的代码例子，下面是一个实现的具体例子。

```python
# -*- coding: utf-8 -*-
"""
Created on Mon Oct 17 10:36:06 2016

@author: cai
"""

import os
import numpy as np
import pandas as pd
import matplotlib.pylab as plt
from sklearn import linear_model

# 计算损失函数
def computeCost(X, y, theta):
    inner = np.power(((X * theta.T) - y), 2)
    return np.sum(inner) / (2 * len(X))

# 梯度下降算法
def gradientDescent(X, y, theta, alpha, iters):
    temp = np.matrix(np.zeros(theta.shape))
    parameters = int(theta.ravel().shape[1])
    cost = np.zeros(iters)

    for i in range(iters):
        error = (X * theta.T) - y

        for j in range(parameters):
            # 计算误差对权值的偏导数
            term = np.multiply(error, X[:, j])
            # 更新权值
            temp[0, j] = theta[0, j] - ((alpha / len(X)) * np.sum(term))

        theta = temp
        cost[i] = computeCost(X, y, theta)
    return theta, cost

dataPath = os.path.join('data', 'ex1data1.txt')
data = pd.read_csv(dataPath, header=None, names=['Population', 'Profit'])
# print(data.head())
# print(data.describe())
# data.plot(kind='scatter', x='Population', y='Profit', figsize=(12, 8))
# 在数据起始位置添加1列数值为1的数据
data.insert(0, 'Ones', 1)
print(data.shape)

cols = data.shape[1]
X = data.iloc[:, 0:cols-1]
y = data.iloc[:, cols-1:cols]

# 从数据帧转换成numpy的矩阵格式
X = np.matrix(X.values)
y = np.matrix(y.values)
# theta = np.matrix(np.array([0, 0]))
theta = np.matrix(np.zeros((1, cols-1)))
print(theta)
print(X.shape, theta.shape, y.shape)
cost = computeCost(X, y, theta)
print("cost = ", cost)

# 初始化学习率和迭代次数
alpha = 0.01
iters = 1000

# 执行梯度下降算法
g, cost = gradientDescent(X, y, theta, alpha, iters)
print(g)

# 可视化结果
x = np.linspace(data.Population.min(),data.Population.max(),100)
f = g[0, 0] + (g[0, 1] * x)

fig, ax = plt.subplots(figsize=(12, 8))
ax.plot(x, f, 'r', label='Prediction')
ax.scatter(data.Population, data.Profit, label='Training Data')
ax.legend(loc=2)
ax.set_xlabel('Population')
ax.set_ylabel('Profit')
ax.set_title('Predicted Profit vs. Population Size')

fig, ax = plt.subplots(figsize=(12, 8))
ax.plot(np.arange(iters), cost, 'r')
ax.set_xlabel('Iteration')
ax.set_ylabel('Cost')
ax.set_title('Error vs. Training Epoch')


# 使用sklearn 包里面实现的线性回归算法
model = linear_model.LinearRegression()
model.fit(X, y)

x = np.array(X[:, 1].A1)
# 预测结果
f = model.predict(X).flatten()
# 可视化
fig, ax = plt.subplots(figsize=(12, 8))
ax.plot(x, f, 'r', label='Prediction')
ax.scatter(data.Population, data.Profit, label='Training Data')
ax.legend(loc=2)
ax.set_xlabel('Population')
ax.set_ylabel('Profit')
ax.set_title('Predicted Profit vs. Population Size(using sklearn)')
plt.show()
```

上述代码参考自 [Part 1 - Simple Linear Regression](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-1/)。具体可以查看 [我的Github](https://github.com/ccc013/CodingPractise/blob/master/Python/MachineLearning/linearRegressionPractise.py)。





# 13. 类别不平衡

## 简介

什么是类别不平衡呢？它是指分类任务中存在某个或者某些类别的样本数量远多于其他类别的样本数量的情况。


比如，一个十分类问题，总共有 10000 个样本，但是类别 1 到 4 分别包含 2000 个样本，剩余 6 个类别的样本数量加起来刚刚 2000 个，即这六个类别各自包含的样本平均数量大约是 333 个，相比前四个类别是相差了 6 倍左右的数量。这种情况就是类别不平衡了。




## 方法


### 1.扩充数据集


首先应该考虑数据集的扩充，在刚刚图片数据集扩充一节介绍了多种数据扩充的办法，而且数据越多，给模型提供的信息也越大，更有利于训练出一个性能更好的模型。

如果在增加小类样本数量的同时，又增加了大类样本数据，可以考虑放弃部分大类数据（通过对其进行欠采样方法）。




### 2.尝试其他评价指标


一般分类任务最常使用的评价指标就是准确度了，但它在类别不平衡的分类任务中并不能反映实际情况，原因就是即便分类器将所有类别都分为大类，准确度也不会差，因为大类包含的数量远远多于小类的数量，所以这个评价指标会偏向于大类类别的数据。


其他可以推荐的评价指标有以下几种


- 混淆矩阵：实际上这个也是在分类任务会采用的一个指标，可以查看分类器对每个类别预测的情况，其对角线数值表示预测正确的数量；
- 精确度(Precision)：表示实际预测正确的结果占所有被预测正确的结果的比例，P=TP / (TP+FP)
- 召回率(Recall)：表示实际预测正确的结果占所有真正正确的结果的比例，R = TP / (TP+FN)
- F1 得分(F1 Score)：精确度和召回率的加权平均，F1=2PR / (P+R)
- Kappa (Cohen kappa)
- ROC 曲线(ROC Curves):常被用于评价一个二值分类器的优劣，而且对于正负样本分布变化的时候，ROC 曲线可以保持不变，即不受类别不平衡的影响。



其中 TP、FP、TN、FN 分别表示正确预测的正类、错误预测的正类、预测正确的负类以及错误预测的负类。图例如下：


![](https://cdn.nlark.com/yuque/0/2019/png/308996/1566915657432-56bec1a1-2c3d-44ce-b5da-cee121a552ca.png#align=left&display=inline&height=493&margin=%5Bobject%20Object%5D&originHeight=493&originWidth=986&size=0&status=done&style=none&width=986)




### 3.对数据集进行重采样


可以使用一些策略该减轻数据的不平衡程度。该策略便是采样(sampling)，主要有两种采样方法来降低数据的不平衡性。


- 对小类的数据样本进行采样来增加小类的数据样本个数，即**过采样**（over-sampling ，采样的个数大于该类样本的个数），代表算法：SMOTE和ADASYN。 

  - SMOTE：通过对训练集中的小类数据进行插值来产生额外的小类样本数据。新的少数类样本产生的策略：对每个少数类样本a，在a的最近邻中随机选一个样本b，然后在a、b之间的连线上随机选一点作为新合成的少数类样本。 	
  - ADASYN：根据学习难度的不同，对不同的少数类别的样本使用加权分布，对于难以学习的少数类的样本，产生更多的综合数据。 通过减少类不平衡引入的偏差和将分类决策边界自适应地转移到困难的样本两种手段，改善了数据分布。

- 对大类的数据样本进行采样来减少该类数据样本的个数，即**欠采样**（under-sampling，采样的次数少于该类样本的个素），缺点是欠采样的时候如果随机丢弃大类样本，可能丢失重要的信息。代表算法是 **EasyEnsemble**。其思想是利用集成学习机制，将大类划分为若干个集合供不同的学习器使用。相当于对每个学习器都进行欠采样，**但对于全局则不会丢失重要信息**。



采样算法往往很容易实现，并且其运行速度快，并且效果也不错。 一些经验法则：


- 考虑对大类下的样本（超过 1 万、十万甚至更多）进行欠采样，即删除部分样本；
- 考虑对小类下的样本（不足 1万甚至更少）进行过采样，即添加部分样本的副本；
- 考虑尝试**随机采样与非随机采样**两种采样方法；
- 考虑对各类别尝试不同的采样比例，比一定是1:1，有时候1:1反而不好，因为与现实情况相差甚远；
- 考虑同时使用过采样与欠采样。



### 4.尝试人工生成数据样本


一种简单的人工样本数据产生的方法便是，**对该类下的所有样本每个属性特征的取值空间中随机选取一个组成新的样本，即属性值随机采样**。


你可以使用**基于经验对属性值进行随机采样**而构造新的人工样本，或者使用类似**朴素贝叶斯方法**假设各属性之间互相独立进行采样，这样便可得到更多的数据，但是无法保证属性之前的线性关系（如果本身是存在的）。


有一个系统的构造人工数据样本的方法 **SMOTE**(Synthetic Minority Over-sampling Technique)。SMOTE 是一种**过采样算法**，它**构造新的小类样本**而不是产生小类中已有的样本的副本，即该算法构造的数据是新样本，原数据集中不存在的。


它基于**距离度量**选择小类别下两个或者更多的相似样本，然后选择其中一个样本，并随机选择一定数量的邻居样本，然后对选择的那个样本的**一个属性增加噪声**，每次处理一个属性。这样就构造了更多的新生数据。


python 实现的 SMOTE 算法代码地址如下，它提供了多种不同实现版本，以及多个重采样算法。

[https://github.com/scikit-learn-contrib/imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn)




### 5.尝试不同分类算法


强烈建议不要对待每一个分类都使用自己喜欢而熟悉的分类算法。应该使用不同的算法对其进行比较，因为不同的算法适用于不同的任务与数据。

**决策树往往在类别不均衡数据上表现不错**。它使用基于类变量的划分规则去创建分类树，因此可以强制地将不同类别的样本分开。目前流行的决策树算法有：C4.5、C5.0、CART和Random Forest等。




### 6.尝试对模型进行惩罚


你可以使用相同的分类算法，但使用一个不同的角度，比如你的分类任务是识别那些小类，那么可以**对分类器的小类样本数据增加权值，降低大类样本的权值**（这种方法其实是产生了新的数据分布，即产生了新的数据集），从而使得分类器将重点集中在小类样本身上。


一个具体做法就是，在训练分类器时，若分类器将小类样本分错时额外增加分类器一个小类样本分错代价，这个额外的代价可以使得分类器更加“关心”小类样本。如 penalized-SVM 和 penalized-LDA 算法。

如果你锁定一个具体的算法时，并且无法通过使用重采样来解决不均衡性问题而得到较差的分类结果。这样你便可以使用**惩罚模型来解决不平衡性**问题。但是，设置惩罚矩阵是一个复杂的事，因此你需要根据你的任务尝试不同的惩罚矩阵，并选取一个较好的惩罚矩阵。




### 7.尝试一个新的角度理解问题


从一个新的角度来理解问题，比如我们可以将小类的样本作为异常点，那么问题就变成异常点检测与变化趋势检测问题。


- 异常点检测：即是对那些罕见事件进行识别。如通过机器的部件的振动识别机器故障，又如通过系统调用序列识别恶意程序。这些事件相对于正常情况是很少见的。
- 变化趋势检测：类似于异常点检测，不同在于其通过**检测不寻常的变化趋势**来识别。如通过观察用户模式或银行交易来检测用户行为的不寻常改变。



将小类样本作为异常点这种思维的转变，可以帮助考虑新的方法去分离或分类样本。这两种方法从不同的角度去思考，让你尝试新的方法去解决问题。




### 8.尝试创新


仔细对问题进行分析和挖掘，是否可以将问题划分为多个更小的问题，可以尝试如下方法：


- 将你的大类压缩成小类；
- 使用 One Class 分类器（将小类作为异常点）；
- 使用集成方式，训练多个分类器，然后联合这些分类器进行分类；



对于类别不平衡问题，还是需要具体问题具体分析，如果有先验知识可以快速挑选合适的方法来解决，否则最好就是逐一测试每一种方法，然后挑选最好的算法。最重要的还是多做项目，多积累经验，这样遇到一个新的问题，也可以快速找到合适的解决方法。





## 参考

- 论文：[Survey on deep learning with class imbalance](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0192-5)
- [【图像分类】 关于图像分类中类别不平衡那些事](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649035421&idx=2&sn=bd5fa631a4701734be38e34603a12457&chksm=8712ace0b06525f6cb40a9a1b5341226721916c404a11f39651d3c95b7233902330a844dfe6b&token=1721275494&lang=zh_CN#rd)
- 《hands-on-ml-with-sklearn-and-tf》第二节



# 14. 朴素贝叶斯



## 1. 极大似然估计原理

极大似然估计的原理，用一张图片来说明，如下图所示

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/%E6%9E%81%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1%E5%9B%BE%E8%A7%A3.png" style="zoom:75%;" />

例：有两个外形完全相同的箱子，1号箱有99只白球，1只黑球；2号箱有1只白球，99只黑球。在一次实验中，取出的是黑球，请问是从哪个箱子中取出的？

一般的根据经验想法，会猜测这只黑球最像是从2号箱取出，**此时描述的“最像”就有“最大似然”的意思，这种想法常称为“最大似然原理”**。

总结起来，**最大似然估计的目的就是：利用已知的样本结果，反推最有可能（最大概率）导致这样结果的参数值**。

极大似然估计是建立在极大似然原理的基础上的一个统计方法。**极大似然估计提供了一种给定观察数据来评估模型参数的方法，即：“模型已定，参数未知”**。

通过若干次试验，观察其结果，**利用试验结果得到某个参数值能够使样本出现的概率为最大，则称为极大似然估计**。

由于样本集中的样本都是独立同分布，可以只考虑一类样本集$D$，来估计参数向量$\vec\theta$。记已知的样本集为：
$$
D=\vec x_{1},\vec x_{2},...,\vec x_{n}
$$

似然函数（likelihood function）：联合概率密度函数 $p(D|\vec\theta )$ 称为相对于$\vec x_{1},\vec x_{2},...,\vec x_{n}$的$\vec\theta$的似然函数。

$$
l(\vec\theta )=p(D|\vec\theta ) =p(\vec x_{1},\vec x_{2},...,\vec x_{n}|\vec\theta )=\prod_{i=1}^{n}p(\vec x_{i}|\vec \theta )
$$

如果$\hat{\vec\theta}$是参数空间中能使似然函数$l(\vec\theta)$最大的$\vec\theta$值，则$\hat{\vec\theta}$应该是“最可能”的参数值，那么$\hat{\vec\theta}$就是$\theta$的极大似然估计量。它是样本集的函数，记作：

$$
\hat{\vec\theta}=d(D)= \mathop {\arg \max}_{\vec\theta} l(\vec\theta )
$$
$ \hat{\vec\theta}(\vec x_{1},\vec x_{2},...,\vec x_{n})$称为极大似然函数估计值。



## 2. 朴素贝叶斯

### 2.1 贝叶斯分类器原理

贝叶斯决策论通过**相关概率已知**的情况下利用**误判损失**来选择最优的类别分类。  

假设有$N$种可能的分类标记，记为$Y=\{c_1,c_2,...,c_N\}$，那对于样本$\boldsymbol{x}$，它属于哪一类呢？

计算步骤如下：

1. 算出样本$\boldsymbol{x}$属于第i个类的概率，即$P(c_i|x)$；
2. 通过比较所有的$P(c_i|\boldsymbol{x})$，得到样本$\boldsymbol{x}$所属的最佳类别。
3. 将类别$c_i$和样本$\boldsymbol{x}$代入到贝叶斯公式中，得到：

$$
P(c_i|\boldsymbol{x})=\frac{P(\boldsymbol{x}|c_i)P(c_i)}{P(\boldsymbol{x})}.
$$

一般来说，$P(c_i)$为先验概率，$P(\boldsymbol{x}|c_i)$为条件概率，$P(\boldsymbol{x})$是用于归一化的证据因子。对于$P(c_i)$可以通过训练样本中类别为$c_i$的样本所占的比例进行估计；此外，由于只需要找出最大的$P(\boldsymbol{x}|c_i)$，因此我们并不需要计算$P(\boldsymbol{x})$。  

为了求解条件概率，基于不同假设提出了不同的方法，以下将介绍朴素贝叶斯分类器和半朴素贝叶斯分类器。





### 2.2 朴素贝叶斯分类器

假设样本$\boldsymbol{x}$包含$d$个属性，即$\boldsymbol{x}=\{ x_1,x_2,...,x_d\}$。于是有：
$$
P(\boldsymbol{x}|c_i)=P(x_1,x_2,\cdots,x_d|c_i)
$$
这个联合概率难以从有限的训练样本中直接估计得到。于是，朴素贝叶斯（Naive Bayesian，简称NB）采用了“属性条件独立性假设”：**对已知类别，假设所有属性相互独立**。于是有：
$$
P(x_1,x_2,\cdots,x_d|c_i)=\prod_{j=1}^d P(x_j|c_i)
$$
这样的话，我们就可以很容易地推出相应的判定准则了：
$$
h_{nb}(\boldsymbol{x})=\mathop{\arg \max}_{c_i\in Y} P(c_i)\prod_{j=1}^dP(x_j|c_i)
$$
**条件概率$P(x_j|c_i)$的求解**

如果$x_j$是标签属性，那么我们可以通过计数的方法估计$P(x_j|c_i)$
$$
P(x_j|c_i)=\frac{P(x_j,c_i)}{P(c_i)}\approx\frac{\#(x_j,c_i)}{\#(c_i)}
$$
其中，$\#(x_j,c_i)$表示在训练样本中$x_j$与$c_{i}$共同出现的次数。

如果$x_j$是数值属性，通常我们假设类别中$c_{i}$的所有样本第$j$个属性的值服从正态分布。我们首先估计这个分布的均值$μ$和方差$σ$，然后计算$x_j$在这个分布中的概率密度$P(x_j|c_i)$。



### 2.3 朴素贝叶斯分类器例子

使用经典的西瓜训练集如下：

| 编号 | 色泽 | 根蒂 | 敲声 | 纹理 | 脐部 | 触感 | 密度  | 含糖率 | 好瓜 |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :---: | :----: | :--: |
|  1   | 青绿 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 0.697 | 0.460  |  是  |
|  2   | 乌黑 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 硬滑 | 0.774 | 0.376  |  是  |
|  3   | 乌黑 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 0.634 | 0.264  |  是  |
|  4   | 青绿 | 蜷缩 | 沉闷 | 清晰 | 凹陷 | 硬滑 | 0.608 | 0.318  |  是  |
|  5   | 浅白 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 0.556 | 0.215  |  是  |
|  6   | 青绿 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 软粘 | 0.403 | 0.237  |  是  |
|  7   | 乌黑 | 稍蜷 | 浊响 | 稍糊 | 稍凹 | 软粘 | 0.481 | 0.149  |  是  |
|  8   | 乌黑 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 硬滑 | 0.437 | 0.211  |  是  |
|  9   | 乌黑 | 稍蜷 | 沉闷 | 稍糊 | 稍凹 | 硬滑 | 0.666 | 0.091  |  否  |
|  10  | 青绿 | 硬挺 | 清脆 | 清晰 | 平坦 | 软粘 | 0.243 | 0.267  |  否  |
|  11  | 浅白 | 硬挺 | 清脆 | 模糊 | 平坦 | 硬滑 | 0.245 | 0.057  |  否  |
|  12  | 浅白 | 蜷缩 | 浊响 | 模糊 | 平坦 | 软粘 | 0.343 | 0.099  |  否  |
|  13  | 青绿 | 稍蜷 | 浊响 | 稍糊 | 凹陷 | 硬滑 | 0.639 | 0.161  |  否  |
|  14  | 浅白 | 稍蜷 | 沉闷 | 稍糊 | 凹陷 | 硬滑 | 0.657 | 0.198  |  否  |
|  15  | 乌黑 | 稍蜷 | 浊响 | 清晰 | 稍凹 | 软粘 | 0.360 | 0.370  |  否  |
|  16  | 浅白 | 蜷缩 | 浊响 | 模糊 | 平坦 | 硬滑 | 0.593 | 0.042  |  否  |
|  17  | 青绿 | 蜷缩 | 沉闷 | 稍糊 | 稍凹 | 硬滑 | 0.719 | 0.103  |  否  |

对下面的测试例“测1”进行 分类：

| 编号 | 色泽 | 根蒂 | 敲声 | 纹理 | 脐部 | 触感 | 密度  | 含糖率 | 好瓜 |
| :--: | :--: | :--: | :--: | :--: | :--: | :--: | :---: | :----: | :--: |
| 测1  | 青绿 | 蜷缩 | 浊响 | 清晰 | 凹陷 | 硬滑 | 0.697 | 0.460  |  ？  |

首先，估计类先验概率$P(c_j)$，有
$$
\begin{align} 
&P(好瓜=是)=\frac{8}{17}=0.471 \newline 
&P(好瓜=否)=\frac{9}{17}=0.529 
\end{align}
$$
然后，为每个属性估计条件概率（这里，对于连续属性，假定它们服从正态分布）
$$
P_{青绿|是}=P（色泽=青绿|好瓜=是）=\frac{3}{8}=0.375
$$

$$
P_{青绿|否}=P（色泽=青绿|好瓜=否）=\frac{3}{9}\approx0.333
$$

$$
P_{蜷缩|是}=P（根蒂=蜷缩|好瓜=是）=\frac{5}{8}=0.625
$$

$$
P_{蜷缩|否}=P（根蒂=蜷缩|好瓜=否）=\frac{3}{9}=0.333
$$

$$
P_{浊响|是}=P（敲声=浊响|好瓜=是）=\frac{6}{8}=0.750
$$

$$
P_{浊响|否}=P（敲声=浊响|好瓜=否）=\frac{4}{9}\approx 0.444
$$

$$
P_{清晰|是}=P（纹理=清晰|好瓜=是）=\frac{7}{8}= 0.875
$$

$$
P_{清晰|否}=P（纹理=清晰|好瓜=否）=\frac{2}{9}\approx 0.222
$$

$$
P_{凹陷|是}=P（脐部=凹陷|好瓜=是）=\frac{6}{8}= 0.750
$$

$$
P_{凹陷|否}=P（脐部=凹陷|好瓜=否）=\frac{2}{9} \approx 0.222
$$

$$
P_{硬滑|是}=P（触感=硬滑|好瓜=是）=\frac{6}{8}= 0.750
$$

$$
P_{硬滑|否}=P（触感=硬滑|好瓜=否）=\frac{6}{9} \approx 0.667
$$

$$
\begin{aligned}
\rho_{密度：0.697|是}&=\rho（密度=0.697|好瓜=是）\\&=\frac{1}{\sqrt{2 \pi}\times0.129}exp\left( -\frac{(0.697-0.574)^2}{2\times0.129^2}\right) \approx 1.959
\end{aligned}
$$

$$
\begin{aligned}
\rho_{密度：0.697|否}&=\rho（密度=0.697|好瓜=否）\\&=\frac{1}{\sqrt{2 \pi}\times0.195}exp\left( -\frac{(0.697-0.496)^2}{2\times0.195^2}\right) \approx 1.203
\end{aligned}
$$

$$
\begin{aligned}
\rho_{含糖：0.460|是}&=\rho（密度=0.460|好瓜=是）\\&=\frac{1}{\sqrt{2 \pi}\times0.101}exp\left( -\frac{(0.460-0.279)^2}{2\times0.101^2}\right) \approx 0.788
\end{aligned}
$$

$$
\begin{aligned}
\rho_{含糖：0.460|否}&=\rho（密度=0.460|好瓜=是）\\&=\frac{1}{\sqrt{2 \pi}\times0.108}exp\left( -\frac{(0.460-0.154)^2}{2\times0.108^2}\right) \approx 0.066
\end{aligned}
$$

于是有
$$
\begin{align} 
P(&好瓜=是)\times P_{青绿|是} \times P_{蜷缩|是} \times P_{浊响|是} \times P_{清晰|是} \times P_{凹陷|是}\newline 
&\times P_{硬滑|是} \times p_{密度：0.697|是} \times p_{含糖：0.460|是} \approx 0.063 \newline\newline 
P(&好瓜=否)\times P_{青绿|否} \times P_{蜷缩|否} \times P_{浊响|否} \times P_{清晰|否} \times P_{凹陷|否}\newline 
&\times P_{硬滑|否} \times p_{密度：0.697|否} \times p_{含糖：0.460|否} \approx 6.80\times 10^{-5} 
\end{align}
$$

由于$0.063>6.80\times 10^{-5}$，因此，朴素贝叶斯分类器将测试样本“测1”判别为“好瓜”。



### 2.4 半朴素贝叶斯分类器

朴素贝叶斯采用了“属性条件独立性假设”，半朴素贝叶斯分类器的基本想法是**适当考虑一部分属性间的相互依赖信息**。

**独依赖估计**（One-Dependence Estimator，简称ODE）是半朴素贝叶斯分类器最常用的一种策略。顾名思义，独依赖是假设每个属性在类别之外最多依赖一个其他属性，即：
$$
P(\boldsymbol{x}|c_i)=\prod_{j=1}^d P(x_j|c_i,{\rm pa}_j)
$$

其中$pa_j$为属性$x_i$所依赖的属性，成为$x_i$的父属性。假设父属性$pa_j$已知，那么可以使用下面的公式估计$P(x_j|c_i,{\rm pa}_j)$
$$
P(x_j|c_i,{\rm pa}_j)=\frac{P(x_j,c_i,{\rm pa}_j)}{P(c_i,{\rm pa}_j)}
$$

## 3. 参数估计

### 3.1 贝叶斯估计/多项式模型

用**极大似然估计可能会出现所要估计的概率值为0**的情况，这会影响到后验概率的计算，使分类产生偏差。解决这个问题的办法是使用**贝叶斯估计**，也被称为多项式模型。

当**特征是离散的时候，使用多项式模型**。多项式模型在计算先验概率$P(y_k)$和条件概率$P(x_i|y_k)$时，会做一些**平滑处理**，具体公式为：
$$
P(y_k)=\frac{N_{y_k}+α}{N+kα}
$$

> $N$是总的样本个数，$k$是总的类别个数，$N_{y_k}$是类别为$y_k$的样本个数，$α$是平滑值。

$$
P(x_i|y_k) = \frac{N_{y_k,x_i} + \alpha}{N_{y_k}+n\alpha}
$$

> $N_{y_k}$是类别为$y_k$的样本个数，$n$是特征的维数，$N_{y_k,x_i}$是类别为$y_k$的样本中，第$i$维特征的值是$x_i$的样本个数，$α$是平滑值。

当$α=1$时，称作**Laplace平滑**，当$0<α<1$时，称作**Lidstone**平滑，$α=0$时不做平滑。

如果不做平滑，当某一维特征的值$x_i$没在训练样本中出现过时，会导致$P(x_i|y_k)=0$，从而导致后验概率为0。加上平滑就可以克服这个问题。



### 3.2 高斯模型

当特征是连续变量的时候，运用多项式模型会导致很多$P(x_i|y_k) = 0$（不做平滑的情况下），即使做平滑，所得到的条件概率也难以描述真实情况，所以处理连续变量，应该采用高斯模型。

高斯模型是假设每一维特征都服从高斯分布（正态分布）：
$$
P(x_{i}|y_{k}) = \frac{1}{\sqrt{2\pi\sigma_{y_{k}}^{2}}}exp( -\frac{(x_{i}-\mu_{y_{k}})^2}  {2\sigma_{y_{k}}^{2}}   )
$$
$\mu_{y_{k},i}$表示类别为$y_k$的样本中，第$i$维特征的均值；
$\sigma_{y_{k},i}^{2}$表示类别为$y_k$的样本中，第$i$维特征的方差。



### 3.3 伯努利模型

与多项式模型一样，伯努利模型适用于**离散特征**的情况，所不同的是，**伯努利模型中每个特征的取值只能是1和0**(以文本分类为例，某个单词在文档中出现过，则其特征值为1，否则为0).

伯努利模型中，条件概率$P(x_i|y_k)$的计算方式是：

当特征值$x_i$为1时，$P(x_i|y_k)=P(x_i=1|y_k)$；

当特征值$x_i$为0时，$P(x_i|y_k)=1−P(x_i=1|y_k)$；



## 4. 和逻辑回归的相同和不同点

### 4.1 相同点

1. 两者都是对特征的线性表达
2. 两者建模的都是条件概率，对最终求得的分类结果有很好的解释性



### 4.2 不同点

1. **Naive Bayes是一个生成模型**，在计算P(y|x)之前，先要从训练数据中计算P(x|y)和P(y)的概率，从而利用贝叶斯公式计算P(y|x)。

   **Logistic Regression是一个判别模型**，它通过在训练数据集上最大化判别函数P(y|x)学习得到，不需要知道P(x|y)和P(y)。

2. Naive Bayes是建立在**条件独立假设**基础之上的，设特征X含有n个特征属性（X1，X2，...Xn），那么在给定Y的情况下，X1，X2，...Xn是条件独立的。

   Logistic Regression的限制则要**宽松很多**，如果数据满足条件独立假设，Logistic Regression能够取得非常好的效果；当数据不满足条件独立假设时，Logistic Regression仍然能够通过调整参数让模型最大化的符合数据的分布，从而训练得到在现有数据集下的一个最优模型。

3. **当数据集比较小的时候，应该选用Naive Bayes**，为了能够取得很好的效果，数据的需求量为O(logn)

   **当数据集比较大的时候，应该选用Logistic Regression，**为了能够取得很好的效果，数据的需求量为O(n)



## 5. 优缺点

### 5.1 优点

- 对小规模的数据表现很好，适合多分类任务，适合增量式训练。
- 所需估计的参数少，对于缺失数据不敏感。
- 有着坚实的数学基础，以及稳定的分类效率。

### 5.2 缺点

- 对输入数据的表达形式很敏感（离散、连续，值极大极小之类的）。
- 需要假设属性之间相互独立，这往往并不成立。（喜欢吃番茄、鸡蛋，却不喜欢吃番茄炒蛋）。
- 需要知道先验概率。
- 分类决策存在错误率。



## 6. 代码实现

下面是使用`sklearn`的代码例子，分别实现上述三种模型,例子来自 [朴素贝叶斯的三个常用模型：高斯、多项式、伯努利](http://www.letiantian.me/2014-10-12-three-models-of-naive-nayes/)。
下面是高斯模型的实现

```python
>>> from sklearn import datasets
>>> iris = datasets.load_iris()
>>> iris.feature_names  # 四个特征的名字
['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']
>>> iris.data
array([[ 5.1,  3.5,  1.4,  0.2],
       [ 4.9,  3. ,  1.4,  0.2],
       [ 4.7,  3.2,  1.3,  0.2],
       [ 4.6,  3.1,  1.5,  0.2],
       [ 5. ,  3.6,  1.4,  0.2],
       [ 5.4,  3.9,  1.7,  0.4],
       [ 4.6,  3.4,  1.4,  0.3],
       [ 5. ,  3.4,  1.5,  0.2],
       ......
       [ 6.5,  3. ,  5.2,  2. ],
       [ 6.2,  3.4,  5.4,  2.3],
       [ 5.9,  3. ,  5.1,  1.8]]) #类型是numpy.array
>>> iris.data.size  
600  #共600/4=150个样本
>>> iris.target_names
array(['setosa', 'versicolor', 'virginica'], 
      dtype='|S10')
>>> iris.target
array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,....., 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ......, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])
>>> iris.target.size
150
>>> from sklearn.naive_bayes import GaussianNB
>>> clf = GaussianNB()
>>> clf.fit(iris.data, iris.target)
>>> clf.predict(iris.data[0])
array([0])   # 预测正确
>>> clf.predict(iris.data[149])
array([2])   # 预测正确
>>> data = numpy.array([6,4,6,2])
>>> clf.predict(data)
array([2])  # 预测结果很合理
```

多项式模型如下：

```python
>>> import numpy as np
>>> X = np.random.randint(5, size=(6, 100))
>>> y = np.array([1, 2, 3, 4, 5, 6])
>>> from sklearn.naive_bayes import MultinomialNB
>>> clf = MultinomialNB()
>>> clf.fit(X, y)
MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)
>>> print(clf.predict(X[2]))
[3]
```

值得注意的是，多项式模型在训练一个数据集结束后可以继续训练其他数据集而无需将两个数据集放在一起进行训练。在 sklearn 中，MultinomialNB() 类的partial_fit() 方法可以进行这种训练。这种方式特别适合于训练集大到内存无法一次性放入的情况。

在第一次调用 `partial_fit()`  时需要给出所有的分类标号。

```python
>>> import numpy
>>> from sklearn.naive_bayes import MultinomialNB
>>> clf = MultinomialNB() 
>>> clf.partial_fit(numpy.array([1,1]), numpy.array(['aa']), ['aa','bb'])
GaussianNB()
>>> clf.partial_fit(numpy.array([6,1]), numpy.array(['bb']))
GaussianNB()
>>> clf.predict(numpy.array([9,1]))
array(['bb'], 
      dtype='|S2')
```

伯努利模型如下：

```python
>>> import numpy as np
>>> X = np.random.randint(2, size=(6, 100))
>>> Y = np.array([1, 2, 3, 4, 4, 5])
>>> from sklearn.naive_bayes import BernoulliNB
>>> clf = BernoulliNB()
>>> clf.fit(X, Y)
BernoulliNB(alpha=1.0, binarize=0.0, class_prior=None, fit_prior=True)
>>> print(clf.predict(X[2]))
[3]
```





------

## 参考

- 《统计学习方法》
- [机器学习常见算法个人总结（面试用）](http://kubicode.me/2015/08/16/Machine%20Learning/Algorithm-Summary-for-Interview/)
- [朴素贝叶斯理论推导与三种常见模型](http://blog.csdn.net/u012162613/article/details/48323777)
- [朴素贝叶斯的三个常用模型：高斯、多项式、伯努利](http://www.letiantian.me/2014-10-12-three-models-of-naive-nayes/)
- 深度学习 500 问：https://github.com/scutan90/DeepLearning-500-questions

















# 参考

1. 《深度学习》
2. 深度学习 500 问：https://github.com/scutan90/DeepLearning-500-questions
3. https://www.zhihu.com/question/358632429/answer/919562000
4. https://www.zhihu.com/question/68109802/answer/263503269
5. 《hands-on-ml-with-sklearn-and-tf》
6. Andrew Ng 在 Coursea 上的[机器学习课程](https://www.coursera.org/learn/machine-learning)



