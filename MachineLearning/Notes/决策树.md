# 1. 基本原理

决策树（Decision Tree）是一种分而治之的决策过程。一个困难的预测问题，通过树的分支节点，被划分成两个或多个较为简单的子集，从结构上划分为不同的子问题。将依规则分割数据集的过程不断递归下去（Recursive Partitioning）。随着树的深度不断增加，分支节点的子集越来越小，所需要提的问题数也逐渐简化。当分支节点的深度或者问题的简单程度满足一定的停止规则（Stopping Rule）时, 该分支节点会停止分裂，此为自上而下的停止阈值（Cutoff Threshold）法；有些决策树也使用自下而上的剪枝（Pruning）法。

决策树学习本质上是从训练数据集中归纳出一组分类规则，即是带有判决规则(if-then)的一种树，也可以说是**由训练数据集估计条件概率模型**。它使用的损失函数通常是**正则化的极大似然函数**，其策略是以损失函数为目标函数的最小化。

决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。





# 2. 决策树的三要素

一棵决策树的生成过程主要分为下3个部分： 

- **特征选择**：从训练数据中众多的特征中选择一个特征作为当前节点的分裂标准，如何选择特征有着很多不同量化评估标准，从而衍生出不同的决策树算法。 

- **决策树生成**：根据选择的特征评估标准，从上至下递归地生成子节点，直到数据集不可分则决策树停止生长。树结构来说，递归结构是最容易理解的方式。 

- **剪枝**：决策树容易过拟合，一般来需要剪枝，缩小树结构规模、缓解过拟合。剪枝技术有预剪枝和后剪枝两种。

**决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优。**



## 2.1 特征选择

熵：度量随机变量的不确定性。 

定义：假设随机变量X的可能取值有$x_{1},x_{2},...,x_{n}$，对于每一个可能的取值$x_{i}$，其概率为$P(X=x_{i})=p_{i},i=1,2...,n$。随机变量的熵为：
$$
H(X)=-\sum_{i=1}^{n}p_{i}log_{2}p_{i}
$$
**熵越大，随机变量的不确定性就越大**。

对于样本集合，假设样本有k个类别，每个类别的概率为$\frac{|C_{k}|}{|D|}$，其中 ${|C_{k}|}{|D|}$为类别为k的样本个数，$|D|$为样本总数。样本集合D的熵为：
$$
H(D)=-\sum_{k=1}^{k}\frac{|C_{k}|}{|D|}log_{2}\frac{|C_{k}|}{|D|}
$$
条件熵的定义为：$H(Y|X) = \sum_{i=1}^n p_iH(Y|X=x_i)$

特征选择的准则通常是**信息增益或者信息增益比**。

已经有了**熵作为衡量训练样例集合纯度**的标准，现在可以定义属性分类训练数据的效力的度量标准。这个标准被称为“**信息增益（information gain）**”。

简单的说，一个属性的信息增益就是由于使用这个属性分割样例而导致的期望熵降低(或者说，**样本按照某属性划分时造成熵减少的期望,个人结合前面理解，总结为用来衡量给定的属性区分训练样例的能力**)。更精确地讲，**一个属性 A 相对样例集合 S 的信息增益 Gain(S,A) 被定义为**：

![](https://img-blog.csdn.net/20170213171939623?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

其中 Values(A) 是属性 A 所有可能值的集合，$S_v$ 是 S 中属性 A 的值为 v 的子集，注意上式第一项就是原集合S 的熵，第二项是用 A 分类 S 后的{熵的期望值，第二项描述的期望熵就是每个子集的熵的加权和，权值为属性 $S_v$ 的样例占原始样例 S 的比例 $\frac{|S_v|}{|S|}$ ，所以 Gain(S,A) 是由于知道属性 A 的值而导致的期望熵减少，换句话来讲，Gain(S,A) 是由于给定属性 A 的值而得到的关于目标函数值的信息。

信息增益的缺点是**存在偏向于选择取值较多的特征的问题**。

为了解决这个问题，可以使用**信息增益比**。

因此，特征 A 对训练数据集 D 的信息增益比 $g_R(D,A)$ 的定义如下：
$$
g_R(D, A) = \frac{g(D,A)}{H_A(D)}
$$
其中 $g(D,A)$ 是信息增益，而 $H_A(D)=-\sum_{i=1}^n \frac{|D_i|}{|D|} log_2 \frac{|D_i|}{|D|}$ ,其中 $n$ 是特征 A 取值的个数。

不过对于信息增益比，其也存在**对可取值数目较少的属性有所偏好的问题**。



## 2.2 决策树的生成

来会介绍决策树生成的算法，包括**ID3, C4.5**算法。

### 2.2.1 ID3算法

ID3 算法的核心是在决策树各个结点上应用**信息增益准则选择特征**，递归地构建决策树。具体步骤如下所示：

![](https://img-blog.csdn.net/20170213204003966?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

ID3 算法思路总结如下：

1. 首先是针对当前的集合，计算每个特征的信息增益
2. 然后选择**信息增益最大的特征**作为当前节点的决策决策特征
3. 根据特征不同的类别划分到不同的子节点（比如年龄特征有青年，中年，老年，则划分到 3 颗子树）
4. 然后继续对子节点进行递归，直到所有特征都被划分



**ID3的缺点是**

- 容易造成过度拟合（over fitting）； 
- 只能处理标称型数据（离散型）； 
- 信息增益的计算依赖于特征数目较多的特征，而属性取值最多的属性并不一定最优； 
- 抗噪性差，训练例子中正例和反例的比例较难控制



### 2.2.2 C4.5算法

C4.5算法继承了 ID3 算法的优点，并在以下几方面对 ID3 算法进行了改进：

- **用信息增益率来选择属性**，克服了用信息增益选择属性时偏向选择取值多的属性的不足；
- 在树构造过程中进行剪枝；
- 能够完成对连续属性的离散化处理；
- 能够对不完整数据进行处理。

C4.5算法有如下优点：**产生的分类规则易于理解，准确率较高**。

其缺点是：

1. **算法低效**，在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效 
2. **内存受限，**只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。 
3. 对可取值数目少的属性有所偏好

算法的实现过程如下:

![](https://img-blog.csdn.net/20170213204817800?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

实际上由于信息增益比的缺点，C4.5 算法并没有直接选择信息增益比最大的候选划分属性，**而是先从候选划分属性中找出信息增益高于平均水平的属性**，再从中选择信息增益比最高的。

无论是 ID3 还是 C4.5 **最好在小数据集上使用**，决策树分类一般**只适用于小数据**。当属性取值很多时最好选择 C4.5 算法，ID3 得出的效果会非常差。



## 2.3 剪枝

在生成树的过程中，如果没有剪枝的操作的话，就会长成每一个叶都是单独的一类的样子。这样对我们的训练集是完全拟合的，但是对测试集则是非常不友好的，泛化能力不行。**因此，我们要减掉一些枝叶，使得模型泛化能力更强。** 
根据剪枝所出现的时间点不同，分为预剪枝和后剪枝。

**预剪枝是在决策树的生成过程中进行的；后剪枝是在决策树生成之后进行的**。 前者虽然简单但实用性不强，因为我们很难精确的判断何时终止树的生长。

决策树的剪枝往往是**通过极小化决策树整体的损失函数或代价函数来实现的**。简单来说，就是对比剪枝前后整体树的损失函数或者是准确率大小来判断是否需要进行剪枝。

决策树剪枝算法有多种，具体参考[决策树剪枝算法](http://blog.csdn.net/yujianmin1990/article/details/49864813)这篇文章。

常见后剪枝方法有四种：

- 悲观错误剪枝（PEP）
- 最小错误剪枝（MEP）
- 代价复杂度剪枝（CCP）
- 基于错误的剪枝（EBP）



## 2.4 三种特征选择算法的对比

### ID.3

1. **最优划分属性选择方法**：信息增益
2. **分支数**：可多分支
3. **能否处理连续值特征**：不能
4. **缺点**：偏好与可取值数目多的属性

### C4.5

1. **最优划分属性选择方法**：增益率
2. **分支数**：可多分支
3. **能否处理连续值特征**：能，C4.5 决策树算法采用的**二分法**机制来处理连续属性。对于连续属性 a，首先将 n 个不同取值进行从小到大排序，选择相邻 a 属性值的平均值 t 作为候选划分点，划分点将数据集分为两类，因此有包含 n－1 个候选划分点的集合，分别计算出每个划分点下的信息增益，选择信息增益最大对应的划分点，<u>仍然以信息增益最大的属性作为分支属性。</u>
4. **缺点**：增益率对可取值数目少的属性有所偏好，因此C4.5算法并不是直接选择增益率最大的候选划分属性，而是使用了一个启发式：**先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。**

### CART

1. **最优划分属性选择方法**：基尼系数
2. **分支数**：二叉树
3. **能否处理连续值特征**：能，做法与C4.5一样。也可以用于回归，用于回归时通过最小化均方差能够找到最靠谱的分枝依据，回归树的具体做法可见机器学习的问题33。
4. **优点**：与ID3、C4.5不同，在ID3或C4.5的一颗子树中，离散特征只会参与一次节点的建立，但是在CART中之前处理过的属性在后面还可以参与子节点的产生选择过程。



# 3. 分类回归树（CART）

分类回归树(Classification And Regression Tree)是一个**决策二叉树**，在通过递归的方式建立，每个节点在分裂的时候都是希望通过最好的方式将剩余的样本划分成两类，这里的分类指标：

1. **分类树：基尼指数最小化(gini_index)**
2. **回归树：平方误差最小化**

分类树的生成步骤如下所示：

![](https://img-blog.csdn.net/20170213212619889?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQvbGMwMTM=/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/SouthEast)

简单总结如下：

1. 首先是根据当前特征计算他们的基尼增益
2. 选择**基尼增益最小**的特征作为划分特征
3. 从该特征中查找基尼指数最小的分类类别作为最优划分点
4. 将当前样本划分成两类，一类是划分特征的类别等于最优划分点，另一类就是不等于
5. 针对这两类递归进行上述的划分工作，直达所有叶子指向同一样本目标或者叶子个数小于一定的阈值

基尼指数的计算公式为$Gini(p) = 1 - \sum_{k=1}^K p_k^2$。K是类别的数目，$p_k$表示样本属于第k类的概率值。它可以用来度量分布不均匀性（或者说不纯），总体的类别越杂乱，GINI指数就越大（跟熵的概念很相似）。

给定一个数据集D，在特征A的条件下，其基尼指数定义为$Gini(D,A) = \sum_{i=1}^n \frac{|D_i|}{|D|} Gini(D_i)$。

回归树：

> 回归树是以平方误差最小化的准则划分为两块区域

1. 遍历特征计算最优的划分点$s$，
   使其最小化的平方误差是：$min \{min(∑_i^{R1}((y_i−c_1)^2))+min(∑_i^{R2}((y_i−c_2)^2))\}$
   计算根据s划分到左侧和右侧子树的目标值与预测值之差的平方和最小，这里的预测值是两个子树上输入$x_i$样本对应$y_i$的均值

2. 找到最小的划分特征j以及其最优的划分点$s$,根据特征$j$以及划分点$s$将现有的样本划分为两个区域，一个是在特征$j$上小于等于$s$，另一个在特征$j$上大于$s$
   $$
   R1(j)= \{x|x(j)≤s\} \\
   R2(j)=\{x|x(j)>s\}  \\
   c_m = \frac{1}{N_m} \sum_{x_i \in R_m(j, s)} y_i, m = 1,2, \quad x\in R_m
   $$

3. 进入两个子区域按上述方法继续划分，直到到达停止条件

回归树的缺点：

- **不如线性回归普遍；**
- **要求大量训练数据；**
- **难以确定某个特征的整体影响；**
- **比线性回归模型难解释**

关于CART剪枝的方法可以参考[决策树系列（五）——CART](http://www.cnblogs.com/yonghao/p/5135386.html)。



# 4. 停止条件

1. 直到每个叶子节点都只有一种类型的记录时停止，（这种方式很容易过拟合）
2. 另一种是当叶子节点的样本数目小于一定的阈值或者节点的信息增益小于一定的阈值时停止



# 5. 关于特征与目标值

1. 特征离散 目标值离散：可以使用ID3，cart
2. 特征连续 目标值离散：将连续的特征离散化 可以使用ID3，cart



# 6. 连续值属性的处理

**C4.5既可以处理离散型属性，也可以处理连续性属性。**在选择某节点上的分枝属性时，对于离散型描述属性，C4.5的处理方法与ID3相同。对于连续分布的特征，其处理方法是：


**先把连续属性转换为离散属性再进行处理。**虽然本质上属性的取值是连续的，但对于有限的采样数据它是离散的，如果有N条样本，那么我们有N-1种离散化的方法：$<=v_j$的分到左子树，$>v_j$的分到右子树。计算这N-1种情况下最大的信息增益率。另外，对于连续属性先进行排序（升序），只有在决策属性（即分类发生了变化）发生改变的地方才需要切开，这可以显著减少运算量。**经证明，在决定连续特征的分界点时采用增益这个指标**（因为若采用增益率，splittedinfo影响分裂点信息度量准确性，若某分界点恰好将连续特征分成数目相等的两部分时其抑制作用最大），**而选择属性的时候才使用增益率这个指标能选择出最佳分类特征。**



在C4.5中，对连续属性的处理如下：

1、对特征的取值进行升序排序

2、两个特征取值之间的中点作为可能的分裂点，将数据集分成两部分，计算**每个可能的分裂点的信息增益**（InforGain）。**优化算法就是只计算分类属性发生改变的那些特征取值。**

3、选择修正后**信息增益(InforGain)最大的分裂点**作为该特征的最佳分裂点

4、计算**最佳分裂点的信息增益率（Gain Ratio）作为特征的Gain Ratio**。注意，此处需对最佳分裂点的信息增益进行修正：减去log2(N-1)/|D|（N是连续特征的取值个数，D是训练数据数目，此修正的原因在于：**当离散属性和连续属性并存时，C4.5算法倾向于选择连续特征做最佳树分裂点**）





# 7. 理想的决策树

1. 叶子节点数尽量少
2. 叶子节点的深度尽量小(太深可能会过拟合)



# 8. 过拟合原因

采用上面算法生成的决策树在事件中往往会导致过滤拟合。也就是该决策树对训练数据可以得到很低的错误率，但是运用到测试数据上却得到非常高的错误率。过渡拟合的原因有以下几点：

- **噪音数据**：训练数据中存在噪音数据，决策树的某些节点有噪音数据作为分割标准，导致决策树无法代表真实数据。
- **缺少代表性数据**：训练数据没有包含所有具有代表性的数据，导致某一类数据无法很好的匹配，这一点可以通过观察混淆矩阵（Confusion Matrix）分析得出。
- **多重比较（Mulitple Comparition）**：举个列子，股票分析师预测股票涨或跌。假设分析师都是靠随机猜测，也就是他们正确的概率是0.5。每一个人预测10次，那么预测正确的次数在8次或8次以上的概率为 [![image](http://images.cnitblog.com/blog/349490/201303/15154352-dd92afccc91e4e2e9a08578d8ba9ab04.png)](http://images.cnitblog.com/blog/349490/201303/15154352-831a596c9a3c4fcca3d6e8f863b2f91f.png)，只有5%左右，比较低。但是如果50个分析师，每个人预测10次，选择至少一个人得到8次或以上的人作为代表，那么概率为 [![image](http://images.cnitblog.com/blog/349490/201303/15154353-c827fc2a20e74c31a3f6a1f1d64a436c.png)](http://images.cnitblog.com/blog/349490/201303/15154352-be8974a2a79a4662a4579210187d31fa.png)，概率十分大，随着分析师人数的增加，概率无限接近1。但是，选出来的分析师其实是打酱油的，他对未来的预测不能做任何保证。上面这个例子就是**多重比较**。这一情况和决策树选取分割点类似，需要在每个变量的每一个值中选取一个作为分割的代表，所以选出一个噪音分割标准的概率是很大的。



# 9. 解决决策树的过拟合

1. 剪枝
   - 预剪枝：**在分裂节点的时候设计比较苛刻的条件**，如不满足则直接停止分裂（这样干决策树无法到最优，也无法得到比较好的效果）

   - 后剪枝：**在树建立完之后，用单个节点代替子树**，节点的分类采用子树中主要的分类（这种方法比较浪费前面的建立过程）
2. 交叉验证
3. 随机森林





# 10. 决策树算法优缺点

## 优点

1、决策树算法易理解，机理解释起来简单。 

2、决策树算法可以用于小数据集。

3、决策树算法的时间复杂度较小，为用于训练决策树的数据点的对数。

4、相比于其他算法智能分析一种类型变量，决策树算法可处理数字和数据的类别。

5、能够处理多输出的问题。 

6、对缺失值不敏感。

7、可以处理不相关特征数据。

8、效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。



## 缺点

1、对连续性的字段比较难预测。

2、容易出现过拟合。

3、当类别太多时，错误可能就会增加的比较快。

4、在处理特征关联性比较强的数据时表现得不是太好。

5、对于各类别样本数量不一致的数据，在决策树当中，信息增益的结果偏向于那些具有更多数值的特征。







------

# 参考

1. 深度学习500 问：https://github.com/scutan90/DeepLearning-500-questions
2. [决策树（三）--完整总结（ID3，C4.5，CART,剪枝，替代）](https://blog.csdn.net/app_12062011/article/details/52136117)
3. [决策树剪枝算法](http://blog.csdn.net/yujianmin1990/article/details/49864813)

