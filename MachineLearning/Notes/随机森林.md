# 随机森林

## 1. 简介

**随机森林**指的是利用多棵树对样本进行训练并预测的一种分类器。它是由多棵CART(Classification And Regression Tree)构成的。它的特点有：

- 对于每棵树，其**使用的训练集是从总的训练集中有放回采样出来的**，这意味着总训练集中有些样本可能多次出现在一棵树的训练集中，也可能从未出现在一棵树的训练集中。

- 在训练每棵树的节点时，**使用的特征是从所有特征中按照一定比例随机地无放回的抽取的**，假设总的特征数是`M`,则这个比例可以是$\sqrt(M), \frac{1}{2} \sqrt(M), 2\sqrt(M)$。





## 2. 基本原理

### 2.1 训练过程

随机森林的训练过程可以总结如下：

1. 首先，给定训练集`S`，测试集`T`，特征维数`F`。确定参数：使用到的CART的数量`t`，每棵树的深度`d`，每个节点使用到的特征数量`f`，终止条件：节点上最少样本数`s`，节点上最少的信息增益`m`

对于第1-t棵树，`i=1-t`：

2. 从S中有放回的抽取大小和S一样的训练集S(i)，作为根节点的样本，从根节点开始训练

3. 如果当前节点上达到终止条件，则设置当前节点为叶子节点，如果是分类问题，该叶子节点的预测输出为当前节点样本集合中数量最多的那一类`c(j)`，概率`p`为`c(j)`占当前样本集的比例；如果是回归问题，预测输出为当前节点样本集各个样本值的平均值。然后继续训练其他节点。

   如果当前节点没有达到终止条件，则从 F 维特征中无**放回的随机选取 f 维特征。利用这 f 维特征，寻找分类效果最好的一维特征`k`及其阈值`th`，当前节点上样本第k维特征小于`th`的样本被划分到左节点，其余的被划分到右节点。**继续训练其他节点。

4. 重复 2-3 步直到所有节点都训练过了或者被标记为叶子节点。

5. 重复 2-4 步直到所有CART都被训练过。



### 2.2 预测过程

对于第1-t 棵树，i=1-t：

1. 从当前树的根节点开始，根据当前节点的阈值th，判断是进入左节点(`<th`)还是进入右节点(`>=th`)，直到到达，某个叶子节点，并输出预测值。

2. 重复执行 1，直到所有 t 棵树都输出了预测值。**如果是分类问题，则输出为所有树中预测概率总和最大的那一个类，即对每个c(j)的p进行累计；如果是回归问题，则输出为所有树的输出的平均值**。



有关分类效果的评判标准，因为使用的是CART，因此使用的也是CART的评判标准，和C3.0,C4.5都不相同。

对于分类问题（将某个样本划分到某一类），也就是离散变量问题，CART使用 Gini 值作为评判标准。定义为$Gini(p) = 1 - \sum_{k=1}^K p_k^2$，  $p_k$ 为当前节点上数据集中第k类样本的比例。

例如：分为2类，当前节点上有100个样本，属于第一类的样本有70个，属于第二类的样本有30个，则$Gini=1-0.7×07-0.3×0.3=0.42$，可以看出，**类别分布越平均，Gini值越大，类分布越不均匀，Gini值越小**。

在寻找最佳的分类特征和阈值时，评判标准为：$argmax（Gini-GiniLeft-GiniRight）$，即寻找最佳的特征f和阈值th，使得当前节点的Gini值减去左子节点的Gini和右子节点的Gini值最大。

对于回归问题，相对更加简单，直接使用 $argmax(Var-VarLeft-VarRight)$作为评判标准，即当前节点训练集的方差Var减去减去左子节点的方差VarLeft和右子节点的方差VarRight值，求其最大值。



## 3. 特征重要性度量和特征选择

### 3.1 特征重要性度量

计算某个特征 X 的重要性时，具体步骤如下：

1. 对每一颗决策树，选择相应的**袋外数据（out of bag，OOB）计算袋外数据误差**，记为 errOOB1.

   所谓袋外数据是指，每次建立决策树时，通过重复抽样得到一个数据用于训练决策树，这时还有**大约1/3的数据没有被利用**，没有参与决策树的建立。这部分数据可以用于对决策树的性能进行评估，计算模型的预测错误率，称为袋外数据误差。

   **这已经经过证明是无偏估计的,所以在随机森林算法中不需要再进行交叉验证或者单独的测试集来获取测试集误差的无偏估计。**

2. 随机对袋外数据OOB所有样本的特征X加入**噪声干扰**（可以随机改变样本在特征X处的值），再次计算袋外数据误差，记为errOOB2。

3. 假设森林中有N棵树，则特征X的重要性=$∑\frac{errOOB2-errOOB1}{N}$。这个数值之所以能够说明特征的重要性是因为，**如果加入随机噪声后，袋外数据准确率大幅度下降（即errOOB2上升），说明这个特征对于样本的预测结果有很大影响，进而说明重要程度比较高。**



### 3.2 特征选择

在特征重要性的基础上，特征选择的步骤如下：

1. 计算每个特征的重要性，并按降序排序
2. 确定要剔除的比例，依据特征重要性剔除相应比例的特征，得到一个新的特征集
3. 用新的特征集重复上述过程，直到剩下m个特征（m为提前设定的值）。
4. 根据上述过程中得到的各个特征集和特征集对应的袋外误差率，选择袋外误差率最低的特征集。



## 4. 和 bagging 的区别

主要是以下两点区别：

1. Random forest是选与**输入样本的数目相同多**的样本（可能一个样本会被选取多次，同时也会造成一些样本不会被选取到），而bagging一般选取比**输入样本的数目少**的样本；

2. bagging是用**全部特征**来得到分类器，而Random forest是需要从全部特征中**选取其中的一部分**来训练得到分类器； **一般Random forest效果比bagging效果好！**





## 5. 优缺点

### 5.1 优点

1. 在数据集上表现良好，在当前的很多数据集上，相对其他算法有着很大的优势
2. 可以处理多种数据情况
   - 它能够处理很高维度（特征很多）的数据，并且不用做特征选择
   - 对于不平衡的数据集来说，它可以平衡误差
   - 可以应用在特征缺失的数据集上，并仍然有不错的性能
3. 对于特征方面，可以评估特征的重要性，并且在训练过程中，能够检测到特征间的互相影响
4. 在创建随机森林的时候，对 generlization error 使用的是无偏估计
5. 实现比较简单，训练速度快，容易做成并行化方法



### 5.2 缺点

- 随机森林已经被证明在某些**噪音较大**的分类或回归问题上会过拟合

- 对于有不同取值的属性的数据，**取值划分较多的属性会对随机森林产生更大的影响**，所以随机森林在这种数据上产出的属性权值是不可信的。





## 6. 代码实现

简单使用 sklearn 中随机森林算法的例子：

```python
#Import Library
from sklearn.ensemble import RandomForestClassifier
#Assumed you have, X (predictor) and Y (target) for training data set and x_test(predictor) of test_dataset

# Create Random Forest object
model= RandomForestClassifier()

# Train the model using the training sets and check score
model.fit(X, y)

#Predict Output
predicted= model.predict(x_test)
```

此外，OpenCV中也实现了随机森林算法。具体使用例子可以查看 [RandomForest随机森林总结](http://www.cnblogs.com/hrlnw/p/3850459.html)。






---
## 参考

1. 深度学习500 问：https://github.com/scutan90/DeepLearning-500-questions
2. [RandomForest随机森林总结](http://www.cnblogs.com/hrlnw/p/3850459.html)