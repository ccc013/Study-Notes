# 分类网络模型

记录一些经典的分类网络模型，从模型介绍、模型结构到模型特点进行介绍，主要包括了：

1. LeNet-5
2. AlexNet
3. ZFNet
4. NIN
5. VGG
6. GoogleNet
7. ResNet
8. Xception
9. MobileNet系列
10. EfficientNet
11. ResNeXt
12. DenseNet
13. SqueezeNet
14. ShuffleNet 系列
15. SENet
16. SKNet





------

## 1.LeNet-5

### 模型介绍

论文：《Gradient-Based Learning Applied to Document Recognition》

论文地址：http://axon.cs.byu.edu/~martinez/classes/678/Papers/Convolution_nets.pdf

LeNet-5是由$LeCun$ 提出的一种用于识别手写数字和机器印刷字符的卷积神经网络（Convolutional Neural Network，CNN），其命名来源于作者$LeCun$的名字，5则是其研究成果的代号，在LeNet-5之前还有LeNet-4和LeNet-1鲜为人知。LeNet-5阐述了图像中像素特征之间的相关性能够由参数共享的卷积操作所提取，同时使用卷积、下采样（池化）和非线性映射这样的组合结构，是当前流行的大多数深度图像识别网络的基础。



### 模型结构

![](images/LeNet-5.png)

如图所示，LeNet-5一共包含7层（输入层不作为网络结构），分别由2个卷积层、2个下采样层和3个连接层组成，网络的参数配置如表4.1所示，其中下采样层和全连接层的核尺寸分别代表采样范围和连接矩阵的尺寸（如卷积核尺寸中的$“5\times5\times1/1,6”$表示核大小为$5\times5\times1$、步长为$1$且核个数为6的卷积核）



|     网络层      |       输入尺寸       |          核尺寸          |       输出尺寸       |          可训练参数量           |
| :-------------: | :------------------: | :----------------------: | :------------------: | :-----------------------------: |
|   卷积层$C_1$   | $32\times32\times1$  |  $5\times5\times1/1,6$   | $28\times28\times6$  |  $(5\times5\times1+1)\times6$   |
|  下采样层$S_2$  | $28\times28\times6$  |       $2\times2/2$       | $14\times14\times6$  |       $(1+1)\times6$ $^*$       |
|   卷积层$C_3$   | $14\times14\times6$  |  $5\times5\times6/1,16$  | $10\times10\times16$ |            $1516^*$             |
|  下采样层$S_4$  | $10\times10\times16$ |       $2\times2/2$       |  $5\times5\times16$  |         $(1+1)\times16$         |
| 卷积层$C_5$$^*$ |  $5\times5\times16$  | $5\times5\times16/1,120$ | $1\times1\times120$  | $(5\times5\times16+1)\times120$ |
|  全连接层$F_6$  | $1\times1\times120$  |      $120\times84$       |  $1\times1\times84$  |        $(120+1)\times84$        |
|     输出层      |  $1\times1\times84$  |       $84\times10$       |  $1\times1\times10$  |        $(84+1)\times10$         |

> $^*$ 在LeNet中，下采样操作和池化操作类似，但是在得到采样结果后会乘以一个系数和加上一个偏置项，所以下采样的参数个数是$(1+1)\times6$而不是零。
> 		
> $^*$ $C_3$卷积层可训练参数并未直接连接$S_2$中所有的特征图（Feature Map），而是采用如图4.2所示的采样特征方式进行连接（稀疏连接），生成的16个通道特征图中分别按照相邻3个特征图、相邻4个特征图、非相邻4个特征图和全部6个特征图进行映射，得到的参数个数计算公式为$6\times(25\times3+1)+6\times(25\times4+1)+3\times(25\times4+1)+1\times(25\times6+1)=1516$，在原论文中解释了使用这种采样方式原因包含两点：限制了连接数不至于过大（当年的计算能力比较弱）;强制限定不同特征图的组合可以使映射得到的特征图学习到不同的特征模式。

![FeatureMap](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/LeNet_featureMap.png)



> $^*$ $C_5$卷积层在图4.1中显示为全连接层，原论文中解释这里实际采用的是卷积操作，只是刚好在$5\times5$卷积后尺寸被压缩为$1\times1$，输出结果看起来和全连接很相似。





### 模型特点

- 卷积网络使用一个3层的序列组合：**卷积、下采样（池化）、非线性映射**（LeNet-5最重要的特性，奠定了目前深层卷积网络的基础）
- 使用卷积提取空间特征
- 使用映射的空间均值进行下采样
- 使用$tanh$或$sigmoid$进行非线性映射
- 多层神经网络（MLP）作为最终的分类器
- 层间的稀疏连接矩阵以避免巨大的计算开销





------

## 2.AlexNet

学习笔记：





## 3. ZFNet

### 模型介绍

论文：《Visualizing and Understanding Convolutional Networks》

论文地址：

ZFNet是由$Matthew$ $D. Zeiler$和$Rob$ $Fergus$在AlexNet基础上提出的大型卷积网络，在2013年ILSVRC图像分类竞赛中以11.19%的错误率获得冠军（实际上原ZFNet所在的队伍并不是真正的冠军，原ZFNet以13.51%错误率排在第8，真正的冠军是$Clarifai$这个队伍，而$Clarifai$这个队伍所对应的一家初创公司的CEO又是$Zeiler$，而且$Clarifai$对ZFNet的改动比较小，所以通常认为是ZFNet获得了冠军）。

ZFNet实际上是微调（fine-tuning）了的AlexNet，并通过反卷积（Deconvolution）的方式可视化各层的输出特征图，进一步解释了卷积操作在大型网络中效果显著的原因。



### 模型结构

![](images/ZFNet.png)

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/AlexNet_2.png)

如图所示，ZFNet与AlexNet类似，都是由8层网络组成的卷积神经网络，其中包含5层卷积层和3层全连接层。两个网络结构最大的不同在于，ZFNet第一层卷积采用了$7\times7\times3/2$的卷积核替代了AlexNet中第一层卷积核$11\times11\times3/4$的卷积核。

下图中ZFNet相比于AlexNet在第一层输出的特征图中包含更多中间频率的信息，而AlexNet第一层输出的特征图大多是低频或高频的信息，对中间频率特征的缺失导致后续网络层次如下图（c）能够学习到的特征不够细致，而导致这个问题的根本原因在于**AlexNet在第一层中采用的卷积核和步长过大**。

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/zfnet-layer1.png)

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/zfnet-layer2.png)

（a）ZFNet第一层输出的特征图（b）AlexNet第一层输出的特征图（c）AlexNet第二层输出的特征图（d）ZFNet第二层输出的特征图

ZFNet网络参数配置如下：

|      网络层       |        输入尺寸        |          核尺寸           |        输出尺寸        |           可训练参数量           |
| :---------------: | :--------------------: | :-----------------------: | :--------------------: | :------------------------------: |
| 卷积层$C_1$ $^*$  | $224\times224\times3$  |  $7\times7\times3/2,96$   | $110\times110\times96$ |  $(7\times7\times3+1)\times96$   |
| 下采样层$S_{max}$ | $110\times110\times96$ |       $3\times3/2$        |  $55\times55\times96$  |                0                 |
| 卷积层$C_2$ $^*$  |  $55\times55\times96$  | $5\times5\times96/2,256$  | $26\times26\times256$  | $(5\times5\times96+1)\times256$  |
| 下采样层$S_{max}$ | $26\times26\times256$  |       $3\times3/2$        | $13\times13\times256$  |                0                 |
|    卷积层$C_3$    | $13\times13\times256$  | $3\times3\times256/1,384$ | $13\times13\times384$  | $(3\times3\times256+1)\times384$ |
|    卷积层$C_4$    | $13\times13\times384$  | $3\times3\times384/1,384$ | $13\times13\times384$  | $(3\times3\times384+1)\times384$ |
|    卷积层$C_5$    | $13\times13\times384$  | $3\times3\times384/1,256$ | $13\times13\times256$  | $(3\times3\times384+1)\times256$ |
| 下采样层$S_{max}$ | $13\times13\times256$  |       $3\times3/2$        |  $6\times6\times256$   |                0                 |
|   全连接层$F_6$   |  $6\times6\times256$   |     $9216\times4096$      |  $1\times1\times4096$  |       $(9216+1)\times4096$       |
|   全连接层$F_7$   |  $1\times1\times4096$  |     $4096\times4096$      |  $1\times1\times4096$  |       $(4096+1)\times4096$       |
|   全连接层$F_8$   |  $1\times1\times4096$  |     $4096\times1000$      |  $1\times1\times1000$  |       $(4096+1)\times1000$       |

> 卷积层$C_1$与AlexNet中的$C_1$有所不同，采用$7\times7\times3/2$的卷积核代替$11\times11\times3/4$，使第一层卷积输出的结果可以包含更多的中频率特征，对后续网络层中多样化的特征组合提供更多选择，有利于捕捉更细致的特征。
>
> 卷积层$C_2$采用了步长2的卷积核，区别于AlexNet中$C_2$的卷积核步长，所以输出的维度有所差异。





### 模型特点

ZFNet与AlexNet在结构上几乎相同，此部分虽属于模型特性，但准确地说应该是ZFNet原论文中可视化技术的贡献。

- 可视化技术揭露了激发模型中每层单独的特征图。
- 可视化技术允许观察在训练阶段特征的演变过程且诊断出模型的潜在问题。
- 可视化技术用到了多层解卷积网络，即由特征激活返回到输入像素空间。
- 可视化技术进行了分类器输出的敏感性分析，即通过阻止部分输入图像来揭示那部分对于分类是重要的。
- 可视化技术提供了一个非参数的不变性来展示来自训练集的哪一块激活哪个特征图，不仅需要裁剪输入图片，而且自上而下的投影来揭露来自每块的结构激活一个特征图。
- 可视化技术依赖于解卷积操作，即卷积操作的逆过程，将特征映射到像素上。



------

## 4. Network in Network

### 模型介绍

论文：《Network in network》

论文地址：

Network In Network (NIN)是由$Min Lin$等人提出，在CIFAR-10和CIFAR-100分类任务中达到当时的最好水平，因其网络结构是由三个多层感知机堆叠而被成为NIN。

NIN以一种全新的角度审视了卷积神经网络中的卷积核设计，**通过引入子网络结构代替纯卷积中的线性映射部分**，这种形式的网络结构激发了更复杂的卷积神经网络的结构设计，其中 GoogLeNet的Inception结构就是来源于这个思想。



### 模型结构

![](images/network_in_network.png)

NIN由三层的多层感知卷积层（MLPConv Layer）构成，每一层多层感知卷积层内部由若干层的局部全连接层和非线性激活函数组成，代替了传统卷积层中采用的线性卷积核。

在网络推理（inference）时，这个多层感知器会对输入特征图的局部特征进行划窗计算，并且每个划窗的局部特征图对应的乘积的权重是共享的，这两点是和传统卷积操作完全一致的，**最大的不同在于多层感知器对局部特征进行了非线性的映射**，而传统卷积的方式是线性的。

NIN的网络参数配置下表所示（原论文并未给出网络参数，表中参数为编者结合网络结构图和CIFAR-100数据集以$3\times3$卷积为例给出）。

|          网络层           |       输入尺寸        |         核尺寸          |       输出尺寸        |            参数个数             |
| :-----------------------: | :-------------------: | :---------------------: | :-------------------: | :-----------------------------: |
| 局部全连接层$L_{11}$ $^*$ |  $32\times32\times3$  | $(3\times3)\times16/1$  | $30\times30\times16$  |  $(3\times3\times3+1)\times16$  |
|   全连接层$L_{12}$ $^*$   | $30\times30\times16$  |      $16\times16$       | $30\times30\times16$  |       $((16+1)\times16)$        |
|   局部全连接层$L_{21}$    | $30\times30\times16$  | $(3\times3)\times64/1$  | $28\times28\times64$  | $(3\times3\times16+1)\times64$  |
|     全连接层$L_{22}$      | $28\times28\times64$  |      $64\times64$       | $28\times28\times64$  |       $((64+1)\times64)$        |
|   局部全连接层$L_{31}$    | $28\times28\times64$  | $(3\times3)\times100/1$ | $26\times26\times100$ | $(3\times3\times64+1)\times100$ |
|     全连接层$L_{32}$      | $26\times26\times100$ |     $100\times100$      | $26\times26\times100$ |      $((100+1)\times100)$       |
|  全局平均采样$GAP$ $^*$   | $26\times26\times100$ | $26\times26\times100/1$ |  $1\times1\times100$  |               $0$               |

> 局部全连接层$L_{11}$实际上是对原始输入图像进行划窗式的全连接操作，因此划窗得到的输出特征尺寸为$30\times30$（$\frac{32-3_k+1}{1_{stride}}=30$）
> 全连接层$L_{12}$是紧跟$L_{11}$后的全连接操作，输入的特征是划窗后经过激活的局部响应特征，因此仅需连接$L_{11}$和$L_{12}$的节点即可，而每个局部全连接层和紧接的全连接层构成代替卷积操作的多层感知卷积层（MLPConv）。
> 全局平均采样层或全局平均池化层$GAP$（Global Average Pooling）将$L_{32}$输出的每一个特征图进行全局的平均池化操作，直接得到最后的类别数，可以有效地减少参数量。



### 模型特点

- 使用多层感知机结构来代替卷积的滤波操作，不但有效减少卷积核数过多而导致的参数量暴涨问题，还能通过引入非线性的映射来提高模型对特征的抽象能力。
- 使用全局平均池化来代替最后一个全连接层，能够有效地减少参数量（没有可训练参数），同时池化用到了整个特征图的信息，对空间信息的转换更加鲁棒，最后得到的输出结果可直接作为对应类别的置信度。



------

## 5. VGGNet





------

## 6. GoogleNet

学习笔记：

https://www.yuque.com/u164072/wnatqs/opcbn0




------

## 7. ResNet

学习笔记：

https://www.yuque.com/u164072/wnatqs/qp374t



------

## 8. Xception

学习笔记：

https://www.yuque.com/u164072/wnatqs/ms5elk



------

## 9. MobileNet 系列

学习笔记参考：

[轻量级网络](https://www.yuque.com/u164072/wnatqs/ms5elk)




------

## 10. EfficientNet

### 模型介绍

论文：

论文地址：





### 模型结构



### 模型特点



------

## 11. ResNeXt

### 模型介绍

论文：

论文地址：

基于ResNet和Inception的split+transform+concate结合。但效果却比ResNet、Inception、Inception-ResNet效果都要好。可以使用group convolution。

一般来说**增加网络表达能力**的途径有三种：

1. **增加网络深度**，如从AlexNet到ResNet，但是实验结果表明由网络深度带来的提升越来越小；

2. **增加网络模块的宽度**，但是宽度的增加必然带来指数级的参数规模提升，也非主流CNN设计；

3. **改善CNN网络结构设计**，如Inception系列和ResNeXt等。且实验发现增加Cardinatity即一个block中所具有的相同分支的数目可以更好的提升模型表达能力。

受精简而高效的Inception模块启发，ResNeXt将ResNet中非短路那一分支变为多个分支。



### 模型结构

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/ResNeXt_fig1.png" alt="preview" style="zoom:80%;" />



### 模型特点

1. 在 ResNet 的短路连接基础上，综合了 Inception 的优点，使用多分支进行处理，但是与 Inception不同的是，其**每个分支的结构都相同**。
2. ResNeXt巧妙地利用**分组卷积**进行实现。





------

## 12. DenseNet

学习笔记：

https://www.yuque.com/u164072/wnatqs/qc5qi1



------

## 13. SqueezeNet

### 模型介绍

论文：

论文地址：

提出了fire-module：squeeze层+expand层。Squeeze层就是1×1卷积，expand层用1×1和3×3分别卷积，然后concatenation。

squeezeNet参数是alexnet的1/50，经过压缩之后是1/510，但是准确率和alexnet相当。



### 模型结构





### 模型特点



------

## 14. ShuffleNet系列

学习笔记参考：

[轻量级网络](https://www.yuque.com/u164072/wnatqs/ms5elk)



------

## 15. SENet

### 模型介绍

论文：

论文地址：

SENet 是 ImageNet 2017年的冠军网络，也是 ImageNet 竞赛的收官之作。SENet 通过额外的分支(gap-fc-fc-sigm)来得到**每个通道的[0, 1]权重**，自适应地校正原各通道激活值响应。以提升有用的通道响应并抑制对当前任务用处不大的通道响应。

这其实是一种**通道注意力机制**，因为不是每个通道的信息对结果都是同等重要的。

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/SENet_fig1.png" alt="preview" style="zoom:60%;" />



### 模型结构



### 模型特点



------

## 16. SKNet

### 模型介绍

论文：

论文地址：



### 模型结构



### 模型特点




---
## 参考资料

1. 深度学习 500 问--经典网络模型

2. [CNN网络结构的发展：从LeNet到EfficientNet](https://mp.weixin.qq.com/s/ooK2aAC_TAPFmK9dPLF-Fw)

