### 采用 Apriori 算法进行关联分析

**定义**：从大规模数据集中寻找物品间的隐含关系被称作**关联分析(association analysis)，或者关联规则学习(association rule learning)**。

**难点**：这是一项非常耗时，计算代价很高，暴力搜索不能解决的问题。



#### 1. 关联分析

关联分析是指在大规模数据集中寻找有趣关系的任务，这种关系分为两种形式：

- **频繁项集：**经常出现在一块的物品的集合；

- **关联规则：**表示两个物品间可能存在很强的关系。

> 关联分析中，最有名的一个例子就是“尿布与啤酒”。据报道，美国中西部一家连锁店发现，男人们会在周四购买尿布和啤酒。



假设有以下一个简单的交易清单例子：

| 交易号码 | 商品                     |
| -------- | ------------------------ |
| 0        | 豆奶、莴苣               |
| 1        | 莴苣、尿布、葡萄酒、甜菜 |
| 2        | 豆奶、尿布、葡萄酒、橙汁 |
| 3        | 莴苣、豆奶、尿布、葡萄酒 |
| 4        | 莴苣、豆奶、尿布、橙汁   |

上述例子中，{尿布、葡萄酒、豆奶} 可以算是一个频繁项集，在 5 次交易记录中出现了两次。

如何衡量定义一个频繁项集呢？这就涉及到评价指标的问题，这里采用两个标准：

- **支持度**：**数据中包含改项集的记录所占的比例**。比如刚说的 {尿布、葡萄酒、豆奶} 出现了两次，支持度就是 2/5。因为支持度是针对项集的，可以设定一个**最小支持度**，并保留满足最小支持度的项集；

- **可信度**：可信度是针对**一条关联规则**的定义，**一条规则 P -> H的可信度定义为 `support(P|H)/support(P)`**。 比如对于 {尿布}->{葡萄酒} 这条关联规则，其可信度就应该是 "支持度({尿布、葡萄酒}) / 支持度({尿布})"，根据上述例子可以有，支持度({尿布、葡萄酒}) = 3/5, 支持度({尿布}) = 4/5，所以其可信度就是 3/4。

所以要找到一个频繁项集，首先就是需要设置一个最小支持度，然后寻找满足最小支持度的所有项集，但这也意味着需要遍历所有可能的物品组合，然后统计每种组合的支持度。

这个做法对于小数据集没有问题，但是大数据集就不适合这种做法，计算代价非常大，比如假设总共有 N 个物品的数据集，所有可能的物品组合数量是 **2^N-1，**这意味着，即便物品数量只有 100 种，存在的所有可能物品组合数量是 1.26×10^30 ！



#### 2. Apriori 原理

Apriori 算法就是可以减少关联规则学习时的计算量，其优缺点如下：

- 优点：易编码实现；

- 缺点：在大数据集上可能较慢

- 适用数据类型：数值型或标称型数据。

它的原理包括两点：

1. 如果某个项集是**频繁**的，那么它的**所有子集也是频繁**的；

1. 如果一个项集是**非频繁**的，那么它的**所有超集也是非频繁**的。

算法的一般过程如下所示：

1. 收集数据：采用任意方法；

1. 准备数据：任何数据类型都可以，只需要保存集合；

1. 分析数据：任意方法；

1. 训练算法：采用 Apriori 算法找到频繁项集；

1. 测试算法：不需要测试过程；

1. 采用算法：发现频繁项集以及物品之间的关联规则。



#### 3. 实现 Apriori 算法

关联分析的两个目标：

- 发现频繁项集

- 发现关联规则

**Apriori** 算法是发现频繁项集的一种方法。该算法只有两个输入参数--数据集和最小支持度，然后实现寻找频繁项集的步骤如下：

1. 首先生成所有**单个物品**的项集列表；
2. 接着计算每个项集的支持度，并**删除不满足最小支持度的项集**；
3. 对满足最小支持度的项集进行**组合**，得到**包含两个物品**的项集，重复步骤 2，删除不满足最小支持度的项集；
4. 重复步骤 3，直到获取所有满足条件的项集。

##### 3.1 生成候选项集



首先是创建一些辅助函数，包括生成测试集、创建包含单个物品的候选项集、计算项集支持度的函数，这几个操作的伪代码如下：

```
对数据集中的每条交易记录 tran
对每个候选集 can：
	检查一下 can 是否是 tran 的子集；
	如果是，则增加 can 的计数值
对每个候选项集：
	如果支持度小于最小支持度，删除，否则保留；
	返回所有频繁项集列表
```

具体代码实现如下所示：

**生成测试集函数：**

```python
def loadDataSet():
    return [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]
```

**创建单物品的项集**

```python
# 创建一个大小为 1 的所有候选项集的集合
def createC1(dataset):
    C1 = []
    for transaction in dataset:
        for item in transaction:
            if not [item] in C1:
                C1.append([item])
    # 排序
    C1.sort()
    # python3 使用 map 直接返回的是一个对象
    return list(map(frozenset, C1))
```

**计算项集的支持度**

```python
# 计算所有项集的支持度
def scanD(D, Ck, minSupport):
    '''
    对输入的候选项集 Ck，计算每个项集的支持度，并返回符合要求的项集和所有频繁项集
    :param D: 数据集
    :param Ck: 包括 k 个物品的候选项集
    :param minSupport: 最小支持度
    :return:
        retList: 满足最小支持度的候选项集
        supportData: 所有的频繁项集和其支持度的数据
    '''
    # ssCnt 是存储每个候选项集的支持度的字典
    ssCnt = {}
    for tid in D:
        for can in Ck:
            if can.issubset(tid):
                ssCnt.setdefault(can, 1)
                ssCnt[can] += 1
    
    numItems = float(len(D))
    retList = []
    supportData = {}
    # 计算支持度
    for key in ssCnt:
        support = ssCnt[key] / numItems
        if support >= minSupport:
            retList.insert(0, key)
        supportData[key] = support

    return retList, supportData
```

然后是一个简单的测试样例：

```python
dataSet = loadDataSet()
print('test dataset:', dataSet) 
C1 = createC1(dataSet)
print('C1:', C1)
# 构建集合表示的数据集 D
D = list(map(set, dataSet))
print('D:', D)
L1, suppData0 = scanD(D, C1, minSupport=0.5)
print('L1:', L1)
```

输出结果：

```python
test dataset: [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]
C1: [frozenset({1}), frozenset({2}), frozenset({3}), frozenset({4}), frozenset({5})]
D: [{1, 3, 4}, {2, 3, 5}, {1, 2, 3, 5}, {2, 5}]
L1: [frozenset({1}), frozenset({3}), frozenset({2}), frozenset({5}), frozenset({4})]
```

##### 3.2 组织完整的 Apriori 算法

完整的 Apriori 算法的伪代码如下：

> 当集合中项的个数大于 0 时
>
> ​	构建一个 k 个项组成的候选项集的列表
>
> ​	检查数据以确认每个项集都是频繁项集
>
> ​	保留频繁项集，并继续构建 k+1 项组成的候选项集的列表

根据上述伪代码，可以继续补充完整的代码，如下所示：

```python
def aprioriGen(Lk, k):
    '''
    生成 Ck 候选项集
    :param Lk: 频繁项集列表
    :param k: 项集元素个数
    :return: 返回指定物品数量的候选项集
    '''
    retList = []
    lenLk = len(Lk)
    for i in range(lenLk):
        for j in range(i + 1, lenLk):
            # 获取两个相邻项集的前 k-2 项
            L1 = list(Lk[i])[:k - 2]
            L2 = list(Lk[j])[:k - 2]
            L1.sort()
            L2.sort()
            if L1 == L2:
                # 如果两个集合相同，进行合并
                retList.append(Lk[i] | Lk[j])
    return retList


def apriori(dataSet, minSupport=0.5):
    '''
    Apriori 算法入口
    :param dataSet: 数据集
    :param minSupport:  最小支持度
    :return:
        L: 最终的满足最小支持度的候选项集
        supportData: 所有的频繁项集和其支持度的数据
    '''
    C1 = createC1(dataSet)
    # 创建集合表示的数据集
    D = list(map(set, dataSet))
    L1, supportData = scanD(D, C1, minSupport)
    L = [L1]
    k = 2
    while len(L[k - 2]) > 0:
        Ck = aprioriGen(L[k - 2], k)
        Lk, supK = scanD(D, Ck, minSupport)
        supportData.update(supK)
        L.append(Lk)
        # 增加候选项集的物品数量
        k += 1
    return L, supportData
```

其中 `aprioriGen()` 函数中采用 **`k-2`** 来选择两个待合并的项集，目的其实是为了减少循环的次数，不用遍历寻找非重复值。

因为此时是采用 `k-1` 个元素的项集来组合得到 `k` 个元素的新项集，那么只需要比较前面 `k-2` 个元素是否相同，如果相同，其实就是第 `k-1` 个元素不相同，那么合并两个项集，就可以得到满足条件的 `k` 个元素的新的候选项集，否则不相同的话，合并的元素个数必然大于 `k` 。

测试例子：

```python
dataSet = loadDataSet()
print('test dataset:', dataSet)

L, suppData = apriori(dataSet)
for i, val in enumerate(L):
	print('{}: {}'.format(i, val))
```

输出结果：

```
test dataset: [[1, 3, 4], [2, 3, 5], [1, 2, 3, 5], [2, 5]]
0: [frozenset({1}), frozenset({3}), frozenset({2}), frozenset({5}), frozenset({4})]
1: [frozenset({3, 5}), frozenset({1, 4}), frozenset({1, 5}), frozenset({1, 2}), frozenset({1, 3}), frozenset({2, 5}), frozenset({2, 3}), frozenset({3, 4})]
2: [frozenset({1, 2, 5}), frozenset({1, 2, 3}), frozenset({1, 3, 5}), frozenset({1, 3, 4}), frozenset({2, 3, 5})]
3: [frozenset({1, 2, 3, 5})]
4: []
```

通过上述代码，我们就可以得到所有候选的频繁项集，接下来就是挖掘关联规则。

#### 4. 从频繁项集挖掘关联规则

上一小节已经完成寻找频繁项集的工作，接下来就是挖掘关联规则，这里通过可信度来衡量是否存在关联。根据第一节的定义：

**一条规则 P -> H的可信度定义为 `support(P|H)/support(P)`**。

所以接下来只需要对频繁项集的每一个项集进行除法运算。









------

#### 参考

- 《机器学习实战》第11章