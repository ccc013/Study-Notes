

# 简介
什么是类别不平衡呢？它是指分类任务中存在某个或者某些类别的样本数量远多于其他类别的样本数量的情况。


比如，一个十分类问题，总共有 10000 个样本，但是类别 1 到 4 分别包含 2000 个样本，剩余 6 个类别的样本数量加起来刚刚 2000 个，即这六个类别各自包含的样本平均数量大约是 333 个，相比前四个类别是相差了 6 倍左右的数量。这种情况就是类别不平衡了。




# 方法


## 1.扩充数据集


首先应该考虑数据集的扩充，在刚刚图片数据集扩充一节介绍了多种数据扩充的办法，而且数据越多，给模型提供的信息也越大，更有利于训练出一个性能更好的模型。

如果在增加小类样本数量的同时，又增加了大类样本数据，可以考虑放弃部分大类数据（通过对其进行欠采样方法）。




## 2.尝试其他评价指标


一般分类任务最常使用的评价指标就是准确度了，但它在类别不平衡的分类任务中并不能反映实际情况，原因就是即便分类器将所有类别都分为大类，准确度也不会差，因为大类包含的数量远远多于小类的数量，所以这个评价指标会偏向于大类类别的数据。


其他可以推荐的评价指标有以下几种


- 混淆矩阵：实际上这个也是在分类任务会采用的一个指标，可以查看分类器对每个类别预测的情况，其对角线数值表示预测正确的数量；
- 精确度(Precision)：表示实际预测正确的结果占所有被预测正确的结果的比例，P=TP / (TP+FP)
- 召回率(Recall)：表示实际预测正确的结果占所有真正正确的结果的比例，R = TP / (TP+FN)
- F1 得分(F1 Score)：精确度和召回率的加权平均，F1=2PR / (P+R)
- Kappa (Cohen kappa)
- ROC 曲线(ROC Curves):常被用于评价一个二值分类器的优劣，而且对于正负样本分布变化的时候，ROC 曲线可以保持不变，即不受类别不平衡的影响。



其中 TP、FP、TN、FN 分别表示正确预测的正类、错误预测的正类、预测正确的负类以及错误预测的负类。图例如下：


![](https://cdn.nlark.com/yuque/0/2019/png/308996/1566915657432-56bec1a1-2c3d-44ce-b5da-cee121a552ca.png#align=left&display=inline&height=493&margin=%5Bobject%20Object%5D&originHeight=493&originWidth=986&size=0&status=done&style=none&width=986)




## 3.对数据集进行重采样


可以使用一些策略该减轻数据的不平衡程度。该策略便是采样(sampling)，主要有两种采样方法来降低数据的不平衡性。


- 对小类的数据样本进行采样来增加小类的数据样本个数，即**过采样**（over-sampling ，采样的个数大于该类样本的个数），代表算法：SMOTE和ADASYN。 

  - SMOTE：通过对训练集中的小类数据进行插值来产生额外的小类样本数据。新的少数类样本产生的策略：对每个少数类样本a，在a的最近邻中随机选一个样本b，然后在a、b之间的连线上随机选一点作为新合成的少数类样本。 	
  - ADASYN：根据学习难度的不同，对不同的少数类别的样本使用加权分布，对于难以学习的少数类的样本，产生更多的综合数据。 通过减少类不平衡引入的偏差和将分类决策边界自适应地转移到困难的样本两种手段，改善了数据分布。

- 对大类的数据样本进行采样来减少该类数据样本的个数，即**欠采样**（under-sampling，采样的次数少于该类样本的个素），缺点是欠采样的时候如果随机丢弃大类样本，可能丢失重要的信息。代表算法是 **EasyEnsemble**。其思想是利用集成学习机制，将大类划分为若干个集合供不同的学习器使用。相当于对每个学习器都进行欠采样，**但对于全局则不会丢失重要信息**。



采样算法往往很容易实现，并且其运行速度快，并且效果也不错。 一些经验法则：


- 考虑对大类下的样本（超过 1 万、十万甚至更多）进行欠采样，即删除部分样本；
- 考虑对小类下的样本（不足 1万甚至更少）进行过采样，即添加部分样本的副本；
- 考虑尝试**随机采样与非随机采样**两种采样方法；
- 考虑对各类别尝试不同的采样比例，比一定是1:1，有时候1:1反而不好，因为与现实情况相差甚远；
- 考虑同时使用过采样与欠采样。



## 4.尝试人工生成数据样本


一种简单的人工样本数据产生的方法便是，**对该类下的所有样本每个属性特征的取值空间中随机选取一个组成新的样本，即属性值随机采样**。


你可以使用**基于经验对属性值进行随机采样**而构造新的人工样本，或者使用类似**朴素贝叶斯方法**假设各属性之间互相独立进行采样，这样便可得到更多的数据，但是无法保证属性之前的线性关系（如果本身是存在的）。


有一个系统的构造人工数据样本的方法 **SMOTE**(Synthetic Minority Over-sampling Technique)。SMOTE 是一种**过采样算法**，它**构造新的小类样本**而不是产生小类中已有的样本的副本，即该算法构造的数据是新样本，原数据集中不存在的。


它基于**距离度量**选择小类别下两个或者更多的相似样本，然后选择其中一个样本，并随机选择一定数量的邻居样本，然后对选择的那个样本的**一个属性增加噪声**，每次处理一个属性。这样就构造了更多的新生数据。


python 实现的 SMOTE 算法代码地址如下，它提供了多种不同实现版本，以及多个重采样算法。

[https://github.com/scikit-learn-contrib/imbalanced-learn](https://github.com/scikit-learn-contrib/imbalanced-learn)




## 5.尝试不同分类算法


强烈建议不要对待每一个分类都使用自己喜欢而熟悉的分类算法。应该使用不同的算法对其进行比较，因为不同的算法适用于不同的任务与数据。

**决策树往往在类别不均衡数据上表现不错**。它使用基于类变量的划分规则去创建分类树，因此可以强制地将不同类别的样本分开。目前流行的决策树算法有：C4.5、C5.0、CART和Random Forest等。




## 6.尝试对模型进行惩罚


你可以使用相同的分类算法，但使用一个不同的角度，比如你的分类任务是识别那些小类，那么可以**对分类器的小类样本数据增加权值，降低大类样本的权值**（这种方法其实是产生了新的数据分布，即产生了新的数据集），从而使得分类器将重点集中在小类样本身上。


一个具体做法就是，在训练分类器时，若分类器将小类样本分错时额外增加分类器一个小类样本分错代价，这个额外的代价可以使得分类器更加“关心”小类样本。如 penalized-SVM 和 penalized-LDA 算法。

如果你锁定一个具体的算法时，并且无法通过使用重采样来解决不均衡性问题而得到较差的分类结果。这样你便可以使用**惩罚模型来解决不平衡性**问题。但是，设置惩罚矩阵是一个复杂的事，因此你需要根据你的任务尝试不同的惩罚矩阵，并选取一个较好的惩罚矩阵。




## 7.尝试一个新的角度理解问题


从一个新的角度来理解问题，比如我们可以将小类的样本作为异常点，那么问题就变成异常点检测与变化趋势检测问题。


- 异常点检测：即是对那些罕见事件进行识别。如通过机器的部件的振动识别机器故障，又如通过系统调用序列识别恶意程序。这些事件相对于正常情况是很少见的。
- 变化趋势检测：类似于异常点检测，不同在于其通过**检测不寻常的变化趋势**来识别。如通过观察用户模式或银行交易来检测用户行为的不寻常改变。



将小类样本作为异常点这种思维的转变，可以帮助考虑新的方法去分离或分类样本。这两种方法从不同的角度去思考，让你尝试新的方法去解决问题。




## 8.尝试创新


仔细对问题进行分析和挖掘，是否可以将问题划分为多个更小的问题，可以尝试如下方法：


- 将你的大类压缩成小类；
- 使用 One Class 分类器（将小类作为异常点）；
- 使用集成方式，训练多个分类器，然后联合这些分类器进行分类；



对于类别不平衡问题，还是需要具体问题具体分析，如果有先验知识可以快速挑选合适的方法来解决，否则最好就是逐一测试每一种方法，然后挑选最好的算法。最重要的还是多做项目，多积累经验，这样遇到一个新的问题，也可以快速找到合适的解决方法。



---



# 参考

- 论文：[Survey on deep learning with class imbalance](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-019-0192-5)
- [【图像分类】 关于图像分类中类别不平衡那些事](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649035421&idx=2&sn=bd5fa631a4701734be38e34603a12457&chksm=8712ace0b06525f6cb40a9a1b5341226721916c404a11f39651d3c95b7233902330a844dfe6b&token=1721275494&lang=zh_CN#rd)
- 《hands-on-ml-with-sklearn-and-tf》第二节



