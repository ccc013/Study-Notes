记录常用的一些功能函数代码，包括日志、展示结果、深度学习一些函数的实现，比如卷积层、BN 等；



# 1. 日志写法

一个日志的写法


```python
import logging
def getLog(logName, logFile):
    log = logging.getLogger(logName)
    log.setLevel(logging.INFO)
    logFormat = logging.Formatter('%(asctime)s - %(filename)s[line:%(lineno)d] - %(levelname)s: %(message)s')
    # logFile = './log/log_request.txt'
    logHandler = TimedRotatingFileHandler(logFile, when='D', interval=30, backupCount=5)
    logHandler.setFormatter(logFormat)
    log.addHandler(logHandler)
    return log
```

另一种写法：


```python
import logging
import time
from datetime import timedelta


class LogFormatter():

    def __init__(self):
        self.start_time = time.time()

    def format(self, record):
        elapsed_seconds = round(record.created - self.start_time)

        prefix = "%s - line:%d - %s - %s - %s" % (
            record.module, # 当前代码文件名字
            record.lineno, # 打印所在行数
            record.levelname, # 打印信息的级别
            time.strftime('%x %X'), # 当前时间
            timedelta(seconds=elapsed_seconds) # 代码运行所用时间
        )
        message = record.getMessage()
        message = message.replace('\n', '\n' + ' ' * (len(prefix) + 3))
        return "%s - %s" % (prefix, message)


def create_logger(filepath):
    """
    Create a logger.
    """
    # create log formatter
    log_formatter = LogFormatter()

    # create file handler and set level to debug
    if filepath is not None:
        file_handler = logging.FileHandler(filepath, "a")
        file_handler.setLevel(logging.DEBUG)
        file_handler.setFormatter(log_formatter)

    # create console handler and set level to info
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    console_handler.setFormatter(log_formatter)

    # create logger and set level to debug
    logger = logging.getLogger()
    logger.handlers = []
    logger.setLevel(logging.DEBUG)
    logger.propagate = False
    if filepath is not None:
        logger.addHandler(file_handler)
    logger.addHandler(console_handler)

    # reset logger elapsed time
    def reset_time():
        log_formatter.start_time = time.time()

    logger.reset_time = reset_time

    return logger
```


# 2. 展示结果
将结果图片用网页形式展示，包括保存结果图片和写入html 文件


```python
# save image and create filesets for append_index()
def save_images(img_infos, output_dir):
    output_image_dir = os.path.join(output_dir, "images")
    if not os.path.exists(output_image_dir):
        os.makedirs(output_image_dir)

    filesets = []
    for i, infos in enumerate(img_infos):
        fileset = dict()
        img_path, true_label, predict_label = infos

        img_name = os.path.basename(img_path)
        fileset['images'] = img_name
        _name, _ext = os.path.splitext(img_name)
        fileset['name'] = _name
        fileset['true_label'] = true_label
        fileset['predict_label'] = predict_label
        # move image
        target_img_path = os.path.join(output_image_dir, img_name)
        shutil.copyfile(img_path, target_img_path)

        filesets.append(fileset)
    return filesets


# create a html to show results
def append_index(filesets, output_dir, html_name=None, title='result'):
    if html_name is None:
        index_path = os.path.join(output_dir, "result.html")
    else:
        index_path = os.path.join(output_dir, html_name)
    if os.path.exists(index_path):
        index = open(index_path, "a")
    else:
        index = open(index_path, "w")
        index.write("""
                    <html>
                    <head>
                    <title>%s</title>
                    </head>
                    <body>""" % title)

    count = 0
    for fileset in filesets:
        # index.write("<td>%s</td>" % fileset["name"])
        index.write("<h3>%d--%s</h3>" % (count, fileset["name"]))
        index.write("<table border='1' style='table-layout: fixed;'><tr>")
        index.write("<tr>")
        for kind in ['images', 'true_label', 'predict_label']:
            index.write("<td halign='center' style='word-wrap: break-word;' valign='top'>")
            index.write("<p>%s<br>" % kind)
            index.write("</td>")
        index.write("</tr>")

        index.write("<tr>")
        for kind in ['images', 'true_label', 'predict_label']:
            index.write("<td halign='center' style='word-wrap: break-word;' valign='top'>")
            if kind == 'images':
                index.write("<p><img src='images/%s'><br>" % fileset[kind])
            else:
                index.write("<p>%s<br>" % fileset[kind])
            index.write("</td>")
        count += 1
        index.write("</tr></table>")

    return index_path
```



 

# 3. 如何解析命令行中的 bool 类型参数
参考代码 [https://github.com/jlezama/disentangling-jacobian/blob/master/conditional_image_generation/src/utils.py](https://github.com/jlezama/disentangling-jacobian/blob/master/conditional_image_generation/src/utils.py)


```python
# 定义命令
import argparse

FALSY_STRINGS = {'off', 'false', '0'}
TRUTHY_STRINGS = {'on', 'true', '1'}

def bool_flag(s):
    """
    Parse boolean arguments from the command line.
    """
    if s.lower() in FALSY_STRINGS:
        return False
    elif s.lower() in TRUTHY_STRINGS:
        return True
    else:
        raise argparse.ArgumentTypeError("invalid value for a boolean flag. use 0 or 1")


def attr_flag(s):
    """
    Parse attributes parameters.
    """
    n_cat = 1

    if s == "*":
        return s
    attr = s.split(',')
    assert len(attr) == len(set(attr))
    attributes = []
    for x in attr:
        if '.' not in x:
            attributes.append((x, n_cat))
        else:
            split = x.split('.')
            assert len(split) == 2 and len(split[0]) > 0
            assert split[1].isdigit() and int(split[1]) >= n_cat
            attributes.append((split[0], int(split[1])))
    return sorted(attributes, key=lambda x: (x[1], x[0]))

parser = argparse.ArgumentParser(description='Images autoencoder')
parser.add_argument("--attr", type=attr_flag, default="Smiling,Male",
                    help="Attributes to classify")
parser.add_argument("--instance_norm", type=bool_flag, default=False,
                    help="Use instance normalization instead of batch   normalization")
params = parser.parse_args()               
```


## 打印参数


```python
# 定义命令
import argparse

parser = argparse.ArgumentParser(description='Images autoencoder')
parser.add_argument("--attr", type=attr_flag, default="Smiling,Male",
                    help="Attributes to classify")
parser.add_argument("--instance_norm", type=bool_flag, default=False,
                    help="Use instance normalization instead of batch   normalization")
params = parser.parse_args()  

print('\n'.join('%s: %s' % (k, str(v)) for k, v
                          in sorted(dict(vars(params)).items())))
```



---

# 4. CNN 一些网络层的实现
## 4.1 卷积
参考：[python写卷积](https://zhuanlan.zhihu.com/p/146706558)
```python
import numpy as np
def conv(image, kernel, mode='same'):
    if mode == 'fill':
        h = kernel.shape[0] // 2
        w = kernel.shape[1] // 2
        image = np.pad(image, ((h, h), (w, w), (0, 0)), 'constant')
    conv_b = _convolve(image[:, :, 0], kernel)
    conv_g = _convolve(image[:, :, 1], kernel)
    conv_r = _convolve(image[:, :, 2], kernel)
    res = np.dstack([conv_b, conv_g, conv_r])
    return res
def _convolve(image, kernel):
    h_kernel, w_kernel = kernel.shape
    h_image, w_image = image.shape
    res_h = h_image - h_kernel + 1
    res_w = w_image - w_kernel + 1
    res = np.zeros((res_h, res_w), np.uint8)
    for i in range(res_h):
        for j in range(res_w):
            res[i, j] = normal(image[i:i+h_kernel, j:j+w_kernel], kernel)
    return res
def normal(image, kernel):
    res = np.multiply(image, kernel).sum()
    if res > 255:
        return 255
    else:
        return res
```
**其实也可以将卷积看成poolsize=(kernel.size(0), kernel.size(1))的pooling，只不过变成了与核函数的求和操作：**
**
```python
import numpy as np
def normal(image, kernel):
    row, col = len(kernel), len(kernel[0])
    res = 0
    for i in range(row):
        for j in range(col):
            res += image[i][j]*kernel[i][j]
    if res > 255:
        return 255
    else:
        return res
def conv(image, kernel, stride):
    h_img, w_img = len(image), len(image[0])
    h_ker, w_ker = len(kernel), len(kernel[0]) # size
    # output
    out_row, out_col = h_img // stride, w_img // stride
    row_remainder, col_remainder = h_img % stride, w_img % stride
    if row_remainder != 0:
        out_row += 1
    if col_remainder != 0:
        out_col += 1
    output = np.zeros((out_row, out_col))
    # padding
    tmp = np.lib.pad(image, ((0, h_ker-row_remainder), (0, w_ker-col_remainder)), 'edge')
    # conv
    for i in range(out_row):
        for j in range(out_col):
            x = i*stride
            y = j*stride
            output[i, j] = normal(tmp[x:x+h_ker, y:y+w_ker], kernel)
    return output
```




## 4.2 pooling
参考：[pooling的原理与Python实现](https://www.cnblogs.com/haoguoeveryone/p/haoguo_3.html)

```python
def pooling(inputMap,poolSize=3,poolStride=2,mode='max'):
    """INPUTS:
              inputMap - input array of the pooling layer
              poolSize - X-size(equivalent to Y-size) of receptive field
              poolStride - the stride size between successive pooling squares
       
       OUTPUTS:
               outputMap - output array of the pooling layer
               
       Padding mode - 'edge'
    """
    # inputMap sizes
    in_row,in_col = np.shape(inputMap)
    
    # outputMap sizes
    out_row,out_col = int(np.floor(in_row/poolStride)),int(np.floor(in_col/poolStride))
    row_remainder,col_remainder = np.mod(in_row,poolStride),np.mod(in_col,poolStride)
    if row_remainder != 0:
        out_row +=1
    if col_remainder != 0:
        out_col +=1
    outputMap = np.zeros((out_row,out_col))
    
    # padding
    temp_map = np.lib.pad(inputMap, ((0,poolSize-row_remainder),(0,poolSize-col_remainder)), 'edge')
    
    # max pooling
    for r_idx in range(0,out_row):
        for c_idx in range(0,out_col):
            startX = c_idx * poolStride
            startY = r_idx * poolStride
            poolField = temp_map[startY:startY + poolSize, startX:startX + poolSize]
            poolOut = np.max(poolField)
            #poolOut = np.mean(poolField)  # 均值池化
            outputMap[r_idx,c_idx] = poolOut
            
    
    # retrun outputMap
    return  outputMap
# 测试实例
test = np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])
test_result = pooling(test, 2, 2, 'max')
print(test_result)
```



## 4.3 归一化方法

### Batch Normalization

实现方法 1：

```python
import numpy as np

def Batchnorm(x, gamma, beta, bn_param):

    # x_shape:[B, C, H, W]
    running_mean = bn_param['running_mean']
    running_var = bn_param['running_var']
    results = 0.
    eps = 1e-5

    x_mean = np.mean(x, axis=(0, 2, 3), keepdims=True)
    x_var = np.var(x, axis=(0, 2, 3), keepdims=True0)
    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)
    results = gamma * x_normalized + beta

    # 因为在测试时是单个图片测试，这里保留训练时的均值和方差，用在后面测试时用
    running_mean = momentum * running_mean + (1 - momentum) * x_mean
    running_var = momentum * running_var + (1 - momentum) * x_var

    bn_param['running_mean'] = running_mean
    bn_param['running_var'] = running_var

    return results, bn_param
```

基于 PyTorch 实现 BN 层

```python
def batch_norm(is_training, X, gamma, beta, moving_mean, moving_var, eps, momentum):
    # 判断当前模式是训练模式还是推理模式
    if not is_training:
        # 如果是在推理模式下，直接使用传入的移动平均所得的均值和方差
        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)
    else:
        assert len(X.shape) in (2, 4)
        if len(X.shape) == 2:
            # 使用全连接层的情况，计算特征维上的均值和方差
            mean = X.mean(dim=0)
            var = ((X - mean) ** 2).mean(dim=0)
        else:
            # 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。这里我们需要保持X的形状以便后面可以做广播运算
            # torch.Tensor 高维矩阵的表示： （nSample）x C x H x W，所以对C维度外的维度求均值
            mean = X.mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)
            var = ((X - mean) ** 2).mean(dim=0, keepdim=True).mean(dim=2, keepdim=True).mean(dim=3, keepdim=True)
        # 训练模式下用当前的均值和方差做标准化
        X_hat = (X - mean) / torch.sqrt(var + eps)
        # 更新移动平均的均值和方差
        moving_mean = momentum * moving_mean + (1.0 - momentum) * mean
        moving_var = momentum * moving_var + (1.0 - momentum) * var
    Y = gamma * X_hat + beta  # 拉伸和偏移（变换重构）
    return Y, moving_mean, moving_var

class BatchNorm(nn.Module):
    def __init__(self, num_features, num_dims):　# num_features就是通道数
        super(BatchNorm, self).__init__()
        if num_dims == 2:
            shape = (1, num_features)
        else:
            shape = (1, num_features, 1, 1)
        # 参与求梯度和迭代的拉伸和偏移参数，分别初始化成0和1
        self.gamma = nn.Parameter(torch.ones(shape))
        self.beta = nn.Parameter(torch.zeros(shape))
        # 不参与求梯度和迭代的变量，全在内存上初始化成0
        self.moving_mean = torch.zeros(shape)
        self.moving_var = torch.zeros(shape)

    def forward(self, X):
        # 如果X不在内存上，将moving_mean和moving_var复制到X所在显存上
        if self.moving_mean.device != X.device:
            self.moving_mean = self.moving_mean.to(X.device)
            self.moving_var = self.moving_var.to(X.device)
        # 保存更新过的moving_mean和moving_var, Module实例的traning属性默认为true, 调用.eval()后设成false
        Y, self.moving_mean, self.moving_var = batch_norm(self.training,
            X, self.gamma, self.beta, self.moving_mean,
            self.moving_var, eps=1e-5, momentum=0.9)
        return Y
```



### Layer Normalization

实现如下：

```python
def ln(x, b, s):
    _eps = 1e-5
    output = (x - x.mean(1)[:,None]) / tensor.sqrt((x.var(1)[:,None] + _eps))
    output = s[None, :] * output + b[None,:]
    return output
```

用在四维图像上：

```python
def Layernorm(x, gamma, beta):

    # x_shape:[B, C, H, W]
    results = 0.
    eps = 1e-5

    x_mean = np.mean(x, axis=(1, 2, 3), keepdims=True)
    x_var = np.var(x, axis=(1, 2, 3), keepdims=True0)
    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)
    results = gamma * x_normalized + beta
    return results
```



### Group Normalization

代码实现：

```python
def GroupNorm(x, gamma, beta, G=16):

    # x_shape:[B, C, H, W]
    results = 0.
    eps = 1e-5
    x = np.reshape(x, (x.shape[0], G, x.shape[1]/16, x.shape[2], x.shape[3]))

    x_mean = np.mean(x, axis=(2, 3, 4), keepdims=True)
    x_var = np.var(x, axis=(2, 3, 4), keepdims=True0)
    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)
    results = gamma * x_normalized + beta
    return results
```



### Instance Normalization

代码实现：

```python
def Instancenorm(x, gamma, beta):

    # x_shape:[B, C, H, W]
    results = 0.
    eps = 1e-5

    x_mean = np.mean(x, axis=(2, 3), keepdims=True)
    x_var = np.var(x, axis=(2, 3), keepdims=True0)
    x_normalized = (x - x_mean) / np.sqrt(x_var + eps)
    results = gamma * x_normalized + beta
    return results
```



### Switchable Normalization

代码实现：

```python
def SwitchableNorm(x, gamma, beta, w_mean, w_var):
    # x_shape:[B, C, H, W]
    results = 0.
    eps = 1e-5

    mean_in = np.mean(x, axis=(2, 3), keepdims=True)
    var_in = np.var(x, axis=(2, 3), keepdims=True)

    mean_ln = np.mean(x, axis=(1, 2, 3), keepdims=True)
    var_ln = np.var(x, axis=(1, 2, 3), keepdims=True)

    mean_bn = np.mean(x, axis=(0, 2, 3), keepdims=True)
    var_bn = np.var(x, axis=(0, 2, 3), keepdims=True)

    mean = w_mean[0] * mean_in + w_mean[1] * mean_ln + w_mean[2] * mean_bn
    var = w_var[0] * var_in + w_var[1] * var_ln + w_var[2] * var_bn

    x_normalized = (x - mean) / np.sqrt(var + eps)
    results = gamma * x_normalized + beta
    return results
```







------

# 5. 目标检测

## NMS

NMS用来去掉重复的框。输入前面得到的框，对于每一类，按照score进行降序排序，最大的那个一定保留，然后和其他的框计算IOU。IOU大于一定阈值视为重复的框，丢弃掉。

```python
import numpy as np
def nms(dets, thresh):
    x1 = dets[:, 0] # bbox top_x
    y1 = dets[:, 1] # bbox top_y
    x2 = dets[:, 2] # bbox bottom_x
    y2 = dets[:, 3] # bbox bottom_y
    scores = dets[:, 4] # 分类score
    areas = (x2 - x1 + 1) * (y2 - y1 + 1)
    order = scores.argsort()[::-1] # 按score做降序排序
    keep = []
    while order.size > 0:
        i = order[0]
        keep.append(i)
        xx1 = np.maximum(x1[i], x1[order[1:]])
        yy1 = np.maximum(y1[i], y1[order[1:]])
        xx2 = np.minimum(x2[i], x2[order[1:]])
        yy2 = np.minimum(y2[i], y2[order[1:]])
        w = np.maximum(0.0, xx2 - xx1 + 1)
        h = np.maximum(0.0, yy2 - yy1 + 1)
        intersection = w * h
        iou = intersection / (areas[i] + areas[order[1:]] - intersection)
        inds = np.where(iou <= thresh)[0]
        order = order[inds + 1] # 加1的原因。假设一个类别有n个框，这里的索引是n-1个iou是对应的，
                                # 这里要映射到原来长度为n的order，所以加1。
    return keep
```





------

# 6. 机器学习

## Kmeans 聚类

下面是简易版本的实现。

注意，`np.random.randint()` 是取值范围是左闭右开区间，python 自带的 `random.randint()` 是闭区间。

```python
import numpy as np
import random

def kmeans(data, k):
    m = len(data)     # 样本个数
    n = len(data[0])  # 维度
    cluster_center = np.zeros((k, n))   # k个聚类中心

    # 选择合适的初始聚类中心
    # 在已有数据中随机选择聚类中心
    # 也可以直接用随机的聚类中心

    init_list = np.random.randint(low=0, high=m, size=k)    # [0, m)
    for index, j in enumerate(init_list):
        cluster_center[index] = data[j][:]

    # 聚类
    cluster = np.zeros(m, dtype=np.int) - 1 # 所有样本尚未聚类
    cc = np.zeros((k, n))   # 下一轮的聚类中心
    c_number = np.zeros(k)    # 每个簇样本的数目

    for times in range(1000):
        for i in range(m):
            c = nearst(data[i], cluster_center)
            cluster[i] = c  # 第i个样本归于第c簇
            c_number[c] += 1
            cc[c] += data[i]
        for i in range(k):
            cluster_center[i] = cc[i] / c_number[i]
        cc.flat = 0
        c_number.flat = 0
    return cluster

def nearst(data, cluster_center):
    nearst_center_index = 0
    dis = np.sum((cluster_center[0] - data) ** 2)
    for index, center in enumerate(cluster_center):
        dis_temp = np.sum((center - data) ** 2)
        if dis_temp < dis:
            nearst_center_index = index
            dis = dis_temp
    return nearst_center_index

if __name__ == "__main__":
    data = [[0,0], [1,0], [0,1], [100,100], [101,101], [102, 100], [-100,-100], [-101,-101], [-102, -100]]
    data = np.array(data)
    cluster = kmeans(data, 3)
    print(cluster)
    # [0 0 0 1 1 1 2 2 2] 每个样本对应的类别，结果有随机性
```



## Softmax



```python
import numpy as np

def convert_label_to_onehot(classes, labels):
    """
    classes: 类别数
    labels: array,shape=(N,)
    """
    return np.eye(classes)[labels].T

def softmax(logits):
    """logits: array, shape=(N, C)"""
    res = np.zeros_like(logits)
    for i, row in enumerate(logits):
        exp_sum = np.sum(np.exp(row))
        for j, val in enumerate(row):
            res[i,j] = np.exp(val)/ exp_sum
    return res

if __name__ == '__main__':
    # 有四个样本，有三个类别
    logits = np.array([[0, 0.45, 0.55],
                       [0.9, 0.05, 0.05],
                       [0.4, 0.6, 0],
                       [1, 0, 0]])
    pred = np.argmax(softmax(logits), axis=1)
    print(pred)
```



## PR曲线和 ROC 曲线的绘制

这两种曲线是评价分类模型performance的常用方法。其中，PR图的纵坐标是precision，横坐标是recall；ROC曲线的纵坐标是True Positive Rate，横坐标是False Positive Rate，它们的公式如下：
$$
precision = \frac{TP}{TP+FP}\\
recall = \frac{TP}{TP+FN}\\
TPR = \frac{TP}{TP+FN}\\
FPR=\frac{FP}{TN+FP}
$$


```python
import numpy as np
import matplotlib.pyplot as plt
data_number = 50
labels = np.random.randint(0, 2, size=data_number)  # 真实的类别
scores = np.random.choice(np.arange(0.1, 1, 0.01), data_number)  # 模型判断样本为类别1的置信度


def pr_curve(y, pred):
    pos = np.sum(y == 1)
    neg = np.sum(y == 0)
    pred_sort = np.sort(pred)[::-1]  
    index = np.argsort(pred)[::-1]  
    y_sort = y[index]
    print(y_sort)

    pre = []
    rec = []
    for i, item in enumerate(pred_sort):
        if i == 0:  
            pre.append(1)
            rec.append(0)
        else:
            pre.append(np.sum((y_sort[:i] == 1)) / i)
            rec.append(np.sum((y_sort[:i] == 1)) / pos)

    # 画图
    plt.plot(rec, pre, 'k')
    # plt.legend(loc='lower right')
    plt.title('PR curve')
    plt.plot([(0, 0), (1, 1)], 'r--')
    plt.xlim([-0.01, 1.01])
    plt.ylim([-0.01, 01.01])
    plt.ylabel('precision')
    plt.xlabel('recall')
    plt.show()


def roc_curve(y, pred):
    pos = np.sum(y == 1)
    neg = np.sum(y == 0)
    pred_sort = np.sort(pred)[::-1]  
    index = np.argsort(pred)[::-1]  
    y_sort = y[index]
    print(y_sort)
    tpr = []
    fpr = []
    thr = []
    for i, item in enumerate(pred_sort):
        tpr.append(np.sum((y_sort[:i] == 1)) / pos)
        fpr.append(np.sum((y_sort[:i] == 0)) / neg)
        thr.append(item)

    # 画图
    plt.plot(fpr, tpr, 'k')
    plt.title('ROC curve')
    plt.plot([(0, 0), (1, 1)], 'r--')
    plt.xlim([-0.01, 1.01])
    plt.ylim([-0.01, 01.01])
    plt.ylabel('True Positive Rate')
    plt.xlabel('False Positive Rate')
    plt.show()


pr_curve(labels, scores)
roc_curve(labels, scores)
```



## 数据分析

### 探索性数据分析

参考：

- [使用pandas_profiling用2行代码完成探索性数据分析](https://mp.weixin.qq.com/s/qRje-oIh_XM67QMrToYslA)



**pandas_profiling** 是可以将pandas数据框快速转化为描述性数据分析报告的package，一行代码即可生成内容丰富的EDA内容，两行代码即可将报告以`.html`格式保存



常规的对数据做 EDA 的做法如下所示：

```python
import numpy as np
import pandas as pd
adult = pd.read_csv('../adult.data')
# 查看前 5 行数据的信息
adult.head()

# 对数据进行统计描述，展示数据的均值、方差、最大值最小值等信息
adult.describe()

# 查看变量信息和缺失情况
adult.info()
```

这是最简单最快速了解一个数据集的方法。当然，更深层次的EDA一定是要借助统计图形来展示的。基于scipy、matplotlib和seaborn等工具的展示这里权且略过。



现在我们有了pandas_profiling。上述过程以及各种统计相关性计算、统计绘图全部由pandas_profiling打包搞定了。pandas_profiling安装，包括pip、conda和源码三种安装方式。

pip：

```
pip install pandas-profilingpip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip
```

conda：

```
conda install -c conda-forge pandas-profiling
```

source：

先下载源码文件，然后解压到setup.py所在的文件目录下：

```
python setup.py install
```

再来看pandas_profiling基本用法，用pandas将数据读入之后，对数据框直接调用profile_report方法生成EDA分析报告，然后使用to_file方法另存为.html文件。

```
profile = df.profile_report(title="Census Dataset")profile.to_file(output_file=Path("./census_report.html"))
```

看看报告效果如何。pandas-profiling EDA报告包括数据整体概览、变量探索、相关性计算、缺失值情况和抽样展示等5个方面。





------

# 参考

1. [【基础算法】常见的ML、DL编程题](https://mp.weixin.qq.com/s/jHS2Vl3Msn7t94_YsrfrKA)

