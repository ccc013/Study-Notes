在机器学习领域中，对模型的评估非常重要，只有选择和问题相匹配的评估方法，才能快速发现算法模型或者训练过程的问题，迭代地对模型进行优化。

模型评估主要分为离线评估和在线评估两个阶段。并且针对分类、回归、排序、序列预测等不同类型的机器学习问题，评估指标的选择也有所不同。

模型评估这部分会介绍以下几方面的内容：

- 性能度量
- 模型评估方法
- 泛化能力
- 过拟合、欠拟合
- 超参数调优



### 1. 性能度量

性能度量就是指对模型泛化能力衡量的评价标准。

#### 1.1 准确率和错误率

分类问题中最常用的两个性能度量标准--准确率和错误率。

**准确率**：指的是分类正确的样本数量占样本总数的比例，定义如下：
$$
Accuracy = \frac{n_{correct}}{N}
$$
**错误率**：指分类错误的样本占样本总数的比例，定义如下：
$$
Error = \frac{n_{error}}{N}
$$
错误率也是损失函数为 0-1 损失时的误差。

这两种评价标准是分类问题中最简单也是最直观的评价指标。但它们都存在一个问题，在类别不平衡的情况下，它们都无法有效评价模型的泛化能力。即如果此时有 99% 的负样本，那么模型预测所有样本都是负样本的时候，可以得到 99% 的准确率。

这种情况就是**在类别不平衡的时候，占比大的类别往往成为影响准确率的最主要因素**！

这种时候，其中一种解决方法就是更换评价指标，比如采用更为有效的平均准确率(**每个类别的样本准确率的算术平均**)，即：
$$
A_{mean}=\frac{a_1+a_2+\dots+a_m}{m}
$$
其中 m 是类别的数量。

对于准确率和错误率，用 Python 代码实现如下图所示：

```python
def accuracy(y_true, y_pred):
    return sum(y == y_p for y, y_p in zip(y_true, y_pred)) / len(y_true)

def error(y_true, y_pred):
    return sum(y != y_p for y, y_p in zip(y_true, y_pred)) / len(y_true)
```

一个简单的二分类测试样例：

```python
y_true = [1, 0, 1, 0, 1]
y_pred = [0, 0, 1, 1, 0]

acc = accuracy(y_true, y_pred)
err = error(y_true, y_pred)
print('accuracy=', acc)
print('error=', err)
```

输出结果如下：

```
accuracy= 0.4
error= 0.6
```



#### 1.2 精确率、召回率、P-R 曲线和 F1

##### 1.2.1 精确率和召回率

精确率，也被称作查准率，是指**所有预测为正类的结果中，真正的正类的比例**。公式如下：
$$
P = \frac{TP}{TP+FP}
$$
召回率，也被称作查全率，是指所有正类中，被分类器找出来的比例。公式如下：
$$
R = \frac{TP}{TP+FN}
$$
对于上述两个公式的符号定义，是在二分类问题中，我们将关注的类别作为正类，其他类别作为负类别，因此，定义：

- `TP(True Positive)`：真正正类的数量，即分类为正类，实际也是正类的样本数量；
- `FP`(False Positive)：假正类的数量，即分类为正类，但实际是负类的样本数量；
- `FN(False Negative)`：假负类的数量，即分类为负类，但实际是正类的样本数量；
- `TN(True Negative)`：真负类的数量，即分类是负类，实际也负类的样本数量。

更形象的说明，可以参考下表，也是**混淆矩阵**的定义：

|            | 预测：正类 | 预测：负类 |
| :--------: | :--------: | :--------: |
| 实际：正类 |     TP     |     FN     |
| 实际：负类 |     FP     |     TN     |

精确率和召回率是一对矛盾的度量，通常精确率高时，召回率往往会比较低；而召回率高时，精确率则会比较低，原因如下：

- 精确率越高，代表预测为正类的比例更高，而要做到这点，通常就是**只选择有把握的样本**。最简单的就是只挑选最有把握的一个样本，此时 `FP=0`，`P=1`，但 `FN` 必然非常大(没把握的都判定为负类)，召回率就非常低了；
- 召回率要高，就是需要找到所有正类出来，要做到这点，最简单的就是**所有类别都判定为正类**，那么 `FN=0` ，但 `FP` 也很大，所有精确率就很低了。

而且不同的问题，侧重的评价指标也不同，比如：

- **对于推荐系统，侧重的是精确率**。也就是希望推荐的结果都是用户感兴趣的结果，即用户感兴趣的信息比例要高，因为通常给用户展示的窗口有限，一般只能展示 5 个，或者 10 个，所以更要求推荐给用户真正感兴趣的信息；
- **对于医学诊断系统，侧重的是召回率**。即希望不漏检任何疾病患者，如果漏检了，就可能耽搁患者治疗，导致病情恶化。

精确率和召回率的代码简单实现如下，这是基于二分类的情况

```python
def precision(y_true, y_pred):
    true_positive = sum(y and y_p for y, y_p in zip(y_true, y_pred))
    predicted_positive = sum(y_pred)
    return true_positive / predicted_positive
def recall(y_true, y_pred):
    true_positive = sum(y and y_p for y, y_p in zip(y_true, y_pred))
    real_positive = sum(y_true)
    return true_positive / real_positive
```

简单的测试样例以及输出如下

```python
y_true = [1, 0, 1, 0, 1]
y_pred = [0, 0, 1, 1, 0]

precisions = precision(y_true, y_pred)
recalls = recall(y_true, y_pred)

print('precisions=', precisions) # 输出为0.5
print('recalls=', recalls)       # 输出为 0.3333
```



##### 1.2.2 P-R 曲线和 F1

很多时候，我们都可以根据分类器的预测结果对样本进行排序，越靠前的是分类器越有把握是正类的样本，而最后面的自然就是分类器觉得最不可能是正类的样本了。

一般来说，这个预测结果其实就是分类器对样本判断为某个类别的置信度，我们可以选择不同的阈值来调整分类器对某个样本的输出结果，比如设置阈值是 0.9，那么只有置信度是大于等于 0.9 的样本才会最终判定为正类，其余的都是负类。

我们设置不同的阈值，自然就会得到不同的正类数量和负类数量，依次计算不同情况的精确率和召回率，然后我们可以**以精确率为纵轴，召回率为横轴，绘制一条“P-R曲线”**，如下图所示：

![来自西瓜书](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/P-R_plot.png)

当然，以上这个曲线是比较理想情况下的，未来绘图方便和美观，实际情况如下图所示：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/P_R.png)

对于 P-R 曲线，有：

1.**曲线从左上角 `(0,1)` 到右下角 `(1,0)` 的走势，正好反映了精确率和召回率是一对矛盾的度量**，一个高另一个低的特点：

- **开始是精确率高**，因为设置阈值很高，只有第一个样本（分类器最有把握是正类）被预测为正类，其他都是负类，所以精确率高，几乎是 1，而召回率几乎是 0，仅仅找到 1 个正类。
- **右下角时候就是召回率很高，精确率很低**，此时设置阈值就是 0，所以类别都被预测为正类，所有正类都被找到了，召回率很高，而精确率非常低，因为大量负类被预测为正类。

2.`P-R` 曲线可以非常直观显示出分类器在样本总体上的精确率和召回率。所以可以对比两个分类器在同个测试集上的 `P-R` 曲线来比较它们的分类能力：

- 如果分类器 `B` 的 `P-R` 曲线被分类器 `A` 的曲线完全包住，如下左图所示，则可以说，`A` 的性能优于 `B`;
- 如果是下面的右图，两者的曲线有交叉，则很难直接判断两个分类器的优劣，只能根据具体的精确率和召回率进行比较：
  - 一个合理的依据是**比较 `P-R` 曲线下方的面积大小**，它在一定程度上表征了分类器在精确率和召回率上取得“双高”的比例，但这个数值不容易计算；
  - 另一个比较就是**平衡点**(Break-Event Point, BEP)，它是**精确率等于召回率时的取值**，如下右图所示，而且可以判定，**平衡点较远的曲线更好**。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/P_R_AB.JPG)

当然了，平衡点还是过于简化，于是有了 **F1 值**这个新的评价标准，它是**精确率和召回率的调和平均值**，定义为：
$$
F1 = \frac{2 \times P \times R}{P+R}=\frac{2\times TP}{样本总数+TP-TN}
$$
F1 还有一个更一般的形式：$F_{\beta}$，能让我们表达出对精确率和召回率的不同偏好，定义如下：
$$
F_{\beta}=\frac{(1+\beta^2)\times P\times R}{(\beta^2 \times P)+R}
$$
其中$\beta > 0$ 度量了召回率对精确率的相对重要性，当 $\beta = 1$，就是 F1；如果 $\beta > 1$，召回率更加重要；如果 $\beta < 1$，则是精确率更加重要。

##### 1.2.3 宏精确率/微精确率、宏召回率/微召回率以及宏 F1 / 微 F1

很多时候，我们会得到不止一个二分类的混淆矩阵，比如多次训练/测试得到多个混淆矩阵，在多个数据集上进行训练/测试来估计算法的“全局”性能，或者是执行多分类任务时对类别两两组合得到多个混淆矩阵。

总之，我们希望在 n 个二分类混淆矩阵上综合考察精确率和召回率。这里一般有两种方法来进行考察：

1.第一种是直接在**各个混淆矩阵上分别计算出精确率和召回率**，记为 $(P_1, R_1), (P_2, R_2), \cdots, (P_n, R_n)$，接着**计算平均值**，就得到宏精确率(macro-P)、宏召回率(macro-R)以及宏 F1(macro-F1) , 定义如下：
$$
macro-P = \frac{1}{n}\sum_{i=1}^n P_i,\\
macro-R = \frac{1}{n}\sum_{i=1}^n R_i,\\
macro-F1 = \frac{2\times macro-P\times macro-R}{marco-P+macro-R}
$$
2.第二种则是**对每个混淆矩阵的对应元素进行平均**，**得到 TP、FP、TN、FN 的平均值**，再基于这些平均值就就得到微精确率(micro-P)、微召回率(micro-R)以及微 F1(micro-F1) , 定义如下：
$$
micro-P = \frac{\overline{TP}}{\overline{TP}+\overline{FP}},\\
micro-R = \frac{\overline{TP}}{\overline{TP}+\overline{FN}},\\
micro-F1 = \frac{2\times micro-P\times micro-R}{micro-P + micro-R}
$$

#### 1.3 ROC 与 AUC

##### 1.3.1 ROC 曲线

ROC 曲线的 Receiver Operating Characteristic 曲线的简称，中文名是“受试者工作特征”，起源于军事领域，后广泛应用于医学领域。

它的横坐标是**假正例率(False Positive Rate, FPR)**，纵坐标是**真正例率(True Positive Rate, TPR)**，两者的定义分别如下：
$$
TPR = \frac{TP}{TP+FN},\\
FPR = \frac{FP}{FP+TN}
$$
TPR 表示**正类中被分类器预测为正类的概率**，刚好就等于正类的召回率；

FPR 表示**负类中被分类器预测为正类的概率**，它等于 1 减去负类的召回率，负类的召回率如下，**称为真反例率(True Negative Rate, TNR)**, 也被称为特异性，表示负类被正确分类的比例。
$$
TNR =\frac{TN}{FP+TN}
$$

跟 P-R 曲线的绘制一样，ROC 曲线其实也是通过**不断调整区分正负类结果的阈值**来绘制得到的，它的纵轴是 TPR，横轴是 FPR，这里借鉴《百面机器学习》上的示例来介绍，首先有下图所示的表格，表格是一个二分类模型的输出结果样例，包含 20 个样本，然后有对应的真实标签，其中 p 表示是正类别，而 n 表示是负类别。然后模型输出概率表示模型对判断该样本是正类的置信度。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/roc_table.jpg)

最开始如果设置阈值是无穷大的时候，那么模型会将所有样本判断为负类，TP 和 FP 都会是 0，也就是 TPR 和 FPR 必然也是 0，ROC 曲线的第一个坐标就是 (0, 0)。接着，阈值设置为 0.9，此时样本序号为 1 的样本会被判断为正样本，并且它确实是正样本，那么 TP = 1，而正类样本的个数是有 10 个，所有 TPR = 0.1；然后没有预测错误的正类，即 FP = 0，FPR = 0，这个时候曲线的第二个坐标就是 (0, 0.1)。

通过不断调整阈值，就可以得到曲线的不同坐标，最终得到下图所示的 ROC 曲线。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/roc_2.jpg)

第二种更直观地绘制 ROC 曲线的方法，首先统计出正负样本的数量，假设分别是 P 和 N，接着，将横轴的刻度间隔设置为 1/N，纵轴的刻度间隔设置为 1/P。然后根据模型输出的概率对样本排序，并按顺序遍历样本，从零点开始绘制 ROC 曲线，**每次遇到一个正样本就沿纵轴方向绘制一个刻度间隔的曲线**，**遇到一个负样本就沿横轴绘制一个刻度间隔的曲线**，直到遍历完所有样本，曲线最终停留在 (1,1) 这个点，此时就完成了 ROC 曲线的绘制了。

当然，更一般的 ROC 曲线是如下图所示的，会更加的平滑，上图是由于样本数量有限才导致的。

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/ROC.png)

对于 ROC 曲线，有以下几点特性：

1.ROC 曲线通常都是从左下角 (0,0) 开始，到右上角 (1,1) 结束。

- 开始时候，**第一个样本被预测为正类**，其他都是预测为负类别；
  - TPR 会很低，几乎是 0，上述例子就是 0.1，此时大量正类没有被分类器找出来；
  - FPR 也很低，可能就是0，上述例子就是 0，这时候被预测为正类的样本可能实际也是正类，所以几乎没有预测错误的正类样本。
- 结束时候，**所有样本都预测为正类**。
  - TPR 几乎就是 1，因为所有样本都预测为正类，那肯定就找出所有的正类样本了；
  - FPR 也是几乎为 1，因为所有负样本都被错误判断为正类。

2.ROC 曲线中：

- **对角线对应于随机猜想模型**，即概率为 0.5；
- **点 `(0,1)` 是理想模型**，因为此时 `TPR=1`，`FPR=0`，也就是正类都预测出来，并且没有预测错误；
- 通常，**ROC 曲线越接近点 `(0, 1)` 越好。**

3.同样可以根据 ROC 曲线来判断两个分类器的性能：

- 如果**分类器 `A` 的 `ROC` 曲线被分类器 `B` 的曲线完全包住，可以说 `B` 的性能好过 `A**`，这对应于上一条说的 ROC 曲线越接近点 `(0, 1)` 越好；
- 如果两个分类器的 `ROC` 曲线发生了交叉，则同样很难直接判断两者的性能优劣，需要借助 `ROC` 曲线下面积大小来做判断，而这个面积被称为 **`AUC:Area Under ROC Curve`**。

简单的代码实现如下：

```python
def true_negative_rate(y_true, y_pred):
    true_negative = sum(1 - (yi or yi_hat) for yi, yi_hat in zip(y_true, y_pred))
    actual_negative = len(y_true) - sum(y_true)
    return true_negative / actual_negative


def roc(y, y_hat_prob):
    thresholds = sorted(set(y_hat_prob), reverse=True)
    ret = [[0, 0]]
    for threshold in thresholds:
        y_hat = [int(yi_hat_prob >= threshold) for yi_hat_prob in y_hat_prob]
        ret.append([recall(y, y_hat), 1 - true_negative_rate(y, y_hat)])
    return ret
```

简单的测试例子如下：

```python
y_true = [1, 0, 1, 0, 1]
y_hat_prob = [0.9, 0.85, 0.8, 0.7, 0.6]

roc_list = roc(y_true, y_hat_prob)
print('roc_list:', roc_list)
# 输出结果是 roc_list: [[0, 0], [0.3333333333333333, 0.0], [0.3333333333333333, 0.5], [0.6666666666666666, 0.5], [0.6666666666666666, 1.0], [1.0, 1.0]]
```



##### 1.3.2 **ROC 和 P-R 曲线的对比**

**相同点**

1.**两者刻画的都是阈值的选择对分类度量指标的影响**。虽然每个分类器对每个样本都会输出一个概率，也就是置信度，但通常我们都会人为设置一个阈值来影响分类器最终判断的结果，比如设置一个很高的阈值--0.95，或者比较低的阈值--0.3。

- **如果是偏向于精确率，则提高阈值**，保证只把有把握的样本判断为正类，此时可以设置阈值为 0.9，或者更高；
- **如果偏向于召回率，那么降低阈值**，保证将更多的样本判断为正类，更容易找出所有真正的正样本，此时设置阈值是 0.5，或者更低。

2.两个曲线的每个点都是**对应某个阈值的选择，该点是在该阈值下的 `(精确率，召回率)` / `(TPR, FPR)**`。然后沿着横轴方向对应阈值的下降。

**不同**

相比较 `P-R` 曲线，`ROC` 曲线有一个特点，就是**正负样本的分布发生变化时，它的曲线形状能够基本保持不变**。如下图所示:

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/ROC_PR_compare.jpg)

分别比较了增加十倍的负样本后， `P-R` 和 `ROC` 曲线的变化，可以看到 `ROC` 曲线的形状基本不变，但 `P-R` 曲线发生了明显的变化。

所以 `ROC` 曲线的这个特点**可以降低不同测试集带来的干扰**，**更加客观地评估模型本身的性能**，因此它适用的场景更多，比如排序、推荐、广告等领域。

这也是由于**现实场景中很多问题都会存在正负样本数量不平衡**的情况，比如计算广告领域经常涉及转化率模型，正样本的数量往往是负样本数量的千分之一甚至万分之一，这时候选择 `ROC` 曲线更加考验反映模型本身的好坏。

当然，如果希望看到模型在特定数据集上的表现，`P-R` 曲线会更直观地反映其性能。所以还是需要具体问题具体分析。

##### 1.3.3 AUC 曲线

`AUC` 是 `ROC` 曲线的面积，其物理意义是：从所有正样本中随机挑选一个样本，模型将其预测为正样本的概率是 $p_1$；从所有负样本中随机挑选一个样本，模型将其预测为正样本的概率是 $p_0$。**$p_1 > p_0$ 的概率就是 `AUC**`。

`AUC` 曲线有以下几个特点：

- 如果完全随机地对样本进行分类，那么 $p_1 > p_0$ 的概率是 0.5，则 `AUC=0.5`；

- **`AUC` 在样本不平衡的条件下依然适用**。

  如：在反欺诈场景下，假设正常用户为正类（设占比 99.9%），欺诈用户为负类（设占比 0.1%）。

  如果使用准确率评估，则将所有用户预测为正类即可获得 99.9%的准确率。很明显这并不是一个很好的预测结果，因为欺诈用户全部未能找出。

  如果使用 `AUC` 评估，则此时 `FPR=1,TPR=1`，对应的 `AUC=0.5` 。因此 `AUC` 成功的指出了这并不是一个很好的预测结果。

- `AUC` 反应的是**模型对于样本的排序能力**（根据样本预测为正类的概率来排序）。如：`AUC=0.8` 表示：给定一个正样本和一个负样本，在 `80%` 的情况下，模型对正样本预测为正类的概率大于对负样本预测为正类的概率。

- **`AUC` 对于均匀采样不敏感**。如：上述反欺诈场景中，假设对正常用户进行均匀的降采样。任意给定一个负样本 n，设模型对其预测为正类的概率为 Pn 。降采样前后，由于是均匀采样，因此预测为正类的概率大于 Pn 和小于 Pn  的真正样本的比例没有发生变化。因此 `AUC` 保持不变。

  但是如果是非均匀的降采样，则预测为正类的概率大于 Pn  和小于 Pn 的真正样本的比例会发生变化，这也会导致 `AUC` 发生变化。

- **正负样本之间的预测为正类概率之间的差距越大，则 `AUC` 越高**。因为这表明正负样本之间排序的把握越大，区分度越高。

  如：在电商场景中，点击率模型的 `AUC` 要低于购买转化模型的 `AUC` 。因为点击行为的成本低于购买行为的成本，所以点击率模型中正负样本的差别要小于购买转化模型中正负样本的差别。

`AUC` 的计算可以通过对 `ROC` 曲线下各部分的面积求和而得。假设 `ROC` 曲线是由坐标为下列这些点按顺序连接而成的： 
$$
{(x_1,y_1),(x_2,y_2),\cdots,(x_m,y_m)}, 其中\ x_1=0, x_m=1
$$
那么 `AUC` 可以这样估算：
$$
AUC = \frac{1}{2}\sum_{i=1}^{m-1}(x_{i+1}-x_i)\times (y_i+y_{i+1})
$$

代码实现如下：

```python
def get_auc(y, y_hat_prob):
    roc_val = iter(roc(y, y_hat_prob))
    tpr_pre, fpr_pre = next(roc_val)
    auc = 0
    for tpr, fpr in roc_val:
        auc += (tpr + tpr_pre) * (fpr - fpr_pre) / 2
        tpr_pre = tpr
        fpr_pre = fpr
    return auc
```

简单的测试样例如下：

```python
y_true = [1, 0, 1, 0, 1]
y_hat_prob = [0.9, 0.85, 0.8, 0.7, 0.6]

auc_val = get_auc(y_true, y_hat_prob)
print('auc_val:', auc_val) # 输出是 0.5
```

#### 1.4 代价矩阵

前面介绍的性能指标都有一个隐式的前提，错误都是**均等代价**。但实际应用过程中，不同类型的错误所造成的后果是不同的。比如将健康人判断为患者，与患者被判断为健康人，代价肯定是不一样的，前者可能就是需要再次进行检查，而后者可能错过治疗的最佳时机。

因此，为了衡量不同类型所造成的不同损失，可以为错误赋予**非均等代价(unequal cost)**。

对于一个二类分类问题，可以设定一个**代价矩阵(cost matrix)**，其中 $cost_{ij}$ 表示将第 `i` 类样本预测为第 `j` 类样本的代价，而预测正确的代价是 0 。如下表所示：

|                | 预测：第 0 类 | 预测：第 1 类 |
| :------------: | :-----------: | :-----------: |
| 真实：第 0 类  |       0       |  $cost_{01}$  |
| 真实： 第 1 类 |  $cost_{10}$  |       0       |

1. 在非均等代价下，希望找到的不再是简单地最小化错误率的模型，而是希望找到**最小化总体代价 `total cost` 的模型**。

2. 在非均等代价下，`ROC` 曲线不能直接反映出分类器的期望总体代价，此时需要使用代价曲线 `cost curve`

   - 代价曲线的横轴是**正例概率代价**，如下所示，其中 p 是正例(第 0 类)的概率

   $$
   P_{+cost} = \frac{p\times cost_{01}}{p\times cost_{01}+(1-p)\times cost_{10}}
   $$





   - 代价曲线的纵轴是归一化代价，如下所示：
     $$
     cost_{norm} = \frac{FNR\times p\times cost_{01}+FPR\times (1-p)\times cost_{10}}{p\times cost_{01}+(1-p)\times cost_{10}}
     $$
     其中，假正例率 `FPR` 表示模型将负样本预测为正类的概率，定义如下：
     $$
     FPR = \frac{FP}{TN+FP}
     $$
     假负例率 `FNR` 表示将正样本预测为负类的概率，定义如下：
     $$
     FNR = 1 - TPR = \frac{FN}{TP+FN}
     $$
     代价曲线如下图所示：

     ![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/cost_curve.png)



#### 1.5 回归问题的性能度量

对于回归问题，常用的性能度量标准有：

1.均方误差(Mean Square Error, MSE)，定义如下：
$$
MSE=\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y_i})^2
$$
2.均方根误差(Root Mean Squared Error, RMSE)，定义如下：
$$
RMSE = \sqrt{\frac{1}{N}\sum_{i=1}^N(y_i-\hat{y_i})^2}
$$
3.均方根对数误差(Root Mean Squared Logarithmic Error, RMSLE)，定义如下
$$
RMSLE=\sqrt{\frac{1}{N}\sum_{i=1}^N[log(y_i+1)- log(\hat{y_i}+1)]^2}
$$
4.平均绝对误差(Mean Absolute Error, MAE)，定义如下：
$$
MAE = \frac{1}{N}\sum_{i=1}^N |y_i-\hat{y_i}|
$$
这四个标准中，比较常用的第一个和第二个，即 `MSE` 和 `RMSE`，这两个标准一般都可以很好反映回归模型预测值和真实值的偏离程度，但如果遇到**个别偏离程度非常大的离群点**时，即便数量很少，也会让这两个指标变得很差。

遇到这种情况，有三种解决思路：

- 将离群点作为噪声点来处理，即数据预处理部分需要过滤掉这些噪声点；
- 从模型性能入手，提高模型的预测能力，将这些离群点产生的机制建模到模型中，但这个方法会比较困难；
- 采用其他指标，比如第三个指标 `RMSLE`，它关注的是预测误差的比例，即便存在离群点，也可以降低这些离群点的影响；或者是 `MAPE`，平均绝对百分比误差(Mean Absolute Percent Error)，定义为：

$$
MAPE = \sum_{i=1}^n |\frac{y_i-\hat{y_i}}{y_i}|\times\frac{100}{n}
$$

`RMSE` 的简单代码实现如下所示：

```python
def rmse(predictions, targets):
    # 真实值和预测值的误差
    differences = predictions - targets
    differences_squared = differences ** 2
    mean_of_differences_squared = differences_squared.mean()
    # 取平方根
    rmse_val = np.sqrt(mean_of_differences_squared)
    return rmse_val
```






#### 1.6 其他评价指标

1. 计算速度：模型训练和预测需要的时间；
2. 鲁棒性：处理缺失值和异常值的能力；
3. 可拓展性：处理大数据集的能力；
4. 可解释性：模型预测标准的可理解性，比如决策树产生的规则就很容易理解，而神经网络被称为黑盒子的原因就是它的大量参数并不好理解。


### 2. 模型评估的方法

#### 2.1 泛化能力

1. **泛化能力**：指模型对**未知的、新鲜的数据的预测能力**，通常是根据**测试误差**来衡量模型的泛化能力，测试误差越小，模型能力越强；
2. 统计理论表明：如果训练集和测试集中的样本都是独立同分布产生的，则有 **模型的训练误差的期望等于模型的测试误差的期望** 。
3. 机器学习的“没有免费的午餐定理”表明：在所有可能的数据生成分布上，没有一个机器学习算法总是比其他的要好。
   - 该结论仅在考虑所有可能的数据分布时才成立。
   - 现实中特定任务的数据分布往往满足某类假设，从而可以设计在这类分布上效果更好的学习算法。
   - 这意味着机器学习并不需要寻找一个通用的学习算法，而是寻找一个在关心的数据分布上效果最好的算法。
4. 正则化是对学习算法做的一个修改，这种修改趋向于降低泛化误差（而不是降低训练误差）。
   - 正则化是机器学习领域的中心问题之一。
   - 没有免费的午餐定理说明了没有最优的学习算法，因此也没有最优的正则化形式。

#### 2.2 泛化能力的评估

常用的对模型泛化能力的评估方法有以下几种，主要区别就是如何划分测试集。

- **留出法**(Holdout)
- **`k-fold` 交叉验证**(Cross Validation)
- **留一法**(Leave One Out, LOO)
- **自助法**(bootstrapping)

##### 2.2.1 **留出法**(Holdout)

留出法是最简单也是最直接的验证方法，它就是将**数据集随机划分为两个互斥的集合**，即训练集和测试集，比如按照 7:3 的比例划分，70% 的数据作为训练集，30% 的数据作为测试集。**也可以划分为三个互斥的集合，此时就增加一个验证集，用于调试参数和选择模型**。

直接采用 `sklearn` 库的 `train_test_split` 函数即可实现，一个简单的示例代码如下，这里简单调用 `knn` 算法，采用 `Iris` 数据集。

```python
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier

# 加载 Iris 数据集
dataset = load_iris()
# 划分训练集和测试集
(trainX, testX, trainY, testY) = train_test_split(dataset.data, dataset.target, random_state=3, test_size=0.3)
# 建立模型
knn = KNeighborsClassifier()
# 训练模型
knn.fit(trainX, trainY)
# 将准确率打印
print('hold_out, score:', knn.score(testX, testY))
```

留出法的使用需要注意：

1. **数据集的划分要尽可能保持数据分布的一致性，避免因为数据划分过程引入额外的偏差而对最终结果产生影响**。比如训练、验证和测试集的类别比例差别很大，则误差估计将由于三个集合数据分布的差异而产生偏差。

   因此，**分类任务中必须保持每个集合中的类别比例相似**。从采样的角度看数据集的划分过程，这种保留类别比例的采样方式称为“分层采样”。

2. 即便确定了训练、验证、测试集的比例，还是有多种划分方式，比如排序后划分、随机划分等等，这些不同的划分方式导致**单次留出法得到的估计结果往往不够稳定可靠**。因此，使用留出法的时候，**往往采用若干次随机划分、重复进行实验后，取平均值作为最终评估结果**。

分层采样的简单代码实现如下所示，主要是调用了 `sklearn.model_selection`  中的 `StratifiedKFold`

```python
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.model_selection import StratifiedKFold
from sklearn.base import clone

def StratifiedKFold_method(n_splits=3):
    '''
    分层采样
    :return:
    '''
    # 加载 Iris 数据集
    dataset = load_iris()
    data = dataset.data
    label = dataset.target
    # 建立模型
    knn = KNeighborsClassifier()
    print('use StratifiedKFold')
    skfolds = StratifiedKFold(n_splits=n_splits, random_state=42)
    scores = 0.
    for train_index, test_index in skfolds.split(data, label):
        clone_clf = clone(knn)
        X_train_folds = data[train_index]
        y_train_folds = (label[train_index])
        X_test_fold = data[test_index]
        y_test_fold = (label[test_index])
        clone_clf.fit(X_train_folds, y_train_folds)
        y_pred = clone_clf.predict(X_test_fold)
        n_correct = sum(y_pred == y_test_fold)
        print(n_correct / len(y_pred))
        scores += n_correct / len(y_pred)
    print('mean scores:', scores / n_splits)
```

留出法也存在以下的缺点：

1. **在验证集或者测试集上的评估结果和划分方式有关系**，这也就是为什么需要多次实验，取平均值；
2. 我们希望评估的是在原始数据集上训练得到的模型的能力，但留出法在划分两个或者三个集合后，训练模型仅使用了原始数据集的一部分，这会降低评估结果的保真性。但这个问题没有完美的解决方法，常见做法是将大约 `2/3 ~ 4/5` 的样本作为训练集，剩余的作为验证集和测试集。

##### 2.2.2 **`k-fold` 交叉验证**(Cross Validation)

**`k-fold` 交叉验证** 的工作流程：

1. 将原始数据集划分为 `k` 个大小相等且互斥的子集；
2. 选择 `k-1` 个子集作为训练集，剩余作为验证集进行模型的训练和评估，重复 `k` 次(每次采用不同子集作为验证集)；
3. 将 `k` 次实验评估指标的平均值作为最终的评估结果。

通常，`k` 取 10。

但和留出法类似，同样存在多种划分 `k` 个子集的方法，所以依然需要随时使用不同方式划分 `p` 次，每次得到 `k` 个子集。

同样，采用 `sklearn.cross_validation` 的 `cross_val_score` 库可以快速实现 `k-fold` 交叉验证法，示例如下：

```python
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn.cross_validation import cross_val_score
# 加载 Iris 数据集
dataset = load_iris()
data = dataset.data
label = dataset.target
# 建立模型
knn = KNeighborsClassifier()
# 使用K折交叉验证模块
scores = cross_val_score(knn, data, label, cv=10, scoring='accuracy')
# 将每次的预测准确率打印出
print(scores)
# 将预测准确平均率打印出
print(scores.mean())
```

##### 2.2.3 留一法

留一法是 `k-fold` 交叉验证的一个特例情况，即让 `k=N`, 其中 `N` 是原始数据集的样本数量，这样**每个子集就只有一个样本，这就是留一法**。

留一法的优点就是**训练数据更接近原始数据集**了，仅仅相差一个样本而已，通过这种方法训练的模型，几乎可以认为就是在原始数据集上训练得到的模型 。

但缺点也比较明显，**计算速度会大大降低**，**特别是原始数据集非常大的时候**，训练 `N` 个模型的计算量和计算时间都很大，因此一般实际应用中很少采用这种方法。

##### 2.2.4 自助法

在留出法和 `k-fold` 交叉验证法中，由于保留了一部分样本用于测试，因此实际训练模型使用的训练集比初始数据集小，**这必然会引入一些因为训练样本规模不同而导致的估计偏差**。

留一法受训练样本规模变化的影响较小，但是**计算复杂度太高**。

自助法是一个以**自助采样法(`bootstrap sampling`)为基础的比较好的解决方案**。同时，它也是随机森林算法中用到的方法。

它的做法就是**对样本数量为 `N` 的数据集进行 `N` 次有放回的随机采样**，得到一个大小是 `N` 的训练集。

在这个过程中将会有一部分数据是没有被采样得到的，一个样本始终没有被采样出来的概率是 $(1-\frac{1}{N})^N$，根据极限可以计算得到:
$$
lim_{N\rightarrow +\infty}(1-\frac{1}{N})^N=\frac{1}{e}\approx0.368
$$
也就是采用自助法，会有 **36.8%** 的样本不会出现在训练集中，使用这部分样本作为测试集。这种方法也被称为包外估计。

自助法的优点有：

- 在**数据集比较小、难以有效划分训练/测试集**时很有用：

- 能从**初始数据集中产生多个不同的训练集**，这对集成学习等方法而言有很大好处。

但也存在如下缺点：

- 产生的数据集**改变了初始数据集的分布，这会引入估计偏差**。因此在**初始数据量足够时，留出法和折交叉验证法更常用**。

#### 2.3 训练集、验证集、测试集

简单介绍下训练集、验证集和测试集各自的作用：

1. **训练集**：主要就是训练模型，理论上越大越好；
2. **验证集**：用于模型调试超参数。通常要求验证集比较大，避免模型会对验证集过拟合；
3. **测试集**：用于评估模型的泛化能力。理论上，测试集越大，评估结果就约精准。另外，测试集必须不包含训练样本，否则会影响对模型泛化能力的评估。

验证集和测试集的对比：

- 测试集通常用于对模型的预测能力进行评估，它是**提供模型预测能力的无偏估计**；如果不需要对模型预测能力的无偏估计，可以不需要测试集；
- 验证集主要是用于超参数的选择。

#### 2.4 划分数据集的比例选择方法

那么一般如何选择划分训练、验证和测试集的比例呢？通常可以按照如下做法：

1. 对于**小批量数据**，数据的拆分的常见比例为：
   - **如果未设置验证集，则将数据三七分**：70% 的数据用作训练集、30% 的数据用作测试集。
   - **如果设置验证集，则将数据划分为**：60% 的数据用作训练集、20%的数据用过验证集、20% 的数据用作测试集。
2. **对于大批量数据，验证集和测试集占总数据的比例会更小**。
   - 对于百万级别的数据，其中 1 万条作为验证集、1 万条作为测试集即可。
   - 验证集的目的就是验证不同的超参数；测试集的目的就是比较不同的模型。
     - 一方面它们要足够大，才足够评估超参数、模型。
     - 另一方面，如果它们太大，则会浪费数据（验证集和训练集的数据无法用于训练）。
3. 在 `k-fold` 交叉验证中：先将所有数据拆分成 `k` 份，然后其中 `1` 份作为测试集，其他 `k-1` 份作为训练集。
   - 这里并没有验证集来做超参数的选择。所有测试集的测试误差的均值作为模型的预测能力的一个估计。
   - **使用 `k-fold` 交叉的原因是：样本集太小**。如果选择一部分数据来训练，则有两个问题：
     - **训练数据的分布可能与真实的分布有偏离**。`k-fold` 交叉让所有的数据参与训练，会使得这种偏离得到一定程度的修正。
     - **训练数据太少，容易陷入过拟合**。`k`-fold 交叉让所有数据参与训练，会一定程度上缓解过拟合。

#### 2.5 分布不匹配

深度学习时代，经常会发生：**训练集和验证集、测试集的数据分布不同**。

如：训练集的数据可能是从网上下载的高清图片，测试集的数据可能是用户上传的、低像素的手机照片。

- **必须保证验证集、测试集的分布一致**，它们都要很好的代表你的真实应用场景中的数据分布。
- **训练数据可以与真实应用场景中的数据分布不一致**，因为最终关心的是在模型真实应用场景中的表现。

如果发生了数据不匹配问题，则可以想办法**让训练集的分布更接近验证集**。

- 一种做法是：**收集更多的、分布接近验证集的数据作为训练集合**。
- 另一种做法是：**人工合成训练数据，使得它更接近验证集**。该策略有一个潜在问题：你可能只是模拟了全部数据空间中的一小部分。导致你的模型对这一小部分过拟合。

当训练集和验证集、测试集的数据分布不同时，有以下经验原则：

- **确保验证集和测试集的数据来自同一分布**。

  因为需要使用验证集来优化超参数，而优化的最终目标是希望模型在测试集上表现更好。

- **确保验证集和测试集能够反映未来得到的数据，或者最关注的数据**。

- **确保数据被随机分配到验证集和测试集上**。

当训练集和验证集、测试集的数据分布不同时，**分析偏差和方差的方式有所不同**。

- 如果**训练集和验证集的分布一致**，那么当训练误差和验证误差相差较大时，我们认为**存在很大的方差问题**。

- 如果训练集和验证集的分布不一致，那么当训练误差和验证误差相差较大时，有两种原因：

  - 第一个原因：**模型只见过训练集数据，没有见过验证集的数据导致的，是数据不匹配的问题**。
  - 第二个原因：**模型本来就存在较大的方差**。

  为了弄清楚原因，需要将训练集再随机划分为：`训练-训练集`、`训练-验证集`。这时候，`训练-训练集`、`训练-验证集` 是同一分布的。

  - **模型在`训练-训练集` 和 `训练-验证集` 上的误差的差距代表了模型的方差**。
  - **模型在`训练-验证集` 和 验证集上的误差的差距代表了数据不匹配问题的程度**。

### 3. 过拟合、欠拟合

机器学习的两个主要挑战是**过拟合和欠拟合**。

**过拟合(overfitting)**：**指算法模型在训练集上的性能非常好，但是泛化能力很差，泛化误差很大，即在测试集上的效果却很糟糕的情况**。

- 过拟合的原因：将**训练样本本身的一些特点当作了所有潜在样本都具有的一般性质**，这会造成泛化能力下降；另一个原因是**模型可能学到训练集中的噪声，并基于噪声进行了预测**；
- **过拟合无法避免，只能缓解**。因为**机器学习的问题通常是 `NP` 难甚至更难的，而有效的学习算法必然是在多项式时间内运行完成**。如果可以避免过拟合，这就意味着构造性的证明了 `P=NP` 。

**欠拟合(underfitting)**：**模型的性能非常差，在训练数据和测试数据上的性能都不好，训练误差和泛化误差都很大**。其原因就是模型的学习能力比较差。

一般可以通过挑战模型的容量来缓解过拟合和欠拟合问题。**模型的容量是指其拟合各种函数的能力**。

- 容量低的模型容易发生欠拟合，模型拟合能力太弱。
- 容量高的模型容易发生过拟合，模型拟合能力太强。

一般解决过拟合的方法有：

- **简化模型**，这包括了采用简单点的模型、减少特征数量，比如神经网络中减少网络层数或者权重参数，决策树模型中降低树的深度、采用剪枝等；
- **增加训练数据**，采用数据增强的方法，比如人工合成训练数据等；
- **早停**，当验证集上的误差没有进一步改善，训练提前终止；
- **正则化**，常用 L1 或者 L2 正则化。
- **集成学习方法**，训练多个模型，并以每个模型的平均输出作为结果，降低单一模型的过拟合风险，常用方法有 `bagging` 、`boosting`、`dropout`(深度学习中的方法)等；
- **噪声注入**：包括输入噪声注入、输出噪声注入、权重噪声注入。将噪声分别注入到输入/输出/权重参数中，虽然噪声可能是模型过拟合的一个原因，但第一可以通过交叉验证来避免；第二就是没有噪声的完美数据也是很有可能发生过拟合；第三可以选择在特征、权值参数加入噪声，而非直接在数据加入噪声。



解决欠拟合的方法有：

- 选择一个**更强大的模型**，带有更多参数
- 用**更好的特征**训练学习算法（特征工程）
- **减小对模型的限制**（比如，减小正则化超参数）



### 4. 超参数调优

超参数调优是一件非常头疼的事情，很多时候都需要一些先验知识来选择合理的参数值，但如果没有这部分先验知识，要找到最优的参数值是很困难，非常耗费时间和精力。但超参数调优确实又可以让模型性能变得更加的好。

在选择超参数调优算法前，需要明确以下几个要素：

- **目标函数**。算法需要最大化/最小化的目标；
- **搜索范围**。一般通过上下限来确定；
- **算法的其他参数**，比如搜索步长。

#### 4.1 搜索策略

常用的几种超参数搜索策略如下：

- **手动搜索**：需要较好的先验知识经验
- **网格搜索**：超参数的数据相对较少的时候，这个方法比较实用
- **随机搜索**：通常推荐这种方式
- **贝叶斯优化算法**：基于模型的搜索方法，利用了历史搜索结果

##### 4.1.1 手动搜索

1. 手动选择超参数需要了解超参数做了些什么，以及机器学习模型如何才能取得良好的泛化。

2. 手动搜索超参数的任务是：**在给定运行时间和内存预算范围的条件下，最小化泛化误差**。

3. 手动调整超参数时不要忘记最终目标：提升测试集性能。

   - 加入正则化只是实现这个目标的一种方法。

   - 如果训练误差很低，也可以通过收集更多的训练数据来减少泛化误差。

     如果训练误差太大，则收集更多的训练数据就没有意义。

   - 实践中的一种暴力方法是：不断提高模型容量和训练集的大小。

     这种方法增加了计算代价，只有在拥有充足的计算资源时才可行

##### 4.1.2 网格搜索

网格搜索可能是最简单也是应用最广泛的超参数搜索算法了。它的几种做法如下：

- **采用较大的搜索范围和较小的搜索步长**，很大概率会搜索到全局最优值，但十分耗费计算资源和时间，特别是超参数比较多的时候；
- **先采用较大搜索范围和较大步长**，寻找全局最优的可能位置，**然后逐渐缩小搜索范围和步长**，来确定更精确的最优值。可以降低所需要的计算时间和计算量，但由于目标函数一般都是非凸的，可能会错过全局最优值。

网格搜索也可以借助 `sklearn` 实现，简单的示例代码如下：

```python
from sklearn.model_selection import	GridSearchCV
from sklearn.ensemble import RandomForestClassifier
param_grid = [
    {'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
    {'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
]
forest_reg = RandomForestRegressor()
grid_search = GridSearchCV(forest_reg, param_grid, cv=5,
                           scoring='neg_mean_squared_error')
grid_search.fit(data, labels)
```



##### 4.1.3 随机搜索

随机搜索是一种可以替代网格搜索的方法，它编程简单、使用方便、能更快收敛到超参数的良好取值。

- 首先为每个超参数定义一个边缘分布，如伯努利分布（对应着二元超参数）或者对数尺度上的均匀分布（对应着正实值超参数）。
- 然后假设超参数之间相互独立，从各分布中抽样出一组超参数。
- 使用这组超参数训练模型。
- 经过多次抽样 -> 训练过程，挑选验证集误差最小的超参数作为最好的超参数。

随机搜索的优点如下：

- **不需要离散化超参数的值，也不需要限定超参数的取值范围**。这允许我们在一个更大的集合上进行搜索。
- 当某些超参数对于性能没有显著影响时，随机搜索相比于网格搜索指数级地高效，它能**更快的减小验证集误差**。

**随机搜索比网格搜索更快的找到良好超参数的原因是：没有浪费的实验**。

- 在网格搜索中，**两次实验之间只会改变一个超参数** （假设为 `m`）的值，而其他超参数的值保持不变。如果这个超参数 `m` 的值对于验证集误差没有明显区别，那么网格搜索相当于进行了两个重复的实验。
- 在随机搜索中，**两次实验之间，所有的超参数值都不会相等，因为每个超参数的值都是从它们的分布函数中随机采样而来**。因此不大可能会出现两个重复的实验。
- 如果  `m`  超参数与泛化误差无关，那么不同的 `m`  值：
  - 在网格搜索中，**不同 `m` 值、相同的其他超参数值，会导致大量的重复实验**。
  - 在随机搜索中，其他超参数值每次也都不同，因此不大可能出现两个重复的实验（除非所有的超参数都与泛化误差无关）。

随机搜索可以采用 `sklearn.model_selection` 中的 `RandomizedSearchCV ` 方法。

##### 4.1.4 贝叶斯优化方法

贝叶斯优化方法是基于模型的参数搜索算法的一种比较常见的算法。它相比于前面的网格搜索和随机搜索，**最大的不同就是利用历史的搜索结果进行优化搜索**。主要是由四部分组成的：

1. 目标函数。大部分情况是模型验证集上的损失；
2. 搜索空间。各类待搜索的超参数；
3. 优化策略。建立的概率模型和选择超参数的方式；
4. 历史的搜索结果。

贝叶斯优化算法的步骤如下：

1. 根据先验分布，假设一个搜索函数；
2. 然后，每一次采用新的采样点来测试目标函数时，利用这个信息更新目标函数的先验分布；
3. 最后，算法测试由后验分布给出的全局最优最可能出现的位置的点。

需要特别注意的是，贝叶斯优化算法容易**陷入局部最优值**：它在找到一个局部最优值后，会不断在该区域进行采样。

因此，贝叶斯优化算法会在探索和利用之间找到一个平衡点，探索是在还未取样的区域获取采样点，利用则是根据后验分布在最可能出现全局最优的区域进行采样。



#### 4.2 调整原则

1. 通常**先对超参数进行粗调，然后在粗调中表现良好的超参数区域进行精调**。

2. 超参数随机搜索，并不意味着是在有效范围内随机均匀取值。需要选择合适的缩放来进行随机选取。

   - 对于学习率，假设其取值范围为 `0.000001~1`。

     如果进行均匀取值，取 10 个，那么有  90% 的随机值都位于区间 `[0.1,1]`。则 `[0.000001,0.1]` 之间没有足够的探索。这种做法明显不合理。

     此时需要使用对数缩放，在对数轴上均匀随机取点。

   - 对于指数加权移动平均的超参数 β  。假设其取值范围为 `0.9~0.9999`。

     由于 $\frac{1}{1-\beta}$ 刻画了结果使用过去多少个周期的数据来加权平均。因此如果进行均匀取值，则：

     -  β  在`0.9~0.9005` 之间取值时， $\frac{1}{1-\beta}$ 变化不大。
     -  β  在`0.9990~0.9995` 之间取值时，$\frac{1}{1-\beta}$  变化非常大。

     β 越接近 1，$\frac{1}{1-\beta}$ 对于它的变化越敏感。此时，需要对 1-β 使用对数缩放，在对数轴上均匀随机取点。

   - 如果选择了错误的缩放，如果取值的总量足够大，也可以得到不错的结果。

     尤其当配合了`粗调 -> 精调` 策略时，最终还是会聚焦到合适的超参数范围上。

3. 通常情况下，**建议至少每隔几个月重新评估或者修改超参数**。因为随着时间的变化，真实场景的数据会逐渐发生改变：

   - 可能是由于用户的行为、偏好发生了改变。
   - 可能是采样的方式发生了改变。
   - 也可能仅仅是由于数据中心更新了服务器。

   由于这些变化，原来设定的超参数可能不再适用。

4. 有两种超参数调整策略：

   - 如果数据足够大且没有足够的计算资源，此时只能一次完成一个试验。

     则**可以每天观察模型的表现，实时的、动态的调整超参数**。

   - 如果数据不大，有足够的计算资源可以同一时间完成大量的试验，则**可以设置多组超参数设定，然后选择其中表现最好的那个**。





------

#### 参考

- 《机器学习》--周志华
- 《百面机器学习》
- 《hands-on-ml-with-sklearn-and-tf》
- [9. 模型评估](http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/9_model_selection.html)
- [分类模型评估的方法及Python实现](https://mp.weixin.qq.com/s/fhVlM8L4dyvMopf3pvau0A)
- [机器学习中用来防止过拟合的方法有哪些？](https://www.zhihu.com/question/59201590)