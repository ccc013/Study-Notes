
目录：

3.2 特征缩放

3.3 特征编码

3.4 特征选择

3.5 特征提取

3.6 特征构建



---

#### 3.2 特征缩放

特征缩放主要分为两种方法，归一化和正则化。

##### 3.2.1 归一化

1. **归一化(Normalization)，也称为标准化**，这里不仅仅是对特征，实际上对于原始数据也可以进行归一化处理，它是将特征（或者数据）都**缩放到一个指定的大致相同的数值区间内**。
2. **归一化的两个原因**：

- 某些算法要求样本数据或特征的数值**具有零均值和单位方差**；
- 为了消除样本数据或者特征之间的**量纲影响，即消除数量级的影响**。如下图所示是包含两个属性的目标函数的等高线
  - **数量级的差异将导致量级较大的属性占据主导地位**。从下图左看到量级较大的属性会让椭圆的等高线压缩为直线，使得目标函数仅依赖于该属性。
  - **数量级的差异会导致迭代收敛速度减慢**。原始的特征进行梯度下降时，每一步梯度的方向会偏离最小值（等高线中心点）的方向，**迭代次数较多，且学习率必须非常小**，否则非常容易引起**宽幅震荡**。但经过标准化后，每一步梯度的方向都几乎指向最小值（等高线中心点）的方向，**迭代次数较少**。
  - **所有依赖于样本距离的算法对于数据的数量级都非常敏感**。比如 KNN 算法需要计算距离当前样本最近的 k 个样本，当属性的量级不同，选择的最近的 k 个样本也会不同。

![图来自《百面机器学习》](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/normalization_example.png)

3. 常用的两种归一化方法：

- **线性函数归一化(Min-Max Scaling)**。它对原始数据进行线性变换，使得结果映射到`[0,1]`的范围，实现对原始数据的等比缩放，公式如下：

$$
X_{norm}=\frac{X-X_{min}}{X_{max}-X_{min}}
$$

其中 X 是原始数据，$X_{max}, X_{min}$分别表示数据最大值和最小值。

- **零均值归一化(Z-Score Normalization)**。它会将原始数据映射到均值为 0，标准差为 1 的分布上。假设原始特征的均值是$\mu$、方差是$\sigma$，则公式如下：

$$
z = \frac{x-\mu}{\sigma}
$$

4. 如果数据集分为训练集、验证集、测试集，那么**三个数据集都采用相同的归一化参数，数值都是通过训练集计算得到**，即上述两种方法中分别需要的数据最大值、最小值，方差和均值都是通过训练集计算得到（这个做法类似于深度学习中批归一化，BN的实现做法）。
5. 归一化不是万能的，实际应用中，**通过梯度下降法求解的模型是需要归一化的，这包括线性回归、逻辑回归、支持向量机、神经网络等模型**。但**决策树模型不需要**，以 C4.5 算法为例，决策树在分裂结点时候主要依据数据集 D 关于特征 x 的信息增益比，而信息增益比和特征是否经过归一化是无关的，归一化不会改变样本在特征 x 上的信息增益。

##### 3.2.2 正则化

1. 正则化是**将样本或者特征的某个范数（如 L1、L2 范数）缩放到单位 1**。

假设数据集为：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/20170417171702256.png)

对样本首先计算 Lp 范数，得到：

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/20170417171715565.png)

正则化后的结果是：每个属性值除以其 Lp 范数

![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/20170417171731785.png)

2. 正则化的过程是针对**单个样本**的，对每个样本将它缩放到单位范数。

   归一化是针对**单个属性**的，需要用到所有样本在该属性上的值。

3. 通常如果使用**二次型（如点积）或者其他核方法计算两个样本之间的相似性**时，该方法会很有用。

#### 3.3 特征编码

##### 3.3.1 序号编码(Ordinal Encoding)

**定义**：序号编码一般用于**处理类别间具有大小关系**的数据。

比如成绩，可以分为高、中、低三个档次，并且存在“高>中>低”的大小关系，那么序号编码可以对这三个档次进行如下编码：高表示为 3，中表示为 2，低表示为 1，这样转换后依然保留了大小关系。

##### 3.3.2 独热编码(One-hot Encoding)

**定义**：独热编码通常用于处理类别间不具有大小关系的特征。

独热编码是采用 **N** 位状态位来对 **N** 个可能的取值进行编码。比如血型，一共有 4 个取值（A、B、AB 以及 O 型），那么独热编码会将血型转换为一个 4 维稀疏向量，分别表示上述四种血型为：

- A型：(1,0,0,0)
- B型：(0,1,0,0)
- AB型：(0,0,1,0)
- O型：(0,0,0,1)

独热编码的优点有以下几个：

- 能够处理**非数值属性**。比如血型、性别等
- 一定程度上扩充了特征。
- 编码后的向量是稀疏向量，只有一位是 1，其他都是 0，可以利用向量的稀疏来**节省存储空间**。
- **能够处理缺失值**。当所有位都是 0，表示发生了缺失。此时可以采用处理缺失值提到的**高维映射**方法，用第 **N+1** 位来表示缺失值。

当然，独热编码也存在一些缺点：

1.高维度特征会带来以下几个方面问题：

- KNN 算法中，**高维空间下两点之间的距离很难得到有效的衡量**；
- 逻辑回归模型中，参数的数量会随着维度的增高而增加，导致**模型复杂，出现过拟合问题**；
- 通常只有部分维度是对分类、预测有帮助，需要**借助特征选择来降低维度**。

2.决策树模型不推荐对离散特征进行独热编码，有以下两个主要原因：

- **产生样本切分不平衡问题，此时切分增益会非常小**。

  比如对血型做独热编码操作，那么对每个特征`是否 A 型、是否 B 型、是否 AB 型、是否 O 型`，会有少量样本是 1 ，大量样本是 0。

  这种划分的增益非常小，因为拆分之后：

	- 较小的那个拆分样本集，它占总样本的比例太小。无论增益多大，乘以该比例之后几乎可以忽略。

	- 较大的那个拆分样本集，它几乎就是原始的样本集，增益几乎为零。

- **影响决策树的学习**。

    决策树依赖的是**数据的统计信息**。而独热码编码会把数据切分到零散的小空间上。在这些零散的小空间上，统计信息是不准确的，学习效果变差。

    本质是因为**独热编码之后的特征的表达能力较差**。该特征的预测能力被人为的拆分成多份，每一份与其他特征竞争最优划分点都失败。最终该特征得到的重要性会比实际值低。

##### 3.3.3 二进制编码(Binary Encoding)

二进制编码主要分为两步：

1. 先采用序号编码给每个类别赋予一个类别 ID；
2. 接着将类别 ID 对应的二进制编码作为结果。

继续以血型为例子，如下表所示：

| 血型 | 类别 ID | 二进制表示 | 独热编码 |
| :--: | :-----: | :--------: | :------: |
|  A   |    1    |   0 0 1    | 1 0 0 0  |
|  B   |    2    |   0 1 0    | 0 1 0 0  |
|  AB  |    3    |   0 1 1    | 0 0 1 0  |
|  O   |    4    |   1 0 0    | 0 0 0 1  |

从上表可以知道，二进制编码本质上是利用**二进制对类别 ID 进行哈希映射，最终得到 0/1 特征向量，并且特征维度小于独热编码，更加节省存储空间**。

##### 3.3.4 二元化

**定义**：特征二元化就是将数值型的属性转换为布尔型的属性。通常用于假设属性取值分布是伯努利分布的情形。

特征二元化的算法比较简单。对属性 `j` 指定一个阈值 `m`。

- 如果样本在属性 `j` 上的值大于等于 `m`, 则二元化后为 1；
- 如果样本在属性 `j` 上的值小于 `m`，则二元化为 0

根据上述定义，`m` 是一个关键的超参数，它的取值需要结合模型和具体的任务来选择。

##### 3.3.5 离散化

**定义**：顾名思义，离散化就是将连续的数值属性转换为离散的数值属性。

那么什么时候需要采用特征离散化呢？

这背后就是需要采用“**海量离散特征+简单模型**”，还是“**少量连续特征+复杂模型**”的做法了。

- 对于线性模型，通常使用“海量离散特征+简单模型”。
  - 优点：模型简单
  - 缺点：特征工程比较困难，但一旦有成功的经验就可以推广，并且可以很多人并行研究。
- 对于非线性模型（比如深度学习），通常使用“少量连续特征+复杂模型”。
  - 优点：不需要复杂的特征工程
  - 缺点：模型复杂

**分桶**

1.离散化的常用方法是**分桶**：

- 将所有样本在连续的数值属性 `j` 的取值从小到大排列 ${a_0, a_1, ..., a_N}$ 。
- 然后从小到大依次选择分桶边界$b_1, b_2, ..., b_M$  。其中：
  - `M` 为分桶的数量，它是一个超参数，需要人工指定。
  - 每个桶的大小$b_{k+1}-b_k$  也是一个超参数，需要人工指定。
- 给定属性 `j` 的取值$a_i$，对其进行分桶：
  - 如果$a_i < b_1$，则分桶编号是 0。分桶后的属性的取值为 0；
  - 如果$b_k \le a_i \le b_{k+1}$，则分桶编号是 `k`。分桶后的属性取值是 `k`；
  - 如果 $a_i \ge b_M$, 则分桶编号是 `M`。分桶后的属性取值是 `M`。

2.分桶的数量和边界通常需要人工指定。一般有两种方法：

- **根据业务领域的经验来指定**。如：对年收入进行分桶时，根据 2017 年全国居民人均可支配收入约为 2.6 万元，可以选择桶的数量为5。其中：
    - 收入小于 1.3 万元（人均的 0.5 倍），则为分桶 0 。
    - 年收入在 1.3 万元 ～5.2 万元（人均的 0.5～2 倍），则为分桶 1 。
    - 年收入在 5.3 万元～26 万元（人均的 2 倍～10 倍），则为分桶 2 。
    - 年收入在 26 万元～260 万元（人均的 10 倍～100 倍），则为分桶 3 。
    - 年收入超过 260 万元，则为分桶 4 。
- **根据模型指定**。根据具体任务来训练分桶之后的数据集，通过超参数搜索来确定最优的分桶数量和分桶边界。

3.选择分桶大小时，有一些经验指导：

- **分桶大小必须足够小**，使得桶内的属性取值变化对样本标记的影响基本在一个不大的范围。

  即不能出现这样的情况：单个分桶的内部，样本标记输出变化很大。

- **分桶大小必须足够大，使每个桶内都有足够的样本**。

  如果桶内样本太少，则随机性太大，不具有统计意义上的说服力。

- 每个桶内的样本尽量**分布均匀**。

**特性**

1.在工业界很少直接将连续值作为逻辑回归模型的特征输入，而是**将连续特征离散化为一系列 0/1 的离散特征**。

其优势有：

- 离散化之后得到的稀疏向量，**内积乘法运算速度更快，计算结果方便存储**。

- 离散化之后的特征对于**异常数据具有很强的鲁棒性**。

  如：销售额作为特征，当销售额在 `[30,100)` 之间时，为1，否则为 0。如果未离散化，则一个异常值 10000 会给模型造成很大的干扰。由于其数值较大，它对权重的学习影响较大。

- 逻辑回归属于广义线性模型，表达能力受限，只能描述线性关系。特征离散化之后，相当于**引入了非线性，提升模型的表达能力，增强拟合能力**。

  假设某个连续特征 `j`  ，它离散化为 `M` 个 0/1 特征 $j_1, j_2, ..., j_M$  。则：$w_j * x_j -> w_{j1} * x_{j1}^` + w_{j2} * x_{j2}^` + ...+w_{jM} * x_{jM}^` $。其中 $x_{j1}^`，x_{j2}^`，..., x_{jM}^`$ 是离散化之后的新的特征，它们的取值空间都是  {0, 1}。

  上式右侧是一个分段线性映射，其表达能力更强。

- **离散化之后可以进行特征交叉**。假设有连续特征`j` ，离散化为 `N` 个 0/1 特征；连续特征 `k`，离散化为 `M` 个 0/1 特征，则分别进行离散化之后引入了 `N+M` 个特征。

  假设离散化时，并不是独立进行离散化，而是特征 `j,k`  联合进行离散化，则可以得到  `N*M` 个组合特征。**这会进一步引入非线性，提高模型表达能力**。

- **离散化之后，模型会更稳定**。

  如对销售额进行离散化，`[30,100)` 作为一个区间。当销售额在40左右浮动时，并不会影响它离散化后的特征的值。

  但是**处于区间连接处的值要小心处理，另外如何划分区间也是需要仔细处理**。

2.**特征离散化简化了逻辑回归模型，同时降低模型过拟合的风险**。

能够对抗过拟合的原因：**经过特征离散化之后，模型不再拟合特征的具体值，而是拟合特征的某个概念**。因此能够**对抗数据的扰动，更具有鲁棒性**。

另外它使得模型要**拟合的值大幅度降低，也降低了模型的复杂度**。

#### 3.4 特征选择

**定义**：从给定的特征集合中选出相关特征子集的过程称为特征选择(feature selection)。

1.对于一个学习任务，给定了属性集，其中某些属性可能对于学习来说很关键，但有些属性意义就不大。

- 对当前学习任务有用的属性或者特征，称为**相关特征**(relevant feature)；
- 对当前学习任务没用的属性或者特征，称为**无关特征**(irrelevant feature)。

2.特征选择可能会降低模型的预测能力，因为被剔除的特征中可能包含了有效的信息，抛弃这部分信息一定程度上会降低模型的性能。但这也是计算复杂度和模型性能之间的取舍：

- 如果保留尽可能多的特征，模型的性能会提升，但同时模型就变复杂，计算复杂度也同样提升；
- 如果剔除尽可能多的特征，模型的性能会有所下降，但模型就变简单，也就降低计算复杂度。

3.常见的特征选择分为三类方法：

- 过滤式(filter)
- 包裹式(wrapper)
- 嵌入式(embedding)

##### 3.4.1 特征选择原理

1.采用特征选择的原因：

- **维数灾难问题**。因为属性或者特征过多造成的问题，如果可以选择重要的特征，使得仅需要一部分特征就可以构建模型，可以大大减轻维数灾难问题，从这个意义上讲，特征选择和降维技术有相似的动机，事实上它们也是处理高维数据的两大主流技术。
- **去除无关特征可以降低学习任务的难度，也同样让模型变得简单，降低计算复杂度**。

2.特征选择最重要的是**确保不丢失重要的特征**，否则就会因为缺少重要的信息而无法得到一个性能很好的模型。

- 给定数据集，学习任务不同，相关的特征很可能也不相同，因此特征选择中的**不相关特征指的是与当前学习任务无关的特征**。
- 有一类特征称作**冗余特征(redundant feature)**，它们所包含的信息可以从其他特征中推演出来。
  - 冗余特征通常都不起作用，去除它们可以减轻模型训练的负担；
  - 但如果冗余特征恰好对应了完成学习任务所需要的某个中间概念，则它是有益的，可以降低学习任务的难度。

3.在没有任何先验知识，即领域知识的前提下，要想从初始特征集合中选择一个包含所有重要信息的特征子集，唯一做法就是遍历所有可能的特征组合。

但这种做法并不实际，也不可行，因为会遭遇组合爆炸，特征数量稍多就无法进行。

一个可选的方案是：

- 产生一个候选子集，评价出它的好坏。
- 基于评价结果产生下一个候选子集，再评价其好坏。
- 这个过程持续进行下去，直至无法找到更好的后续子集为止。

这里有两个问题：如何根据评价结果获取下一个候选特征子集？如何评价候选特征子集的好坏？

###### 3.4.1.1 子集搜索

1.**子集搜索**方法步骤如下：

- 给定特征集合$A={A_1,A_2,...,A_d}$  ，首先将每个特征看作一个候选子集（即每个子集中只有一个元素），然后对这 $d$ 个候选子集进行评价。

  假设 $A_2$ 最优，于是将  $A_2$  作为第一轮的选定子集。

- 然后在上一轮的选定子集中加入一个特征，构成了包含两个特征的候选子集。

  假定 $A_2$,A_5   最优，且优于  $A_2$  ，于是将  $A_2,A_5$  作为第二轮的选定子集。

- ....

- 假定在第 `k+1` 轮时，**本轮的最优的特征子集不如上一轮的最优的特征子集**，则停止生成候选子集，并将上一轮选定的特征子集作为特征选择的结果。

2.这种逐渐增加相关特征的策略称作**前向 `forward`搜索**

类似地，如果从完整的特征集合开始，每次尝试去掉一个无关特征，这种逐渐减小特征的策略称作**后向`backward`搜索**

3.也可以将前向和后向搜索结合起来，每一轮逐渐增加选定的相关特征（这些特征在后续迭代中确定不会被去除），同时减少无关特征，这样的策略被称作是**双向`bidirectional`搜索**。

4该策略是贪心的，因为它们仅仅考虑了使本轮选定集最优。但是除非进行穷举搜索，否则这样的问题无法避免。

###### 3.4.1.2 子集评价

1.子集评价的做法如下：

给定数据集 D，假设所有属性均为离散型。对属性子集 A，假定根据其取值将 D 分成了 V 个子集：${D_1, D_2, \cdots,  D_V}$

可以计算属性子集 A 的信息增益：
$$
g(D, A) = H(D) - H(D|A)=H(D)-\sum^V_{v=1}\frac{|D_v|}{|D|}H(D_v)
$$
其中，$|•|$表示集合大小，$H(•)$表示熵。

**信息增益越大，表明特征子集 A 包含的有助于分类的信息越多**。所以对于每个候选特征子集，可以基于训练集 D 来计算其信息增益作为评价准则。

2.更一般地，特征子集 A 实际上确定了对数据集 D 的一个划分规则。

- 每个划分区域对应着 A 上的一个取值，而样本标记信息 y 则对应着 D 的真实划分。
- 通过估算这两种划分之间的差异，就能对 A 进行评价：与 y 对应的划分的差异越小，则说明 A 越好。
- **信息熵仅仅是判断这个差异的一种方法，其他能判断这两个划分差异的机制都能够用于特征子集的评价**。

3.**将特征子集搜索机制与子集评价机制结合就能得到特征选择方法**。

- 事实上，决策树可以用于特征选择，所有树结点的划分属性所组成的集合就是选择出来的特征子集。
- 其他特征选择方法本质上都是显式或者隐式地结合了某些子集搜索机制和子集评价机制。

4.常见的特征选择方法分为以下三种，主要区别在于特征选择部分是否使用后续的学习器。

- **过滤式**(filter)：先对数据集进行特征选择，其过程与后续学习器无关，即设计一些统计量来过滤特征，并不考虑后续学习器问题
- **包裹式**(wrapper)：实际上就是一个分类器，它是将后续的学习器的性能作为特征子集的评价标准。
- **嵌入式**(embedding)：实际上是学习器自主选择特征。

5.最简单的特征选择方法是：**去掉取值变化小的特征**。

假如某特征只有 0 和 1 的两种取值，并且所有输入样本中，95% 的样本的该特征取值都是 1 ，那就可以认为该特征作用不大。

当然，该方法的一个前提是，特征值都是**离散型**才使用该方法；如果是连续型，需要离散化后再使用，并且实际上一般不会出现 95% 以上都取某个值的特征的存在。

所以，这个方法简单，但不太好用，可以作为特征选择的一个预处理，先去掉变化小的特征，然后再开始选择上述三种类型的特征选择方法。

##### 3.4.2 过滤式选择

该方法**先对数据集进行特征选择，然后再训练学习器**。特征选择过程与后续学习器无关。

也就是先采用特征选择对初始特征进行过滤，然后用过滤后的特征训练模型。

- 优点是**计算时间上比较高效，而且对过拟合问题有较高的鲁棒性**；
- 缺点是**倾向于选择冗余特征**，即没有考虑到特征之间的相关性。

###### 3.4.2.1 Relief 方法

1.`Relief:Relevant Features`是一种著名的过滤式特征选择方法。该方法设计了一个**相关统计量来度量特征的重要性**。

- 该统计量是一个向量，其中每个分量都对应于一个初始特征。特征子集的重要性则是由**该子集中每个特征所对应的相关统计量分量之和来决定的**。

- 最终只需要指定一个阈值 $\gamma$，然后**选择比 $\gamma$ 大的相关统计量分量所对应的特征**即可。

  也可以**指定特征个数 m** ，然后选择相关统计量分量最大的 m 个特征。

2.给定训练集$D={(\vec{x_1}, \breve{y_1}), \cdots, (\vec{x_N}, \breve{y_N})}, \breve{y_i}\in{0, 1}$。对于每个样本$\vec{x_i}$：

- `Relief` 先在$\vec{x_i}$同类样本中寻找其最近邻 $\vec{x_{nm_i}}$，称作猜中近邻 `near-hit` ；
- 然后从$\vec{x_i}$ 的异类样本寻找最近邻$\vec{x_{nm_i}}$，称作猜错近邻`near-miss`。
- 然后相关统计量对应于属性`j`的分量为：

$$
\delta_j = \sum^N_{i=1}(-diff(x_{i,j}, x_{nh_i,j})^2 + diff(x_{i,j},x_{nm_i,j})^2)
$$

其中$diff(x_{a,j},x_{b,j})$是两个样本在属性 `j` 上的差异值，其结果取决于该属性是离散的还是连续的：

- 如果是离散的，则有

$$
    diff(x{a,j},x{b,j})=
    \begin{cases}
    0,\quad if\; x_{a,j}=x_{b,j}\\
    1, \quad else
    \end{cases}
$$

- 如果是连续的，则有

$$
diff(x{a,j},x{b,j})=|x_{a,j} - x_{b,j}|
$$

​     注意，此时需要对$x_{a,j},x_{b,j}$进行标准化到`[0,1]`区间。

3.根据 3 的公式可以知道，如果

- 如果 $\vec{x_i}$ 与其猜中近邻  $\vec{x_{nh_i}}$ 在属性 `j` 上的距离小于 $\vec{x_i}$ 与其猜错近邻 $\vec{x_{nm_i}}$ 的距离，则说明属性 `j` 对于区分同类与异类样本是有益的，于是增大属性 `j` 所对应的统计量分量。
- 如果 $\vec{x_i}$ 与其猜中近邻  $\vec{x_{nh_i}}$ 在属性 `j` 上的距离大于$\vec{x_i}$  与其猜错近邻 $\vec{x_{nm_i}}$ 的距离，则说明属性 `j` 对于区分同类与异类样本是起负作用的，于是减小属性 `j` 所对应的统计量分量。
- 最后对基于不同样本得到的估计结果进行平均，就得到各属性的相关统计量分量。**分量值越大，则对应属性的分类能力越强**。

4.`Relief` 是为二分类问题设计的，其拓展变体 `Relief-F` 可以处理多分类问题。

假定数据集 D 中的样本类别为：${c_1, c_2, \cdots, c_K}$  。对于样本 $\vec{x_i}$ ，假设 $\breve{y_i}=c_k$ 。

- `Relief-F` 先在类别 $c_k$ 的样本中寻找 $\vec{x_i}$ 的最近邻 $\vec{x_{nh_i}}$ 作为猜中近邻。
- 然后在 $c_k$  之外的每个类别中分别找到一个 $\vec{x_i}$的最近邻 $\vec{x_{nm_i^l}}, l=1,2,\cdots,K; l \neq k$$   作为猜错近邻。
- 于是相关统计量对应于属性  j 的分量为：

$$
\delta_j = \sum^N_{i=1}(-diff(x_{i,j},x_{nh_{i,j}})^2+\sum_{l\neq k}(p_l\times diff(x_{i,j},x_{nm_{i,j}^l})^2))
$$

​     其中 $p_l$ 作为第 $l$ 类的样本在数据集 D 中所占的比例。

###### 3.4.2.2 方差选择法

使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用 sklearn 的 feature_selection 库的 VarianceThreshold 类来选择特征的代码如下：

```python
from sklearn.feature_selection import VarianceThreshold

#方差选择法，返回值为特征选择后的数据
#参数threshold为方差的阈值
VarianceThreshold(threshold=3).fit_transform(data)
```

###### 3.4.2.3 相关系数法

使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的 P 值。用 feature_selection 库的 SelectKBest 类结合相关系数来选择特征的代码如下：

```python
from sklearn.feature_selection import SelectKBest
from scipy.stats import pearsonr

#选择 K 个最好的特征，返回选择特征后的数据
#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数
#参数k为选择的特征个数
SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
```



###### 3.4.2.4 卡方检验

经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有 N 种取值，因变量有 M 种取值，考虑自变量等于 i 且因变量等于 j 的样本频数的观察值与期望的差距，构建统计量：
$$
X^2 = \sum\frac{(A-E)^2}{E}
$$
不难发现，这个统计量的含义简而言之就是自变量对因变量的相关性。用 feature_selection 库的 SelectKBest 类结合卡方检验来选择特征的代码如下：

```python
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

#选择K个最好的特征，返回选择特征后的数据
SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)
```

###### 3.4.2.5 互信息法

经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：
$$
I(X;Y)=\sum_{x\in X}\sum_{y\in Y}p(x,y)log\frac{p(x,y)}{p(x)p(y)}
$$
为了处理定量数据，**最大信息系数法**被提出，使用 feature_selection 库的 SelectKBest 类结合最大信息系数法来选择特征的代码如下

```python
from sklearn.feature_selection import SelectKBest
 from minepy import MINE
 
 #由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5
 def mic(x, y):
     m = MINE()
     m.compute_score(x, y)
     return (m.mic(), 0.5)

#选择K个最好的特征，返回特征选择后的数据
SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
```



##### 3.4.3 包裹式选择

1.相比于过滤式特征选择不考虑后续学习器，包裹式特征选择**直接把最终将要使用的学习器的性能作为特征子集的评价原则**。其目的就是为给定学习器选择最有利于其性能、量身定做的特征子集。

- 优点是直接针对特定学习器进行优化，考虑到特征之间的关联性，因此**通常包裹式特征选择比过滤式特征选择能训练得到一个更好性能的学习器**，
- 缺点是由于特征选择过程需要多次训练学习器，故计算**开销要比过滤式特征选择要大得多**。

2.`LVW:Las Vegas Wrapper`是一个典型的包裹式特征选择方法。它是` Las Vegas method`   框架下使用随机策略来进行子集搜索，并以最终分类器的误差作为特征子集的评价标准。

3. `LVW` 算法：

- 输入：数据集 D，特征集 A，学习器 estimator, 迭代停止条件 T

- 输出：最优特征子集 $A^*$

- 算法步骤：

  - 初始化：令候选的最优特征子集$\breve{A^*}=A$, 然后学习器 estimator 在特征子集 $\breve{A^*}$上使用交叉验证法进行学习，通过学习结果评估学习器的误差 $err^*$。
  - 迭代，停止条件为迭代次数达到 T。迭代过程是：
    - 随机产生特征子集 $A^\prime$
    - 学习器在特征子集 $A^\prime$ 上使用交叉验证法进行学习，通过学习评估学习器的误差 $err^\prime$
    - 如果 $err^\prime$ 小于 $err^*$，或者$err^\prime = err^*$，但是特征子集 $A^\prime$ 的特征数量少于候选的最优特征子集$\breve{A^*}$，则令$A^\prime$ 为候选的最优特征子集，即 $\breve{A^*} = A^\prime; err^* = err^\prime$

  - 最终有 $A^* = \breve{A^*}$

4.由于 `LVW` 算法中**每次特征子集评价都需要训练学习器，计算开销很大**，因此算法设计了停止条件控制参数 $T$。

但是如果初始特征数量很多、$T$ 设置较大、以及每一轮训练的时间较长， 则很可能算法运行很长时间都不会停止。即：**如果有运行时间限制，则有可能给不出解**。

5.**递归特征消除法**：使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。使用 feature_selection 库的 RFE 类来选择特征的代码如下：

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

#递归特征消除法，返回特征选择后的数据
#参数estimator为基模型
#参数n_features_to_select为选择的特征个数
RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target)
```



##### 3.4.4 嵌入式选择

1.在过滤式和包裹式特征选择方法中，特征选择过程与学习器训练过程有明显的分别。

嵌入式特征选择是将特征选择与学习器训练过程融为一体，两者在同一个优化过程中完成的。**即学习器训练过程中自动进行了特征选择**。

常用的方法包括：

- 利用正则化，如`L_1, L_2` 范数，主要应用于如线性回归、逻辑回归以及支持向量机(SVM)等算法；
- 使用决策树思想，包括决策树、随机森林、Gradient Boosting 等。

2.以线性回归模型为例。给定数据集 $D={(\vec{x_1}, \breve{y_1}), \cdots, (\vec{x_N}, \breve{y_N})}, \breve{y_i}\in R$。以平方差为损失函数，则优化目标是 $min_{\vec{w}} \sum^N_{i=1}(\breve{y_i}-\vec{w^T}\vec{x_i})^2$

- 如果使用 $L_2$ 范数正则化，优化目标就是 $min_{\vec{w}} \sum^N_{i=1}(\breve{y_i}-\vec{w^T}\vec{x_i})^2+ \lambda|| \vec{w}||^2_2 , \lambda > 0$，此时称作**岭回归**(ridge regression)。 
- 如果使用 $L_1$ 范数正则化，优化目标就是 $min_{\vec{w}} \sum^N_{i=1}(\breve{y_i}-\vec{w^T}\vec{x_i})^2+ \lambda|| \vec{w}||_1 , \lambda > 0$，此时称作**LASSO(Least Absolute Shrinkage and Selection Operator)回归**。 

3.引入 $L_1$ 范数除了**降低过拟合风险**之外，还有一个好处：它求得的 $\vec{w}$ 会有较多的分量为零。即：**它更容易获得稀疏解**。

于是基于 $L_1$ 正则化的学习方法就是一种嵌入式特征选择方法，其特征选择过程与学习器训练过程融为一体，二者同时完成。

4.常见的嵌入式选择模型：

- 在 `Lasso` 中，$\lambda$ 参数控制了稀疏性：
  - 如果 $\lambda$ 越小，则稀疏性越小，被选择的特征越多；
  - 相反，$\lambda$ 越大，则稀疏性越大，被选择的特征越少；
- 在 `SVM` 和 逻辑回归中，参数 `C` 控制了稀疏性：
  - 如果 `C` 越小，则稀疏性越大，被选择的特征越少；
  - 如果 `C` 越大， 则稀疏性越小，被选择的特征越多。



#### 3.5 特征提取

特征提取一般是在特征选择之前，它提取的对象是**原始数据**，目的就是自动地构建新的特征，**将原始数据转换为一组具有明显物理意义（比如 Gabor、几何特征、纹理特征）或者统计意义的特征**。

一般常用的方法包括降维（PCA、ICA、LDA等）、图像方面的SIFT、Gabor、HOG等、文本方面的词袋模型、词嵌入模型等，这里简单介绍这几种方法的一些基本概念。

##### 3.5.1 降维

1.**PCA**(Principal Component Analysis，主成分分析)

PCA 是降维最经典的方法，它旨在是找到**数据中的主成分，并利用这些主成分来表征原始数据，从而达到降维的目的**。

PCA 的思想是通过坐标轴转换，寻找数据分布的最优子空间。

比如，在三维空间中有一系列数据点，它们分布在过原点的平面上，如果采用自然坐标系的 x，y，z 三个轴表示数据，需要三个维度，但实际上这些数据点都在同一个二维平面上，如果我们可以通过坐标轴转换使得数据所在平面和 x，y 平面重合，我们就可以通过新的 x'、y' 轴来表示原始数据，并且没有任何损失，这就完成了降维的目的，而且这两个新的轴就是我们需要找的主成分。

因此，PCA 的解法一般分为以下几个步骤：

1. 对样本数据进行中心化处理；
2. 求样本协方差矩阵；
3. 对协方差矩阵进行特征值分解，将特征值从大到小排列；
4. 取特征值前 n 个最大的对应的特征向量 `W1, W2, ..., Wn` ，这样将原来 m 维的样本降低到 n 维。

通过 PCA ，就可以将方差较小的特征给抛弃，这里，特征向量可以理解为坐标转换中新坐标轴的方向，特征值表示在对应特征向量上的方差，**特征值越大，方差越大，信息量也就越大**。这也是为什么选择前 n 个最大的特征值对应的特征向量，因为这些特征包含更多重要的信息。

**PCA 是一种线性降维方法，这也是它的一个局限性**。不过也有很多解决方法，比如采用核映射对 PCA 进行拓展得到核主成分分析(KPCA)，或者是采用流形映射的降维方法，比如等距映射、局部线性嵌入、拉普拉斯特征映射等，对一些 PCA 效果不好的复杂数据集进行非线性降维操作。

2.**LDA**(Linear Discriminant Analysis，线性判别分析)

LDA 是一种有监督学习算法，相比较 PCA，它考虑到数据的类别信息，而 PCA 没有考虑，只是将数据映射到方差比较大的方向上而已。

因为考虑数据类别信息，所以 LDA 的目的不仅仅是降维，还需要找到一个投影方向，使得投影后的样本尽可能按照原始类别分开，即寻找一个**可以最大化类间距离以及最小化类内距离**的方向。

LDA 的主要步骤如下：

1. 分别计算每个类别 i 的原始中心点：$m_i = \frac{1}{n_i}\sum{x}$
2. 类别 i 投影后的中心点为: $m_i = w^Tm_i$
3. 衡量类别 i 投影后，类间的分散程度，用方差来表示：$s_i = \sum{(y-m_i)^2}$
4. 使用下面的式子表示 LDA 投影到 w 后的损失函数，最大化 J(w) 可以求出最优的 w , $J(w) = \frac{|m_1-m_2|^2}{s_1^2 + s_2^2}$，具体的解法参考[机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)](https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html)。

LDA 的优点如下：

- 相比较 PCA，LDA 更加擅长处理带有类别信息的数据；
- 线性模型对噪声的鲁棒性比较好，LDA 是一种有效的降维方法。

相应的，也有如下缺点：

- LDA 对**数据的分布做出了很强的假设**，比如每个类别数据都是高斯分布、各个类的协方差相等。这些假设在实际中不一定完全满足。
- **LDA 模型简单，表达能力有一定局限性**。但这可以通过引入**核函数**拓展 LDA 来处理分布比较复杂的数据。

3.**ICA**(Independent Component Analysis，独立成分分析)

PCA特征转换降维，提取的是**不相关**的部分，ICA独立成分分析，获得的是**相互独立**的属性。ICA算法本质寻找一个线性变换 `z = Wx`，使得 z 的**各个特征分量之间的独立性最大**。

通常先采用 PCA 对数据进行降维，然后再用 ICA 来从多个维度分离出有用数据。PCA 是 ICA 的数据预处理方法。

具体可以查看知乎上的这个问题和回答 [独立成分分析 ( ICA ) 与主成分分析 ( PCA ) 的区别在哪里？](https://www.zhihu.com/question/28845451)。

##### 3.5.2 图像特征提取

图像的特征提取，在深度学习火起来之前，是有很多传统的特征提取方法，比较常见的包括以下几种。

1.**SIFT** 特征

SIFT 是图像特征提取中非常广泛应用的特征。它包含以下几种优点：

- 具有旋转、尺度、平移、视角及亮度不变性，有利于对目标特征信息进行有效表达；
- SIFT 特征对参数调整鲁棒性好，可以根据场景需要调整适宜的特征点数量进行特征描述，以便进行特征分析。

SIFT 对图像局部特征点的提取主要包括四个步骤：

1. 疑似特征点检测
2. 去除伪特征点
3. 特征点梯度与方向匹配
4. 特征描述向量的生成

SIFT 的缺点是不借助硬件加速或者专门的图像处理器很难实现。

2.**SURF** 特征

SURF 特征是对 SIFT 算法的改进，降低了时间复杂度，并且提高了鲁棒性。

它主要是简化了 SIFT 的一些运算，如将 SIFT 中的高斯二阶微分的模型进行了简化，使得卷积平滑操作仅需要转换成加减运算。并且最终生成的特征向量维度从 128 维减少为 64 维。

3.**HOG** 特征

方向梯度直方图(HOG)特征是 2005 年针对行人检测问题提出的直方图特征，它通过计算和统计图像局部区域的梯度方向直方图来实现特征描述。

HOG 特征提取步骤如下：

1. **归一化处理**。先将图像转为灰度图像，再利用伽马校正实现。这一步骤是为了提高图像特征描述对光照及环境变化的鲁棒性，降低图像局部的阴影、局部曝光过多和纹理失真，尽可能抵制噪声干扰；
2. **计算图像梯度**；
3. **统计梯度方向**；
4. **特征向量归一化**；为克服光照不均匀变化及前景与背景的对比差异，需要对块内的特征向量进行归一化处理。
5. **生成特征向量**。



4.**LBP** 特征

局部二值模式（LBP）是一种描述图像局部纹理的特征算子，它具有旋转不变性和灰度不变性的优点。

LBP 特征描述的是一种灰度范围内的图像处理操作技术，针对的是**输入为 8 位或者 16 位的灰度图像**。

LBP 特征通过对**窗口中心点与邻域点的关系进行比较**，重新编码形成新特征以消除对外界场景对图像的影响，因此一定程度上解决了**复杂场景下（光照变换）特征**描述问题。

根据窗口领域的不同分为两种，经典 LBP 和圆形 LBP。前者的窗口是 3×3 的正方形窗口，后者将窗口从正方形拓展为任意圆形领域。

更详细的可以参考这篇文章--[图像特征检测描述(一):SIFT、SURF、ORB、HOG、LBP特征的原理概述及OpenCV代码实现](https://blog.csdn.net/wenhao_ir/article/details/52046569)

当然上述特征都是比较传统的图像特征提取方法了，现在图像基本都直接利用 CNN（卷积神经网络）来进行特征提取以及分类。

##### 3.5.3 文本特征提取

1.**词袋模型**

最基础的文本表示模型是词袋模型。

具体地说，就是将整段文本以词为单位切分开，然后每篇文章可以表示成一个长向量，向量的每一个维度代表一个单词，而该维度的权重反映了该单词在原来文章中的重要程度。

通常采用 **TF-IDF** 计算权重，公式为 `TF-IDF(t, d) = TF(t,d) × IDF(t)`

其中 TF(t, d) 表示单词 t 在文档 d 中出现的频率，IDF(t) 是逆文档频率，用来衡量单词 t 对表达语义所起的重要性，其表示为：
$$
IDF(t)=log\frac{文章总数}{包含单词 t 的文章总数+1}
$$
直观的解释就是，如果这个单词在多篇文章都出现过，那么它很可能是比较通用的词汇，对于区分文章的贡献比较小，自然其权重也就比较小，即 IDF(t) 会比较小。

2.**N-gram 模型**

词袋模型是以单词为单位进行划分，但有时候进行单词级别划分并不是很好的做法，毕竟有的单词组合起来才是其要表达的含义，比如说 `natural language processing(自然语言处理)`、`computer vision(计算机视觉)` 等。

因此可以将连续出现的 n 个词 (n <= N) 组成的词组(N-gram)作为一个单独的特征放到向量表示中，构成了 N-gram 模型。

另外，同一个词可能会有多种词性变化，但却具有相同含义，所以实际应用中还会对单词进行**词干抽取**(Word Stemming)处理，即将不同词性的单词统一为同一词干的形式。

3.**词嵌入模型**

词嵌入是一类将词向量化的模型的统称，核心思想是将**每个词都映射成低维空间（通常 K=50~300 维）上的一个稠密向量（Dense Vector）**。

常用的词嵌入模型是 **Word2Vec**。它是一种底层的神经网络模型，有两种网络结构，分别是 CBOW(Continues Bag of Words) 和 Skip-gram。

CBOW 是根据**上下文出现的词语预测当前词**的生成概率；Skip-gram 是根据**当前词来预测上下文中各个词的生成概率**。

词嵌入模型是将每个词都映射成一个 K 维的向量，如果一篇文档有 N 个单词，那么每篇文档就可以用一个 N×K 的矩阵进行表示，但这种表示过于底层。实际应用中，如果直接将该矩阵作为原文本的特征表示输入到模型中训练，通常很难得到满意的结果，一般还需要对该矩阵进行处理，提取和构造更高层的特征。

深度学习模型的出现正好提供了一种自动进行特征工程的方法，它的每个隐含层都相当于不同抽象层次的特征。卷积神经网络（CNN)和循环神经网络（RNN)在文本表示中都取得了很好的效果，这是因为它们可以很好地对文本进行建模，抽取出一些高层的语义特征。

##### 3.5.4 特征提取和特征选择的区别

特征提取与特征选择都是为了从原始特征中找出最有效的特征。

它们之间的区别是特征提取强调通过**特征转换**的方式得到一组具有明显物理或统计意义的特征；

而特征选择是从特征集合中挑选一组具有明显物理或统计意义的**特征子集**。

两者都能**帮助减少特征的维度、数据冗余**，特征提取有时能发现更有意义的特征属性，特征选择的过程经常能表示出每个特征的重要性对于模型构建的重要性。

#### 3.6 特征构建

特征构建是指**从原始数据中人工的构建新的特征**。需要花时间去观察原始数据，思考问题的潜在形式和数据结构，对数据敏感性和机器学习实战经验能帮助特征构建。

特征构建需要很强的洞察力和分析能力，要求我们能够从原始数据中找出一些具有物理意义的特征。假设原始数据是表格数据，一般你可以使用**混合属性或者组合属性**来创建新的特征，或是**分解或切分原有的特征**来创建新的特征。

特征构建非常需要相关的领域知识或者丰富的实践经验才能很好构建出更好的有用的新特征，相比于特征提取，特征提取是通过一些现成的特征提取方法来将原始数据进行特征转换，而特征构建就需要我们自己人为的手工构建特征，比如组合两个特征，或者分解一个特征为多个新的特征。

------

### 参考

- 《hands-on-ml-with-sklearn-and-tf》第二节
- https://www.jiqizhixin.com/articles/091202
- https://blog.csdn.net/fendegao/article/details/79968994
- https://blog.csdn.net/xg123321123/article/details/80781611
- https://towardsdatascience.com/top-sources-for-machine-learning-datasets-bb6d0dc3378b
- 《百面机器学习》第一章 特征工程
- [机器学习之特征工程](https://blog.csdn.net/dream_angel_z/article/details/49388733#commentBox)
- [[数据预处理（方法总结）]](https://www.cnblogs.com/sherial/archive/2018/03/07/8522405.html)
- [Python数据分析（三）——数据预处理](https://gofisher.github.io/2018/06/22/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/)
- [Python数据分析（二）——数据探索](https://gofisher.github.io/2018/06/20/%E6%95%B0%E6%8D%AE%E6%8E%A2%E7%B4%A2/)
- [【Python数据分析基础】: 异常值检测和处理](https://juejin.im/post/5b6a44f55188251aa8294b8c)
- http://www.huaxiaozhuan.com/%E7%BB%9F%E8%AE%A1%E5%AD%A6%E4%B9%A0/chapters/8_feature_selection.html
- [Python机器学习-数据预处理技术（标准化处理、归一化、二值化、独热编码、标记编码）](https://blog.csdn.net/weixin_38168620/article/details/79233086)
- [[Scikit-learn介绍几种常用的特征选择方法](http://dataunion.org/14072.html)](http://dataunion.org/14072.html)
- [博客园--机器学习之特征工程](https://www.cnblogs.com/wxquare/p/5484636.html)
- [机器学习中的数学(4)-线性判别分析（LDA）, 主成分分析(PCA)](https://www.cnblogs.com/LeftNotEasy/archive/2011/01/08/lda-and-pca-machine-learning.html)
- [独立成分分析 ( ICA ) 与主成分分析 ( PCA ) 的区别在哪里？](https://www.zhihu.com/question/28845451)
- [图像特征检测描述(一):SIFT、SURF、ORB、HOG、LBP特征的原理概述及OpenCV代码实现](https://blog.csdn.net/wenhao_ir/article/details/52046569)
- [Gabor特征提取](https://blog.csdn.net/xidianzhimeng/article/details/19493019)


