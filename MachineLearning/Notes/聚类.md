# 聚类

所谓聚类，就是指在不知道任何样本的标签的情况下，通过数据之间的内在关系将样本分成若干个类别，使得相同类别样本之间的相似度高，不同类别之间的样本相似度低。

聚类的相关算法，包括：

1. kmeans
2. GMM





## Kmeans

参考：

- 《机器学习》
- [机器学习&数据挖掘笔记_16（常见面试之机器学习算法思想简单梳理）](http://www.cnblogs.com/tornadomeet/p/3395593.html)
- [K-Means Clustering](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/)
- [斯坦福大学公开课 ：机器学习课程](http://v.163.com/special/opencourse/machinelearning.html)
- [26_k-means算法原理](https://github.com/GYee/CV_interviews_Q-A/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/26_k-means%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86.md)



------

### 1. 简介

K-均值是最普及的聚类算法，算法接受一个未标记的数据集，然后将数据集聚类成不同的组。

● k-means是一种聚类算法。所谓的聚类，就是指在不知道任何样本的标签的情况下，通过数据之间的内在关系将样本分成若干个类别，使得相同类别样本之间的相似度高，不同类别之间的样本相似度低。因此，k-means算法属于非监督学习的范畴。

● k 是指 k 个簇（cluster），means 是指每个簇内的样本均值，也就是聚类中心。

● 基本思想：通过迭代的方式寻找 k 个簇的划分方案，使得聚类结果对应的代价函数最小。代价函数可以定义为各个样本距离它所属的簇的中心点的误差平方和：
$$
J(c,\mu)=\sum_{i=1}^{N} ||x_{i} - \mu _{c_{i}}||^{2} \\
其中，x_{i}代表第i个样本，c_{i}是x_{i}所属的簇，\mu_{c_{i}}代表簇对应的中心点（即均值），N是样本总数.
$$



### 2. 优化目标

K-均值最小化问题，就是**最小化所有的数据点与其所关联的聚类中心之间的距离之和**，因此K-均值的代价函数（又称为**畸变函数**）为： 
$$
J(c^{(1)},c^{(2)},…,c^{(m)},μ_1,μ_2,…,μ_m)=\frac{1}{m}∑_{i=1}^m||x^{(i)}−μ_{c^{(i)}}||^2
$$
其中$\mu_{c^{(i)}}$代表与$x^{(i)}$最近的聚类中心点。

所以我们的优化目标是找出是的代价函数最小的$和c^{(1)},c^{(2)},\ldots,c^{(m)}和\mu_1,\mu_2,\ldots,\mu_m$: 
$$
min_{c^{(1)},c^{(2)},\ldots,c^{(m)}, \mu_1,\mu_2,\ldots,\mu_m}J(c^{(1)},c^{(2)},\ldots,c^{(m)},\mu_1,\mu_2,\ldots,\mu_m)
$$
回顾K-均值迭代算法的过程可知，第一个循环就是用于减小$c^{(i)}$引起的代价，而第二个循环则是用于减小$μ_i$引起的代价，因此，**迭代的过程一定会是每一次迭代都在减小代价函数，不然便是出现了错误。**



### 3. 算法流程

K-均值是一个迭代算法，采用的是**贪心策略**，假设我们想要将数据聚类成 n 个组，其方法为：

1. 首先选择 **K** 个随机的点，称其为**聚类中心**，记为 $\mu_1^{(0)},\mu_{2}^{(0)},...,\mu_{k}^{(0)}$
2. 对于数据集中的每一个数据，按照距离 **K** 个中心点的距离，将其与距离最近的中心点关联起来，与同一个中心点关联的所有点聚成一个类：

$$
c_{i}^{(t)} \leftarrow \underset{k'}{argmin} ||x_{i}-\mu _{k'}^{(t)}||^{2} \\ 
其中,t为当前迭代步数，k'为第k'个簇（类别）(k'=1,2,..,k)
$$



3. 计算每一个组的**平均值**，将该组所关联的中心点移动到平均值的位置：

$$
\mu_{k}^{(t+1)} \leftarrow \underset{\mu}{argmin} \sum_{i:c_{i}^{(t)}=k'} ||x_{i}-\mu||^{2}
$$



4. 重复步骤 2-3，直到中心点不再变化

这个过程中分两个主要步骤，第一个就是第二步，将训练集中的样本点根据其与聚类中心的距离，分配到距离最近的聚类中心处，接着第二个就是第三步，更新类中心，做法是计算每个类的所有样本的平均值，然后将这个平均值作为新的类中心值，接着继续这两个步骤，直到达到终止条件，一般是指达到设定好的迭代次数。

当然在这个过程中可能遇到有聚类中心是没有分配数据点给它的，通常的一个做法是**删除这种聚类中心，或者是重新选择聚类中心，保证聚类中心数还是初始设定的 K 个**。



k-means算法迭代示意图如下：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/kmeans_fig.png)



### 4. 随机初始化

在运行 K-均值算法之前，首先需要随机初始化所有的聚类中心点，做法如下：

1. 首先应该选择 $K<m$ ,即**聚类中心点的个数要小于所有训练集实例的数量**
2. 随机选择 $K$ 个训练实例，然后令 $K$ 个聚类中心分别和这 K 个训练实例相等

K-均值的一个问题在于，**它有可能会停留在一个局部最小值处，而这取决于初始化的情况。**

为了解决这个问题，通常需要**多次运行 K-均值算法，每一次都重新进行随机初始化，最后再比较多次运行K-均值的结果，选择代价函数最小的结果。**这种方法在 **K较小（2-10）**的时候还是可行的，但是如果 K 较大，这种做法可能不会有明显地改善。



### 5. 优缺点

#### 优点

1. k-means算法是解决聚类问题的一种经典算法，**算法简单、快速**。
2. 对处理大数据集，该算法是**相对可伸缩的和高效率**的，因为它的复杂度大约是 $O(nkt)$，其中$n$是所有对象的数目，$k$ 是簇的数目, $t$ 是迭代的次数。通常 $k<<n $。这个算法通常**局部收敛**。
3. 算法尝试找出**使平方误差函数值最小的 k 个划分**。当簇是密集的、球状或团状的，且**簇与簇之间区别明显**时，聚类效果较好。
4. 虽然以局部最优结束，但一般情况下达到的局部最优已经可以满足聚类的需求。



#### 缺点

1. k-平均方法只有在簇的**平均值被定义**的情况下才能使用，且对有些分类属性的数据不适合。
2. 要求用户必须事先给出要生成的簇的数目$k$，这个数值与实际的类别数量可能不吻合。
3. **不适合于发现非凸面形状的簇，或者大小差别很大的簇**。
4. **K均值只能收敛到局部最优。**因为求解这个代价函数是个NP问题，采用的是贪心策略，所以只能通过多次迭代收敛到局部最优，而不是全局最优。
5. **K均值的效果受初始值和离群点的影响大。**因为 k 均值本质上是基于距离度量来划分的，均值和方差大的维度将对数据的聚类结果产生决定性的影响，因此需要进行归一化处理；此外，离群点或噪声对均值会产生影响，导致中心偏移，因此需要进行预处理。
6. 对于**数据簇分布差别较大的情况聚类效果很差**。例如一个类别的样本数是另一类的100倍。
7. 样本只能被划分到一个单一的类别中。



### 6. k-means++算法

由于 k-means 算法中，初始K值是人为地凭借经验来设置的，聚类的结果可能不符合实际的情况。因此，K值的设置对算法的效果影响其实是很大的，那么，如何设置这个K值才能取得更好的效果呢？

**k-means++算法的主要思想：**

● 首先随机选取1个初始聚类中心（n=1）。

● 假设已经选取了n个初始聚类中心（0<n<k），那么在选择第n+1个聚类中心时，距离当前已有的n个聚类中心越远的点越有可能被选取为第n+1个聚类中心。

● 可以这样理解上面的思想：聚类中心自然是越互相远离越好，即不同的类别尽可能地分开。



### 7. 如何选取k值？

如何才能合理地选取k值是k-means算法最大的问题之一，一般可以采取手肘法和`Gap Statistic`方法。

#### 手肘法

k值的选择一般基于经验或者多次实验来确定，手肘法便是如此，其主要思想是：**通过多次实验分别选取不同的k值，将不同k值的聚类结果对应的最小代价画成折线图，将曲线趋于平稳前的拐点作为最佳k值**。如下图所示：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/kmeans_fig2.png)

> 上图中，k取值在1~3时，曲线急速下降；当k>3时，曲线趋于平稳。因此，在k=3处被视为拐点，所以手肘法认为最佳的k值就是3。
>
> 然而，实际中很多时候曲线并非如同上图一样拐点处那么明显，因此单靠肉眼去分辨是很容易出错的。于是，就又有了一个改进的方法，可以不需要靠肉眼与分辨拐点，而是寻找某个最大值Gap(k)，具体如下。



#### Gap Statistic 方法

Gap Statistics 定义为：
$$
Gap(k)=E(logD_{k})-logD_{k} \\
其中，D_{k}是第k簇聚类对应的损失值，E(logD_{k})是logD_{k}的期望。
$$
对于上式的 $E(logD_{k})$，一般通过蒙特卡洛模拟产生。具体操作是：在样本所在的区域内，按照均匀分布随机产生和原样本数目一样的随机样本，计算这些随机样本的均值，得到一个 $D_{k}$，重复多次即可计算出 $E(logD_{k})$ 的近似值。

$Gap(k)$ 可以看做是**随机样本的损失与实际样本的损失之差**，假设实际样本最佳的簇类数目为 k，那么实际样本的损失应该相对较小，随机样本的损失与实际样本的损失的差值相应地达到最大，即**最大的 $Gap(k)$ 值应该对应最佳的k值。**

因此，我们只需要用不同的k值进行多次实验，找出使得$Gap(k)$最大的k即可。

> 到现在为止我们可以发现，上面的算法中，k值都是通过人为地凭借经验或者多次实验事先确定下来了的，但是当我们遇到高维度、海量的数据集时，可能就很难估计出准确的k值。那么，有没有办法可以帮助我们自动地确定k值呢？有的，下面来看看另一个算法。



### 8. ISODATA算法

ISODATA，全称是迭代自组织数据分析法，这种方法是针对传统 k-means 算法需要人为地预先确定k值的问题而改进的，其主要的思想是：

● 当某个类别样本数目过多、分散程度较大时，将该类别分为两个子类别。（分裂操作，即增加聚类中心数）

● 当属于某个类别的样本数目过少时，把该类别去除掉。（合并操作，即减少聚类中心数）



**算法优点：** 可以自动寻找合适的k值。

**算法缺点：** 除了要设置一个参考聚类数量 $k_{0}$ 外，还需要指定额外的3个阈值，来约束上述的分裂和合并操作。具体如下：

1. 预期的聚类数目  $k_{0}$ 作为参考值，最终的结果在  $k_{0}$ 的一半到两倍之间。
2. 每个类的最少样本数目 $N_{min}$，若分裂后样本数目会少于该值，则该簇不会分裂。
3. 最大方差  $Sigma$，用于控制某个簇的样本分散程度，操作该值且满足条件2，则分裂成两个簇。
4. 两个簇最小距离  $D_{min}$，若两个簇距离小于该值，则合并成一个簇。



### 9. 代码实现

代码参考自[K-Means Clustering](http://www.johnwittenauer.net/machine-learning-exercises-in-python-part-7/)。

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
@Time    : 2016/10/21 16:35
@Author  : cai

实现 K-Means 聚类算法
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.io import loadmat
import os

# 寻址最近的中心点
def find_closest_centroids(X, centroids):
    m = X.shape[0]
    k = centroids.shape[0]
    idx = np.zeros(m)

    for i in range(m):
        min_dist = 1000000
        for j in range(k):
            # 计算每个训练样本和中心点的距离
            dist = np.sum((X[i, :] - centroids[j, :]) ** 2)
            if dist < min_dist:
                # 记录当前最短距离和其中心的索引值
                min_dist = dist
                idx[i] = j

    return idx

# 计算聚类中心
def compute_centroids(X, idx, k):
    m, n = X.shape
    centroids = np.zeros((k, n))

    for i in range(k):
        indices = np.where(idx == i)
        # 计算下一个聚类中心，这里简单的将该类中心的所有数值求平均值作为新的类中心
        centroids[i, :] = (np.sum(X[indices, :], axis=1) / len(indices[0])).ravel()

    return centroids

# 初始化聚类中心
def init_centroids(X, k):
    m, n = X.shape
    centroids = np.zeros((k, n))
    # 随机初始化 k 个 [0,m]的整数
    idx = np.random.randint(0, m, k)

    for i in range(k):
        centroids[i, :] = X[idx[i], :]

    return centroids

# 实现 kmeans 算法
def run_k_means(X, initial_centroids, max_iters):
    m, n = X.shape
    # 聚类中心的数目
    k = initial_centroids.shape[0]
    idx = np.zeros(m)
    centroids = initial_centroids

    for i in range(max_iters):
        idx = find_closest_centroids(X, centroids)
        centroids = compute_centroids(X, idx, k)

    return idx, centroids

dataPath = os.path.join('data', 'ex7data2.mat')
data = loadmat(dataPath)
X = data['X']

initial_centroids = init_centroids(X, 3)
# print(initial_centroids)
# idx = find_closest_centroids(X, initial_centroids)
# print(idx)

# print(compute_centroids(X, idx, 3))

idx, centroids = run_k_means(X, initial_centroids, 10)
# 可视化聚类结果
cluster1 = X[np.where(idx == 0)[0], :]
cluster2 = X[np.where(idx == 1)[0], :]
cluster3 = X[np.where(idx == 2)[0], :]

fig, ax = plt.subplots(figsize=(12, 8))
ax.scatter(cluster1[:, 0], cluster1[:, 1], s=30, color='r', label='Cluster 1')
ax.scatter(cluster2[:, 0], cluster2[:, 1], s=30, color='g', label='Cluster 2')
ax.scatter(cluster3[:, 0], cluster3[:, 1], s=30, color='b', label='Cluster 3')
ax.legend()
plt.show()

# 载入一张测试图片，进行测试
imageDataPath = os.path.join('data', 'bird_small.mat')
image = loadmat(imageDataPath)
# print(image)

A = image['A']
print(A.shape)

# 对图片进行归一化
A = A / 255.

# 重新调整数组的尺寸
X = np.reshape(A, (A.shape[0] * A.shape[1], A.shape[2]))
# 随机初始化聚类中心
initial_centroids = init_centroids(X, 16)
# 运行聚类算法
idx, centroids = run_k_means(X, initial_centroids, 10)

# 得到最后一次的最近中心点
idx = find_closest_centroids(X, centroids)
# map each pixel to the centroid value
X_recovered = centroids[idx.astype(int), :]
# reshape to the original dimensions
X_recovered = np.reshape(X_recovered, (A.shape[0], A.shape[1], A.shape[2]))

# plt.imshow(X_recovered)
# plt.show()
```

完整代码例子和数据可以查看[Kmeans练习代码](https://github.com/ccc013/CodingPractise/blob/master/Python/MachineLearning/kMeansPractise.py)。



------

## GMM(高斯混合模型)

参考：

- [29_k-means和GMM的区别与联系](https://github.com/GYee/CV_interviews_Q-A/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/29_k-means%E5%92%8CGMM%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB.md)

- 李航--《统计学习方法》

- [K-means算法和高斯混合模型的异同](https://blog.csdn.net/qq_38150441/article/details/80498590) https://blog.csdn.net/qq_38150441/article/details/80498590

  

 定义：高斯混合模型是指具有如下形式的概率分布模型：
$$
P(x|\theta)=\sum_{k=1}^{K} \alpha_{k} \phi (x|\theta_{k})\\
其中，\alpha_{k}是高斯混合系数，\alpha_{k} \geq 0 \ 且\sum_{k=1}^{K}\alpha_{k}=1;
\theta_{k}=(\mu_{k},\sigma_{k}^{2});\\
\phi (x|\theta_{k})是第k个高斯分布模型的概率密度函数，具体形式如下：\\
\phi (x|\theta_{k})=\frac{1}{\sqrt {2\pi} \sigma_{k}} exp\left ( -\frac{(y-\mu_{k})^{2}}{2\sigma_{k}^{2}} \right )
$$

### 1. GMM聚类

高斯混合模型（GMM）聚类的思想和 k-means 其实有点相似，都是通过**迭代的方式将样本分配到某个簇类中，然后更新簇类的信息**，不同的是GMM是**基于概率模型**来实现的，而 k-means 是非概率模型，采用欧氏距离的度量方式来分配样本。



#### GMM聚类主要思想和流程

每个GMM由K个混合成分组成，每个混合成分都是一个高斯分布，$\alpha_{k}$ 为相应的混合系数。GMM模型假设所有的样本都根据高斯混合分布生成，那么每个高斯分布其实就代表了一个簇类。具体流程如下：

1. 先初始化高斯混合模型的参数 $\{(\alpha_{k},\mu_{k},\sigma_{k}^{2})\ | \ 1\leq k \leq K \ \}$ ，训练一个GMM模型需要估计这些参数，如何估计后面会介绍。
2. 对每个样本，固定各个高斯分布，计算样本在各个高斯分布上的概率（即该样本是由某个高斯分布生成而来的概率）。
3. 然后固定样本的生成概率，更新参数以获得更好的高斯混合分布。
4. 迭代至指定条件结束。



> 上面的1-4给出了GMM算法的大致思想，虽然简略了一些，但对比k-means算法的思想一起来看应该也很容易理解。
>
> ● k-means初始化K个均值，GMM初始化K个高斯分布和相应的混合系数；
>
> ● k-means计算样本到各个簇中心的欧氏距离并选择最小距离来划分样本应该属于哪个簇类，而GMM给出的是样本由某个高斯分布生成而来的概率，比如有80%的概率是由A分布生成的，有20%的概率是B分布生成的。这一点在医学诊断上很有意义，比如相比于k-means算法会很硬性地认为某位病人得了肿瘤或正常，GMM给出病人有51%的概率患有肿瘤这样的结果往往会更有参考意义。
>
> ● 两者都是采用迭代的方式来不断更新参数，以求得最优解（都是局部最优解）。



### 2. EM算法估计GMM参数

上面提到，要训练一个GMM模型，就需要估计每个高斯分布的参数 $\{(\alpha_{k},\mu_{k},\sigma_{k}^{2})\ | \ 1\leq k \leq K \ \}$，才能知道每个样本是由哪个高斯混合成分生成的，也就是说，数据集的所有样本是可观测数据， $\{(\alpha_{k},\mu_{k},\sigma_{k}^{2})\ | \ 1\leq k \leq K \ \}$ 这些是待观测数据(隐变量)，而估计待观测数据常用的算法就是EM算法。

下面给出EM算法估计高斯混合模型的参数的步骤，详细的推导过程可以参考《统计学习方法》第9.3节的内容：

1. 给定数据集$D=\{x_{1},x_{2},...,x_{m} \}$，初始化高斯混合分布的模型参数 $\{(\alpha_{k},\mu_{k},\sigma_{k}^{2})\ | \ 1\leq k \leq K \ \}$。

2. **E步：**遍历每个样本，对每个样本 $x_{i}$，计算其属于第k个高斯分布的概率：
   $$
   \gamma_{ik}=\frac{\alpha_{k}\phi(x_{i}|\theta_{k})}{\sum_{k=1}^{K}\alpha_{k}\phi(x_{i}|\theta_{k})} \ ,\quad 其中，\theta_{k}=(\mu_{k},\sigma_{k}^{2})
   $$

3. **M步：**更新各个高斯分布的参数为$\{(\hat{\alpha}_{k},\hat{\mu}_{k},\hat{\sigma}_{k}^{2})\ | \ 1\leq k \leq K \ \}$ :
   $$
   \hat{\alpha}_{k}=\frac{\sum_{i=1}^{m} \gamma_{ik} x_{i}}{\sum_{i=1}^{m} \gamma_{ik}} \\
   \hat{\mu}_{k}=\frac{\sum_{i=1}^{m} \gamma_{ik} (x_{i}-\mu_{k})^{2}}{\sum_{i=1}^{m} \gamma_{ik}} \\
   \hat{\sigma}_{k}^{2}=\frac{\sum_{i=1}^{m} \gamma_{ik}}{m}
   $$

4. 重复2-3步，直至收敛。

> 注意，EM算法通过迭代的方式估计GMM模型的参数，得到的是**局部最优解**而不是全局最优。



在了解了EM算法后，让我们再来看看高斯混合聚类是怎么操作的吧。。。

**在迭代收敛后，遍历所有的样本，对于每个样本 $x_{i}$，计算它在各个高斯分布中的概率，将样本划分到概率最大的高斯分布中**（每个高斯分布都相当于是一个簇类，因此可以理解为是将每个样本划分到相应的类别中，不过实际上是给出属于每个类别的概率而非属于某个类别）。



### 3. k-means和GMM算法的区别与联系

终于要回到正题了，不过相信从上面的分析看下来，应该对这两种算法的区别与联系已经有了大致理解了吧，下面就再来总结一下：

#### 区别

1. k-means算法是非概率模型，而GMM是概率模型。

> 具体来讲就是，k-means算法基于欧氏距离的度量方式来将样本划分到与它距离最小的簇类，而GMM则是计算由各个高斯分布生成样本的概率，将样本划分到取得最大概率的高斯分布中。

2. 两者需要计算的参数不同。

> k-means计算的是簇类的均值，GMM计算的是高斯分布的参数（即均值、方差和高斯混合系数）

3. k-means是硬聚类，要么属于这一类要么属于那一类；而GMM算法是软聚类，给出的是属于某些类别的概率。

4. GMM每一步迭代的计算量比k-means要大。



#### 联系

1. 都是聚类算法 
2. 都需要指定K值，且都受初始值的影响。k-means 初始化 k 个聚类中心，GMM初始化 k 个高斯分布。
3. 都是通过迭代的方式求解，而且都是局部最优解。k-means 的求解过程其实也可以用 EM 算法的 E 步和 M 步来理解。



------







