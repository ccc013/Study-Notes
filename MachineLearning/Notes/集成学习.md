# 集成学习

参考：

- [Boosting原理](https://www.jianshu.com/p/11083abc5738)
- [提升方法(boosting)详解](https://www.cnblogs.com/linyuanzhou/p/5019166.html) 
- [Bagging和Boosting的区别](https://www.cnblogs.com/earendil/p/8872001.html)
- [为什么说bagging是减少variance，而boosting是减少bias?](https://www.zhihu.com/question/26760839/answer/40337791)



集成学习通过训练多个分类器，然后将其组合起来，从而达到更好的预测性能，提高分类器的泛化能力。

目前集成学习有３个主要框架：

- bagging
- boosting
- stacking。



------

## 1. bagging套袋法

bagging是并行集成学习方法的最著名代表，其算法过程如下：

1. 从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）
2. 每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）
3. 对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

![bagging](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/bagging_fig.png)

bagging对训练数据集的采样使用的是 **bootstrap 自助采样法**，因此这里先对这个方法进行简单介绍：

>给定包含 m 个样本的数据集 $D$ ，我们对它进行采样产生数据集 $D'$ ：每次随机从 $D$ 中挑选一个样本，将其拷贝放入 $D'$ ，然后再将该样本放回初始数据集 $D$ 中，使得该样本在下次采样时仍有可能被采到；这个过程反复执行 m 次后，我们就得到了包含 m 个样本的数据集 $D'$ ，这就是自助采样法的结果。显然，$D$ 中有一部分样本会在 $D'$ 中多次出现，而另一部分样本不出现，可以做个简单的统计，样本在 m 次采样中始终不被采到的概率是 $(1-\frac{1}{m})^m$ ，取极限得到：
>$$
>\lim_{m \rightarrow \infty}(1-\frac{1}{m})^m = \frac{1}{e} \approx 0.368
>$$

照上面的自助采样法，我们可以采样出 T 个含有 m 个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合，这便是 bagging 方法的基本流程。**在对预测进行结合时，Bagging 通常对分类任务使用简单投票法，对回归任务使用简单平均法。**

<u>bagging方法之所以有效，是因为并非所有的分类器都会产生相同的误差，只要有不同的分类器产生的误差不同，就会对减小泛化误差有效。</u>

**与 Adaboost 的区别：**

>标准 AdaBoost 只适用于二分类任务，而 Bagging 能不经修改地用于多分类与回归任务。



### Bagging 与 Dropout 的联系

dropout 思想继承自 bagging方法。bagging是每次训练一个基分类器的时候，都有一些样本对该基分类器不可见，而dropout是每次训练的时候，都有一些神经元对样本不可见。

我们可以把 dropout 类比成将许多大的神经网络进行集成的一种 bagging 方法。但是每一个神经元的训练是非常耗时和占用内存的，训练很多的神经网络进行集合分类就显得太不实际了，但是 dropout可以看做是训练所有子网络的集合，这些子网络通过去除整个网络中的一些神经元来获得。

**dropout 具体怎么去除一个神经元呢？**可以在每个神经元结点处独立采样一个二进制掩膜，采样一个掩膜值为 0 的概率是一个固定的超参数，则掩膜值为 0 的被去除，掩膜值为 1 的正常输出。



#### bagging与dropout训练的对比

- 在bagging中，所有的分类器都是独立的，而在dropout中，所有的模型都是共享参数的。
- 在bagging中，所有的分类器都是在特定的数据集下训练至收敛，而在dropout中没有明确的模型训练过程。网络都是在一步中训练一次（输入一个批次样本，随机训练一个子网络）

#### dropout的优势

- very computationally cheap。在dropout训练阶段，每一个样本每一次更新只需要O(n) ，同时要生成n个二进制数字与每个状态相乘。除此之外，还需要O(n)的额外空间存储这些二进制数字，直到反向传播阶段。
- 没有很显著的限制模型的大小和训练的过程。





## 2. boosting(提升方法)

俗话说“三个臭皮匠顶个诸葛亮”，对于一个复杂任务来说，将多个专家的判断进行适当的综合所得出的判断，要比其中任何一个专家单独的判断好。

 Leslie Valiant首先提出了“**强可学习**（strongly learnable）”和”**弱可学习**（weakly learnable）”的概念，并且指出：在概率近似正确（probably  approximately correct,  PAC）学习的框架中，一个概念（一个类），如果存在一个多项式的学习算法能够学习它，并且正确率很高，那么就称这个概念是强可学习的，如果正确率不高，仅仅比随即猜测略好，那么就称这个概念是弱可学习的。2010年的图灵奖给了L. Valiant，以表彰他的PAC理论。非常有趣的是Schapire后来证明强可学习与弱可学习是等价的，也就是说，在PAC学习的框架下，一个概念是强可学习的充要条件是这个概念是可学习的。

这样一来，问题便成为，在学习中，如果已经发现了“弱学习算法”，那么能否将它提升（boost）为”强学习算法”。大家知道，发现弱学习算法通常比发现强学习算法容易得多。那么如何具体实施提升，便成为开发提升方法时所要解决的问题。

对于分类问题而言，给定一个训练数据，求一个比较粗糙的分类器（即弱分类器）要比求一个精确的分类器（即强分类器）容易得多。

提升方法就是从弱学习算法出发，反复学习，得到一系列弱分类器，然后组合这些弱分类器，构成一个强分类器。

**大多数的提升方法都是改变训练数据的概率分布（训练数据中的各个数据点的权值分布），调用弱学习算法得到一个弱分类器，再改变训练数据的概率分布，再调用弱学习算法得到一个弱分类器，如此反复，得到一系列弱分类器**。

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/boosting_fig.png" alt="boosting" style="zoom:67%;" />

这样，对于提升方法来说，有两个问题需要回答：

1. 是在每一轮如何改变训练数据的概率分布

2. 是如何将多个弱分类器组合成一个强分类器。

关于第一个问题，AdaBoost的做法是，**提高那些被前几轮弱分类器线性组成的分类器错误分类的的样本的权值**。这样一来，那些没有得到正确分类的数据，由于权值加大而受到后一轮的弱分类器的更大关注。于是，分类问题被一系列的弱分类器”分而治之”。

至于第二个问题，AdaBoost采取**加权多数表决**的方法。具体地，**加大分类误差率小的弱分类器的权值**，使其在表决中起较大的作用，减小分类误差率大的弱分类器的权值，使其在表决中起较小的作用。



## 3. Bagging和Boosting的区别

**1）样本选择上：**

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

**2）样例权重：**

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

**3）预测函数：**

Bagging：所有预测函数的权重相等。

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

**4）并行计算：**

Bagging：各个预测函数可以并行生成

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

**5）偏差和方差：**

- Boosting从优化角度来看，是用 forward-stagewise 这种贪心法去最小化 loss 函数,由于采取的是串行优化的策略，**各子模型之间是强相关的，于是子模型之和并不能显著降低 variance**，而每一个新的分类器都在前一个分类器的预测结果上改进，力求预测结果接近真实值，所以说 boosting 主要还是靠降低 bias 来提升预测精度。

- 对于 Bagging，则是**通过降低方差**
  - Bagging对样本重采样，对每一重采样得到的子样本集训练一个模型，最后取平均。由于子样本集的相似性以及使用的是同种模型，因此各模型有近似相等的bias和variance（事实上，各模型的分布也近似相同，但不独立）。
  - 由于 $E[\frac{\sum X_i}{n}] = E[X_i]$ ，所以bagging后的bias和单个子模型的接近，一般来说不能显著降低bias。
  - 另一方面，若各子模型独立，则有 $Var(\frac{\sum X_i}{n}) = \frac{Var(X_i)}{n}$ ，此时可以显著降低variance。若各子模型完全相同，则 $Var(\frac{\sum X_i}{n}) = Var(X_i)$ ，此时不会降低variance。bagging方法得到的各子模型是有一定相关性的，属于上面两个极端状况的中间态，因此可以一定程度降低variance。
  - 为了进一步降低variance，Random forest **通过随机选取特征子集，进一步减少了模型之间的相关性**，从而使得variance进一步降低。





## 4. stacking 模型融合

stacking 就是当用初始训练数据学习出若干个基学习器后，将这几个学习器的预测结果作为新的训练集，来学习一个新的学习器。stacking在深度学习大数据比赛中经常被使用，大概流程可以看看[这篇博客](https://www.jianshu.com/p/59313f43916f)。

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/stacking_fig.png" alt="stacking" style="zoom:60%;" />





