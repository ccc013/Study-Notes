场景文本检测和识别的学习笔记。

# 简介

**OCR**（Optical Character Recognition, 光学字符识别）传统上指对输入扫描文档图像进行分析处理，识别出图像中文字信息。

场景文字识别（Scene Text Recognition，STR） 指**识别自然场景图片中的文字信息**。自然场景图像中的文字识别，其难度远大于扫描文档图像中的文字识别，因为它的文字展现形式极其丰富，存在以下这些情况：


- 允许多种语言文本混合，字符可以有不同的大小、字体、颜色、亮度、对比度等。
- 文本行可能有横向、竖向、弯曲、旋转、扭曲等式样。
- 图像中的文字区域还可能会产生变形(透视、仿射变换)、残缺、模糊等现象。
- 自然场景图像的背景极其多样。如文字可以出现在平面、曲面或折皱面上；文字区域附近有复杂的干扰纹理、或者非文字区域有近似文字的纹理，比如沙地、草丛、栅栏、砖墙等。

![本图摘自新浪微博《光学字符识别技术：让电脑像人一样阅读》](/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/OCR_img1.jpg)

目前文字检测和识别技术的应用场景非常广泛，包括名片、身份证、车牌、路牌、试卷、票据等等识别的情况。

在传统的方案中，一般是分别训练文字检测和文本识别两个模型，然后在服务阶段，将两个模型串联起来，即将输入的图片先传入检测模型得到检测框，然后将检测框和输入图片一起传到文字识别模型，然后得到识别的结果，返回识别结果和检测结果。

而在最新的解决方案，主要是考虑训练一个端到端的模型，即将检测和识别同时集成在一个模型中，直接输入图片，经过模型，然后输出检测到的文字位置和识别的文字。这种方案的优势在于训练模型效率更高、服务阶段资源开销更少，但一般单模型的精度不如分开训练检测和识别模型的精度。



------

# 模型基础

## 基础网络(backbone network)

一般基础网络都是一些常用的分类网络，比如 AlexNet、VGGNet、ResNet、InceptionNet、DenseNet、Inside-Outside Net、Se-Net 等。





## 专用网络

在分类网络模型中也有一些应用在特定场景的专业网络模型，比如擅长提取图像细节特征的 FCN 网络，擅长做图形矫正的 STN 网络等。



### 全卷积网络（FCN）

全卷积网络（FCN, Fully Convolutional Network），是在传统的 CNN 基础上，将全连接层替换为卷积层的网络，最初是应用在语义分割任务中。



#### 优点

1. **通过反卷积(deconvolution)、上池化(unpooling)等上采样(upsampling)操作，将特征矩阵恢复到接近原图尺寸，然后对每个像素做类别预测，从而可以识别出更清晰的物体边界**。

2. **不需要通过候选区域来回归出物体边框**，而是直接根据高分辨率的特征图直接预测物体边框。

3. 因为不需要像 Faster-RCNN 在训练前定义好候选框的长宽比例，FCN 在预测不规则物体边界的时候更加鲁棒。
4. FCN 最后一层特征图的像素分辨率较高，图文识别任务需要依赖清晰的文字笔画来区分不同字符（特别是汉字），所以 FCN 网络很适合用来提取文字特征。





#### 缺点





### 空间变换网络（STN）

空间变换网络（STN, Spatital Transformer Networks）的作用是**对输入特征图片进行空间位置矫正**，得到输出特征图，这个矫正过程可以进行梯度传导，从而能够**支持端到端的模型训练**。

STN的网络结构如下所示，由三个部分组成，定位网络、网格生成器和采样器。定位网络根据原始特征图 U 计算出一套控制参数，网格生成器根据这套控制参数产生采样网格，采样器则根据采用网格核函数将原始图 U 中像素对应采样到目标图 V 中。

![](/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/STN_img.jpg)

空间变换的控制参数是**根据原始特征图 U 动态生成的**，而**生成空间变换控制参数的元参数则是在模型训练阶段学习得到的，并存放在定位网络的权重矩阵中**。



## 检测网络框架

常用的检测网络框架包括：

- RCNN 系列，包括 RCNN、Fast RCNN、Faster RCNN 等
- YOLO 系列
- SSD



### Faster RCNN

Faster RCNN 的网络结构如下所示

![](/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/FasterRCNN.jpg)

其特点有这几个：

1. 它在 Fast RCNN 的基础上加入了**区域建议网络（RPN，Region Proposal Network)**，加快产生和目标物体大小接近的多个候选区域参考框（anchor）；
2. 通过 ROI(Region of Interest) Pooling 层为多种尺寸参考框生成**归一化固定尺寸的区域特征**；
3. 利用共享的 CNN 同事向 RPN 和 ROI Pooling 层输入特征映射（Feature Maps），从而**减少卷积层参数量和计算量**。
4. 采用多目标损失函数，包括 RPN 网络、ROI Pooling 层的边框分类 loss 和坐标回归 loss，通过 loss 的梯度反向传播，可以调节候选框的坐标，并增大它和标注对象边框的交并比（IOU, Intersection over Union)。



需要注意的是，RPN 网络生成的候选框初始值有固定位置和长宽比例，所以如果初始的设置和图像中物体形状差别很大，就很难通过回归找到一个紧凑包围它的边框。







### SSD

SSD（Single Shot MultiBox Detector），是2016年提出的一种全卷积目标检测算法，其网络结构如下所示：

![](/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/SSD.jpg)

其特点有这几个：

1. 速度快，对比 Faster RCNN 有明显的速度优势；
2. SSD 是 one stage 算法，也就是直接输出目标对象的边框和得分；
3. 利用的是多尺度思想，在不同尺度的特征图输出和目标物体长宽比例接近的多个默认框，然后进行回归和分类，最后采用非极大值抑制（NMS）得到最终的检测结果；
4. 训练过程采用 Hard negative mining 策略进行训练，保存正负样本比是 1：3，并采用多种数据增强方法进行训练，提升模型性能。



## 文本检测模型

相比于常规物体，文本检测的难度有这几个：


- 相比于常规物体，文字行长度、长宽比例变化范围很大。
- 文本行是有方向性的。常规物体边框BBox的四元组描述方式信息量不充足。
- 自然场景中某些物体局部图像与字母形状相似，如果不参考图像全局信息将有误报。
- 有些艺术字体使用了弯曲的文本行，而手写字体变化模式也很多。
- 由于丰富的背景图像干扰，手工设计特征在自然场景文本识别任务中不够鲁棒。

这都是不能直接套用上述检测网络框架（SSD, YOLO, Faster RCNN）的原因，因此，需要结合文本的特点，设计可以解决上述文本检测问题的算法，目前的算法主要是从特征提取、区域建议网络（RPN）、多目标协同训练、非极大值抑制、半监督学习等方面来修改目标检测网络，从而提升了文本检测的性能。

下面是近年来常用的文本检测算法：


- CTPN方案中，用BLSTM模块提取字符所在图像上下文特征，以提高文本块识别精度。
- RRPN等方案中，文本框标注采用BBOX +方向角度值的形式，模型中产生出可旋转的文字区域候选框，并在边框回归计算过程中找到待测文本行的倾斜角度。
- DMPNet等方案中，使用四边形（非矩形）标注文本框，来更紧凑的包围文本区域。
- SegLink  将单词切割为更易检测的小文字块，再预测邻近连接将小文字块连成词。
- TextBoxes等方案中，调整了文字区域参考框的长宽比例，并将特征层卷积核调整为长方形，从而更适合检测出细长型的文本行。
- FTSN方案中，作者使用Mask-NMS代替传统BBOX的NMS算法来过滤候选框。
- WordSup方案中，采用半监督学习策略，用单词级标注数据来训练字符级文本检测模型。



### CTPN

2016 年的论文《Detecting Text in Natural Image with Connectionist Text Proposal Network》

目前流传最广、影响最大的开源文本检测模型，可以检测**水平或者微斜的文本行**。

CTPN 的预测流传是这样的，输入图片是先经过 VGG16 进行提取每个字符的局部图像特征，然后是通过 BLSTM 层提取字符序列的上下文特征，最后经过 FC 全连接层，然后经过预测分支输出各个文字块的坐标值和分类结果概率值。最后在后处理阶段，将合并相邻的小文字块为文本行。



### RRPN

2017 年的论文《Arbitrary-Oriented Scene Text Detection via Rotation Proposals》

基于**旋转区域候选网络**（RRPN, Rotation Region Proposal Networks）的方案，将**旋转因素并入经典区域候选网络**（如Faster RCNN）

这种方案中，一个文本区域的ground truth被表示为**具有5元组(x,y,h,w,θ)的旋转边框**, 坐标(x,y)表示边框的几何中心, 高度h设定为边框的短边，宽度w为长边，方向是长边的方向。

训练时，首先生成含有文本方向角的倾斜候选框，然后在边框回归过程中学习文本方向角。

RRPN中方案中提出了旋转感兴趣区域（RRoI，Rotation Region-of-Interest）池化层，**将任意方向的区域建议先划分成子区域，然后对这些子区域分别做max pooling、并将结果投影到具有固定空间尺寸小特征图上**。



### FTSN

2017 年的论文《Fused Text Segmentation Networks for Multi-oriented Scene Text Detection》

FTSN（Fused Text Segmentation Networks）模型**使用分割网络支持倾斜文本检测**。它使用Resnet-101做基础网络，使用了**多尺度融合**的特征图。

标注数据包括文本实例的像素掩码和边框，使用像素预测与边框检测多目标联合训练。

![](/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/FTSN_1.jpg)

基于文本实例间像素级重合度的Mask-NMS， 替代了传统基于水平边框间重合度的NMS算法。下图左边子图是传统NMS算法执行结果，中间白色边框被错误地抑制掉了。下图右边子图是Mask-NMS算法执行结果， 三个边框都被成功保留下来。

![](/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/FTSN_2.jpg)



### DMPNet

2017年的论文《Deep Matching Prior Network: Toward Tighter Multi-oriented Text Detection》

DMPNet（Deep Matching Prior Network）中，**使用四边形（非矩形）来更紧凑地标注文本区域边界，其训练出的模型对倾斜文本块检测效果更好**。

如下图所示，它使用**滑动窗口在特征图上获取文本区域候选框**，候选框既有正方形的、也有倾斜四边形的。接着，使用基于像素点采样的Monte-Carlo方法，来快速计算四边形候选框与标注框间的面积重合度。然后，计算四个顶点坐标到四边形中心点的距离，将它们与标注值相比计算出目标loss。

文章中推荐用Ln loss来取代L1、L2 loss，从而对大小文本框都有**较快的训练回归（regress）速度**。

![](/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/DMPNet.jpg)



### EAST

2017 年的论文《EAST: An Efficient and Accurate Scene Text Detector》

EAST（Efficient and Accuracy Scene Text detection pipeline）模型中，首先**使用全卷积网络（FCN）生成多尺度融合的特征图**，然后在此基础上直接进行像素级的文本块预测。

该模型中，支持**旋转矩形框、任意四边形两种文本区域标注形式**。对应于四边形标注，模型执行时会对特征图中每个像素预测其到四个顶点的坐标差值。对应于旋转矩形框标注，模型执行时会对特征图中每个像素预测其到矩形框四边的距离、以及矩形框的方向角。

根据开源工程中预训练模型的测试，**该模型检测英文单词效果较好、检测中文长文本行效果欠佳**。或许，根据中文数据特点进行针对性训练后，检测效果还有提升空间。

上述过程中，省略了其他模型中常见的区域建议、单词分割、子块合并等步骤，因此**该模型的执行速度很快**。



### SegLink

2017 年的论文《Detecting Oriented Text in Natural Images by Linking Segments》

SegLink模型的标注数据中，先将每个单词切割为更易检测的有方向的小文字块（segment），然后用邻近连接（link ）将各个小文字块连接成单词。**这种方案方便于识别长度变化范围很大的、带方向的单词和文本行**，它不会象Faster-RCNN等方案因为候选框长宽比例原因检测不出长文本行。相比于CTPN等文本检测模型，**SegLink的图片处理速度快很多**。



### PixelLink 模型

2018 年论文《Detecting Scene Text via Instance Segmentation》

**自然场景图像中一组文字块经常紧挨在一起**，通过语义分割方法很难将它们识别开来，所以 PixelLink 模型尝试用**实例分割**方法解决这个问题。

该模型的特征提取部分，为**VGG16基础上构建的FCN网络**。模型执行流程如下图所示。首先，借助于CNN 模块执行两个像素级预测：一个文本二分类预测，一个链接二分类预测。接着，用正链接去连接邻居正文本像素，得到文字块实例分割结果。然后，由分割结果直接就获得文字块边框， 而且允许生成倾斜边框。

上述过程中，**省掉了其他模型中常见的边框回归步骤，因此训练收敛速度更快些**。

训练阶段，使用了平衡策略，使得每个文字块在总LOSS中的权值相同。训练过程中，通过预处理增加了各种方向角度的文字块实例。



### **Textboxes/Textboxes++模型**

2016 年的论文《TextBoxes: A Fast Text Detector with a Single Deep Neural Network》

2018 年的论文《TextBoxes++: A Single-Shot Oriented Scene Text Detector》

Textboxes 是基于SSD框架的图文检测模型，**训练方式是端到端的，运行速度也较快**。

为了**适应文字行细长型的特点**，候选框的长宽比增加了1,2,3,5,7,10这样初始值，特征层也用长条形卷积核代替了其他模型中常见的正方形卷积核。

为了防止漏检文本行，还在**垂直方向增加了候选框数量**。为了检测大小不同的字符块，在多个尺度的特征图上并行预测文本框， 然后对预测结果做NMS过滤。

Textboxes++ 是Textboxes的升级版本，**目的是增加对倾斜文本的支持**。为此，将标注数据改为了旋转矩形框和不规则四边形的格式；对候选框的长宽比例、特征图层卷积核的形状都作了相应调整。



### WordSup

2017 年的论文《WordSup： Exploiting Word Annotations for Character based Text Detection》

如下图所示，在数学公式图文识别、不规则形变文本行识别等应用中，**字符级检测模型是一个关键基础模块**。由于字符级自然场景图文标注成本很高、相关公开数据集稀少，导致现在多数图文检测模型只能在文本行、单词级标注数据上做训练。

WordSup提出了一种弱监督的训练框架， 可以在文本行、单词级标注数据集上训练出字符级检测模型。

![](/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/WordSup_1.jpg)

如下图所示，WordSup弱监督训练框架中，两个训练步骤被交替执行：

- 给定当前字符检测模型，并结合单词级标注数据，计算出字符中心点掩码图；
- 给定字符中心点掩码图，有监督地训练字符级检测模型.

![](/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/WordSup_2.jpg)

如下图，训练好字符检测器后，可以在数据流水线中加入合适的文本结构分析模块，以输出符合应用场景格式要求的文本内容。该文作者例举了多种文本结构分析模块的实现方法。

![](/Users/luocai/Nutstore Files/Study-Notes/MachineLearning/Notes/images/WordSup_3.jpg)





## 文本识别模型

文本识别就是从检测到的文本区域识别出文本内容。

### CRNN

2015 年的论文《An End-to-End Trainable Neural Network for Image-based Sequence Recognition and Its Application to Scene Text Recognition》

CRNN(Convolutional Recurrent Neural Network）是目前较为流行的图文识别模型，可识别较长的文本序列。**它包含CNN特征提取层和BLSTM序列特征提取层，能够进行端到端的联合训练**。 它利用BLSTM和CTC部件学习字符图像中的上下文关系， 从而有效提升文本识别准确率，使得模型更加鲁棒。预测过程中，前端使用标准的CNN网络提取文本图像的特征，利用BLSTM将特征向量进行融合以提取字符序列的上下文特征，然后得到每列特征的概率分布，最后通过转录层(CTC rule)进行预测得到文本序列。



### RARE

2016 年的论文《Robust Scene Text Recognition with Automatic Rectification》

RARE（Robust text recognizer with Automatic Rectification）模型在**识别变形的图像文本时效果很好**。

模型预测过程中，输入图像首先要被送到一个**空间变换网络**中做处理，矫正过的图像然后被送入序列识别网络中得到文本预测结果。

空间变换网络内部包含定位网络、网格生成器、采样器三个部件。经过训练后，它可以根据输入图像的特征图动态地产生空间变换网格，然后采样器根据变换网格核函数从原始图像中采样获得一个矩形的文本图像。

RARE中支持一种称为TPS（thin-plate splines）的空间变换，**从而能够比较准确地识别透视变换过的文本、以及弯曲的文本**。





## 端到端模型

端到端模型的目标是一站式直接从图片中定位和识别出所有文本内容来。

### FOTS Rotation-Sensitive Regression

2018 年的论文《FOTS: Fast Oriented Text Spotting with a Unified Network》

FOTS（Fast Oriented Text Spotting）是图像文本检测与识别同步训练、端到端可学习的网络模型。检测和识别任务共享卷积特征层，既节省了计算时间，也比两阶段训练方式学习到更多图像特征。引入了旋转感兴趣区域（RoIRotate）, 可以从卷积特征图中产生出定向的文本区域，从而支持倾斜文本的识别.



### STN-OCR模型

2017 年的论文《STN-OCR: A single Neural Network for Text Detection and Text Recognition》

STN-OCR是集成了了图文检测和识别功能的端到端可学习模型。在它的检测部分**嵌入了一个空间变换网络（STN）**来对原始输入图像进行仿射（affine）变换。利用这个空间变换网络，可以对检测到的多个文本块分别执行旋转、缩放和倾斜等图形矫正动作，从而在后续文本识别阶段得到更好的识别精度。在训练上STN-OCR属于**半监督学习方法，只需要提供文本内容标注，而不要求文本定位信息**。

作者也提到，如果从头开始训练则网络收敛速度较慢，因此建议渐进地增加训练难度。STN-OCR已经开放了工程源代码和预训练模型。







------

# 数据集

常用的一些数据集

## Chinese Text in the Wild(CTW)

该数据集包含32285张图像，1018402个中文字符(来自于腾讯街景), 包含平面文本，凸起文本，城市文本，农村文本，低亮度文本，远处文本，部分遮挡文本。图像大小2048*2048，数据集大小为31GB。以(8:1:1)的比例将数据集分为训练集(25887张图像，812872个汉字)，测试集(3269张图像，103519个汉字)，验证集(3129张图像，103519个汉字)。

```text
文献链接：https://arxiv.org/pdf/1803.00085.pdf 
数据集下载地址：https://ctwdataset.github.io/
```



## Reading Chinese Text in the Wild(RCTW-17)

该数据集包含12263张图像，训练集8034张，测试集4229张，共11.4GB。大部分图像由手机相机拍摄，含有少量的屏幕截图，图像中包含中文文本与少量英文文本。图像分辨率大小不等。

```text
下载地址http://mclab.eic.hust.edu.cn/icdar2017chinese/dataset.html
文献：http://arxiv.org/pdf/1708.09585v2
```



## ICPR MWI 2018 挑战赛

大赛提供20000张图像作为数据集，其中50%作为训练集，50%作为测试集。主要由合成图像，产品描述，网络广告构成。该数据集数据量充分，中英文混合，涵盖数十种字体，字体大小不一，多种版式，背景复杂。文件大小为2GB。

```text
下载地址：
https://tianchi.aliyun.com/competition/information.htm?raceId=231651&_is_login_redirect=true&accounttraceid=595a06c3-7530-4b8a-ad3d-40165e22dbfe   
```



## Total-Text

该数据集共1555张图像，11459文本行，包含水平文本，倾斜文本，弯曲文本。文件大小441MB。大部分为英文文本，少量中文文本。训练集：1255张 测试集：300

```text
下载地址：http://www.cs-chan.com/source/ICDAR2017/totaltext.zip
文献：http:// arxiv.org/pdf/1710.10400v
```



## Google FSNS(谷歌街景文本数据集)

该数据集是从谷歌法国街景图片上获得的一百多万张街道名字标志，每一张包含同一街道标志牌的不同视角，图像大小为600*150，训练集1044868张，验证集16150张，测试集20404张。

```text
下载地址：http://rrc.cvc.uab.es/?ch=6&com=downloads
文献：http:// arxiv.org/pdf/1702.03970v1
```



## COCO-TEXT

该数据集，包括63686幅图像，173589个文本实例，包括手写版和打印版，清晰版和非清晰版。文件大小12.58GB，训练集：43686张，测试集：10000张，验证集：10000张

```text
文献: http://arxiv.org/pdf/1601.07140v2
下载地址：https://vision.cornell.edu/se3/coco-text-2/
```



## Synthetic Data for Text Localisation

在复杂背景下**人工合成的自然场景文本数据**。包含858750张图像，共7266866个单词实例，28971487个字符，文件大小为41GB。该合成算法，不需要人工标注就可知道文字的label信息和位置信息，可得到大量自然场景文本标注数据。

```text
下载地址：http://www.robots.ox.ac.uk/~vgg/data/scenetext/
文献：http://www.robots.ox.ac.uk/~ankush/textloc.pdf
Code: https://github.com/ankush-me/SynthText (英文版)
Code https://github.com/wang-tf/Chinese_OCR_synthetic_data(中文版)
```



## Synthetic Word Dataset

合成文本识别数据集，包含9百万张图像，涵盖了9万个英语单词。文件大小为10GB

```text
下载地址：http://www.robots.ox.ac.uk/~vgg/data/text/
```



## Caffe-ocr中文合成数据

数据利用中文语料库，通过字体、大小、灰度、模糊、透视、拉伸等变化随机生成，共360万张图片，图像分辨率为280x32，涵盖了汉字、标点、英文、数字共5990个字符。文件大小约为8.6GB

```text
下载地址：https://pan.baidu.com/s/1dFda6R3
```






------

# 参考文献

1. [自然场景文本检测识别技术综述](https://zhuanlan.zhihu.com/p/38655369)
2. FCN：Fully Convolutional Networks for Semantic Segmentation
3. STN：Spatial Transformer Networks
4. Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks
5. SSD: Single Shot MultiBox Detector
6. CTPN:Detecting Text in Natural Image with Connectionist Text Proposal Network
7. 









