简单学习并记录数学方面的知识点，对于深度学习来说，主要需要的数学包括这几个方面：

1. 线性代数
2. 概率论
3. 微积分



# 1.线性代数

## 1.1 向量和矩阵

### 1.1.1 标量、向量、矩阵、张量之间的联系

**标量（scalar）** 

一个标量表示一个单独的数，它不同于线性代数中研究的其他大部分对象（通常是多个数的数组）。我们用斜体表示标量。标量通常被赋予小写的变量名称。 一般会明确标量属于哪种类型，比如定义实数标量时，会说“令 $s\in R$ 表示一条线的斜率”。

**向量（vector）** 

一个向量表示一组有序排列的数。通过次序中的索引，我们可以确定每个单独的数。通常我们赋予向量粗体的小写变量名称，比如xx。向量中的元素可以通过带脚标的斜体表示。向量$X$的第一个元素是$X_1$，第二个元素是$X_2$，以此类推。我们也会注明存储在向量中的元素的类型（实数、虚数等）。

一个向量如下所示，一个向量可以看作空间中的点，即每个元素可以表示不同坐标轴上的坐标。
$$
x = 
 \left[
 \begin{matrix}
   x_1 \\
   x_2 \\
   x_3 \\
   \cdots \\
   x_n
  \end{matrix}
  \right] 
$$




**矩阵（matrix）** 

矩阵是具有相同特征和纬度的对象的集合，表现为一张二维数据表。其意义是一个对象表示为矩阵中的一行，一个特征表示为矩阵中的一列，每个特征都有数值型的取值。通常会赋予矩阵粗体的大写变量名称，比如$A$。

一个矩阵的表示例子如下所示：
$$
A = 
\left[
 \begin{matrix}
   A_{1,1} & A_{1,2} \\
   A_{2,1} & A_{2,2} \\
  \end{matrix}
  \right] 
$$

**转置**是矩阵的重要操作之一，其转置是以对角线为轴的镜像，这条从左上角到右下角的对角线被称为**主对角线**，定义如下:
$$
(A^T){i,j} = A_{j,i} 
$$
一个示例操作如下：
$$
A = 
\left[
 \begin{matrix}
   A_{1,1} & A_{1,2} \\
   A_{2,1} & A_{2,2} \\
   A_{3,1} & A_{3,2}
  \end{matrix}
  \right]
  ==> 
  A^T = 
  \left[
 \begin{matrix}
   A_{1,1} & A_{2,1} & A_{3, 1} \\
   A_{1,2} & A_{2,2} & A_{3,2}\\
  \end{matrix}
  \right] 
$$

从一个 $3\times 2$ 的矩阵变为了 $ 2\times 3$ 的矩阵。



**张量（tensor）** 

在某些情况下，我们会讨论坐标超过两维的数组。一般地，一个数组中的元素分布在若干维坐标的规则网格中，我们将其称之为张量。使用 $A$ 来表示张量“A”。张量$A$中坐标为$(i,j,k)$的元素记作$A_{(i,j,k)}$。 



**四者之间关系**  

（来自深度学习 500 问第一章数学基础）

> 标量是0阶张量，向量是一阶张量。举例：  
> ​标量就是知道棍子的长度，但是你不会知道棍子指向哪儿。  
> ​向量就是不但知道棍子的长度，还知道棍子指向前面还是后面。  
> ​张量就是不但知道棍子的长度，也知道棍子指向前面还是后面，还能知道这棍子又向上/下和左/右偏转了多少。





### 1.1.2 张量与矩阵的区别

- 从代数角度讲， 矩阵它是向量的推广。向量可以看成一维的“表格”（即分量按照顺序排成一排）， 矩阵是二维的“表格”（分量按照纵横位置排列）， 那么$n$阶张量就是所谓的$n$维的“表格”。 张量的严格定义是利用线性映射来描述。
- 从几何角度讲， 矩阵是一个真正的几何量，也就是说，它是一个不随参照系的坐标变换而变化的东西。向量也具有这种特性。
- 张量可以用3×3矩阵形式来表达。 
- 表示标量的数和表示向量的三维数组也可分别看作1×1，1×3的矩阵。 



### 1.1.3 矩阵和向量相乘结果   

若使用爱因斯坦求和约定（Einstein summation convention），矩阵$A$, $B$相乘得到矩阵 $C$ 可以用下式表示：
$$ AB = C ==> a_{ik}*b_{kj}=c_{ij}  $$ 

其中，$a_{ik}$, $b_{kj}$, $c_{ij}$分别表示矩阵$A, B, C$的元素，$k$出现两次，是一个哑变量（Dummy Variables）表示对该参数进行遍历求和。

用一个例子表示就是：
$$
A=
\left[
 \begin{matrix}
   A_{1,1} & A_{1,2} \\
   A_{2,1} & A_{2,2} \\
  \end{matrix}
  \right] \
  B = 
  \left[
 \begin{matrix}
   B_{1,1} & B_{1,2} \\
   B_{2,1} & B_{2,2} \\
  \end{matrix}
  \right] \\
  A \times B = C = 
  \left[
 \begin{matrix}
   A_{1,1}\times B_{1,1}+A_{1,2}\times B_{2,1} & A_{1,1}\times B_{1,2}+A_{1,2}\times B_{2,2} \\
   A_{2,1}\times B_{1,1}+A_{2,2}\times B_{2,1} & A_{2,1}\times B_{1,2}+A_{2,2}\times B_{2,2} \\
  \end{matrix}
  \right]
  = 
   \left[
 \begin{matrix}
   C_{1,1} & C_{1,2} \\
   C_{2,1} & C_{2,2} \\
  \end{matrix}
  \right]
$$
所以矩阵相乘有一个前提，**矩阵 A 的列数必须和矩阵 B 的行数相等**，也就是如果 A 的维度是 $m\times n$，B 的维度必须是 $n \times p$，相乘得到的 C 矩阵的维度就是 $m\times p$。

另外还有一种矩阵乘法，是矩阵对应元素相乘，这种称为**元素对应乘积，或者 Hadamard 乘积**，记为 A ⊙ B


而矩阵和向量相乘可以看成是矩阵相乘的一个特殊情况，例如：矩阵$B$是一个$n \times 1$的矩阵。



矩阵乘积满足这些定律：

1. 服从分配率：A(B+C) = AB + AC
2. 服从结合律：A(BC) = (AB)C

但是**不服从交换律**，即 AB 不一定等于 BA。

矩阵的乘积满足：$（AB)^T = A^TB^T$



两个相同维度的向量 x 和 y 的点积(dot product)，可以看作矩阵乘积--$x^Ty$。也就是说可以将矩阵乘积 $C=AB$ 中计算 $C_{i,j}$的步骤看作是 A 的第 i 行和 B 的第 j 列之间的点积。毕竟，矩阵的每一行或者每一列都是一个向量。

而向量的点积是满足交换律的：
$$
x^Ty = y^Tx 
$$
证明主要是根据：

1. 两个向量的点积是标量
2. 标量的转置也是自身

所以有：
$$
x^Ty = (x^Ty)^T = xy^T 
$$

### 1.1.4 单位矩阵和逆矩阵

单位矩阵的定义如下，**用 I 表示单位矩阵，任何向量和单位矩阵相乘，都不会改变**，即：
$$
\forall x \in R^n, I_n x = x \tag{1-1-8}
$$
单位矩阵的结构很简单，就是主对角线是 1，其他位置是 0，如下图所示的单位矩阵 $I_3$ ：
$$
\left[
 \begin{matrix}
   1 & 0 & 0 \\
   0 & 1 & 0 \\
   0 & 0 & 1
  \end{matrix}
  \right]
$$
而逆矩阵记作 $A^{-1}$，其满足如下条件：
$$
A^{-1}A=I_n 
$$



### 1.1.5 线性方程组和线性相关

现在有一个线性方程组，如下所示：
$$
Ax = b
$$
其中，$A\in R^{m\times n}$ 是已知的矩阵，$b\in R^m$ 是已知的向量，然后 $x\in R^n$ 是需要求解的未知向量。

这里根据矩阵相乘（x 相当于一个 $n\times 1$ 的矩阵），可以将上述公式拓展开来：
$$
A_{1,:}x = b_1 ==> A_{1,1}x_1 + A_{1,2}x_2+\cdots+A_{1,n}x_n = b_1 \\
A_{2,:}x = b_2 ==> A_{2,1}x_1 + A_{2,2}x_2+\cdots+A_{2,n}x_n = b_2 \\
\cdots \\
A_{m,:}x = b_m ==> A_{m,1}x_1 + A_{m,2}x_2+\cdots+A_{m,n}x_n = b_m \\
$$
在我们定义了逆矩阵后，那么可以这么求解：
$$
Ax=b\\
A^{-1}Ax = A^{-1}b\\
I_nx = A^{-1}b \\
x = A^{-1}b
$$
所以求解的关键就是是否存在一个逆矩阵，并找到它。

当逆矩阵$A^{-1}$存在的时候，对每个向量 b 肯定恰好存在一个解。

但对于方程组来说，向量 b 的某些值，有可能不存在解，或者有无限多个解，不存在多于1 个解，但有限解的情况，比如 x 和 y 都是方程组的解，则有：
$$
z = \alpha x + (1-\alpha)y
$$
其中，$\alpha$ 是任意实数，那么 z 也是方程组的解，这种组合是无限的，所以不存在有限解（多于 1 个）。

确定 Ax=b 是否有解，**关键是确定向量 b 是否在 A 列向量的生成子空间中**，这个特殊的生成子空间，被称为 A 的列空间或者 A 的值域。

> 一组向量的线性组合是指每个向量乘以对应标量系数之后的和，即 $\sum_i c_i v^{(i)}$
>
> 一组向量的生成子空间是原始向量线性组合后所能抵达的点的集合。

那么为了让上述成立，**应该让 A 的列空间构成整个 $R^m$ 空间**，如果这个空间某个点不在 A 的列空间，那么对应的 b 会使得方程无解。而要让其成立，**即要满足不等式 $n\ge m$ **。

但该不等式只是方程对每个 b 有解的必要条件，非充分条件。因为存在一种情况，某些列向量可能是冗余的，比如一个 $2\times 2$的矩阵，如果两个列向量都是相同的，那该矩阵的列空间和它的一个列向量作为矩阵的列空间是一样的，并不能满足覆盖了整个 $R^2$ 空间。

这种冗余也被称为**线性相关**，而**如果一组向量中任意一个向量都不能表示为其他向量的线性组合，则这组向量称为线性无关**。

所以，**如果一个矩阵的列空间要覆盖整个 $R^m$，那么该矩阵必须包含至少一组m 个线性无关的向量，这才是对每个 b 都有解的充分必要条件**。

此外，要**让矩阵可逆**，还必须保证 Ax=b 对每个 b 的取值至多只有一个解，那必须保证该矩阵至多有 m 个列向量，否则方程有不止一个解。

综上，**那么矩阵就必须是方阵，也就是 m = n，并且所有列向量都是线性无关的**。一个列向量都是线性无关的方阵被称为是**奇异的**。

假如 A 不是方阵或者不是奇异的方阵，也可能有解，但是不能通过逆矩阵去求解。






### 1.1.6 向量和矩阵的范数归纳  
**向量的范数(norm)** 

通常衡量向量的大小是通过**范数**来衡量的，形式上 $L^P$范数定义如下：

$$
L_p=\Vert\vec{x}\Vert_p=\sqrt[p]{\sum_{i=1}^{N}|{x_i}|^p}
$$

这里 $p\ge 1$。

范数是将向量映射到非负数的函数，直观上来说，向量 x 的范数衡量从原点到点 x 的距离。

范数是满足下列性质的任意函数：
$$
f(x)=0=>x=0 \\
f(x+y)\le f(x)+f(y)(三角不等式)\\
\forall \alpha \in R, f(\alpha x) = |\alpha|f(x)
$$




定义一个向量为：$\vec{a}=[-5, 6, 8, -10]$。任意一组向量设为$\vec{x}=(x_1,x_2,...,x_N)$。其不同范数求解如下：

- 向量的1范数：向量的各个元素的绝对值之和，上述向量$\vec{a}$的1范数结果就是：x = |-5|+|6|+|8|+|-10| = 29。
  
$$
\Vert\vec{x}\Vert_1=\sum_{i=1}^N\vert{x_i}\vert
$$

- 向量的2范数（欧几里得范数）：向量的每个元素的平方和再开平方根，上述$\vec{a}$的2范数结果就是：$x=\sqrt{(-5)^2+(6)^2+(8)^2+(-10)^2}15$。
  
$$
\Vert\vec{x}\Vert_2=\sqrt{\sum_{i=1}^N{\vert{x_i}\vert}^2}
$$

- 向量的负无穷范数：向量的所有元素的绝对值中最小的：上述向量$\vec{a}$的负无穷范数结果就是：5。  
  
$$
\Vert\vec{x}\Vert_{-\infty}=\min{|{x_i}|}
$$

- 向量的正无穷范数：向量的所有元素的绝对值中最大的：上述向量$\vec{a}$的正无穷范数结果就是：10。 
  
$$
\Vert\vec{x}\Vert_{+\infty}=\max{|{x_i}|}
$$





**矩阵的范数**  

定义一个矩阵。 
$$
A =
\left[
 \begin{matrix}
   -1 & 2 & -3 \\
   4 & -6 & 6 \\
  \end{matrix}
  \right]
$$


任意矩阵定义为：$A_{m\times n}$，其元素为 $a_{ij}$。

矩阵的范数定义为

$$
\Vert{A}\Vert_p :=\sup_{x\neq 0}\frac{\Vert{Ax}\Vert_p}{\Vert{x}\Vert_p}
$$

当向量取不同范数时, 相应得到了不同的矩阵范数。

- **矩阵的1范数（列范数）**：先对矩阵的每一列元素的绝对值求和，再从中取个最大的（列和最大），上述矩阵$A$的1范数先得到$[5,8,9]$，再取最大的最终结果就是：9。
$$
\Vert A\Vert_1=\max_{1\le j\le n}\sum_{i=1}^m|{a_{ij}}|
$$

- **矩阵的2范数**：矩阵$A^TA$的最大特征值开平方根，上述矩阵$A$的2范数得到的最终结果是：10.0623。 
  
$$
\Vert A\Vert_2=\sqrt{\lambda_{max}(A^T A)}
$$

其中， $\lambda_{max}(A^T A)$ 为 $A^T A$ 的特征值绝对值的最大值。
- **矩阵的无穷范数（行范数）**：矩阵的每一行上的元素绝对值先求和，再从中取个最大的，（行和最大），上述矩阵$A$的行范数先得到$[6；16]$，再取最大的最终结果就是：16。 
$$
\Vert A\Vert_{\infty}=\max_{1\le i \le m}\sum_{j=1}^n |{a_{ij}}|
$$

- **矩阵的核范数**：矩阵的奇异值（将矩阵svd分解）之和，这个范数可以用来低秩表示（因为最小化核范数，相当于最小化矩阵的秩——低秩），上述矩阵A最终结果就是：10.9287。  

- **矩阵的L0范数**：矩阵的非0元素的个数，通常用它来表示稀疏，L0范数越小0元素越多，也就越稀疏，上述矩阵$A$最终结果就是：6。
- **矩阵的L1范数**：矩阵中的每个元素绝对值之和，它是L0范数的最优凸近似，因此它也可以表示稀疏，上述矩阵$A$最终结果就是：22。  
- **矩阵的F范数**：最常用的矩阵的范数，矩阵的各个元素平方之和再开平方根，它通常也叫做矩阵的L2范数，**它的优点在于它是一个凸函数，可以求导求解，易于计算**，上述矩阵A最终结果就是：10.0995。  
$$
\Vert A\Vert_F=\sqrt{(\sum_{i=1}^m\sum_{j=1}^n{| a_{ij}|}^2)}
$$

- **矩阵的L21范数**：矩阵先以每一列为单位，求每一列的F范数（也可认为是向量的2范数），然后再将得到的结果求L1范数（也可认为是向量的1范数），很容易看出它是介于L1和L2之间的一种范数，上述矩阵$A$最终结果就是：17.1559。 
- **矩阵的 p范数** 
  
$$
\Vert A\Vert_p=\sqrt[p]{(\sum_{i=1}^m\sum_{j=1}^n{| a_{ij}|}^p)}
$$

两个向量的点积可以用范数来表示：
$$
x^Ty =\Vert x \Vert_2 \Vert y \Vert_2 cos\theta
$$
这里 $\theta$ 就是 x 和 y 之间的夹角。



### 1.1.7 一些特殊的矩阵和向量

**对角矩阵**：只在对角线上有非零元素，其他位置都是零。之前介绍的单位矩阵就是对角矩阵的一种；

**对称矩阵**：转置和自己相等的矩阵，即：$A = A^T$。

**单位向量**：具有单位范数的向量，也就是 $\Vert x \Vert_2 =1$



**向量正交**：如果 $x^Ty=0$，那么就说向量 x 和 y 互相正交。如果向量不仅互相正交，范数还是 1，那么就称为**标准正交**。



**正交矩阵**：行向量和列向量是分别标准正交的方阵，即
$$
A^TA=AA^T=I
$$
也就是有：
$$
A^{-1}=A^T
$$
所以正交矩阵的一个优点就是求逆计算代价小。



### 1.1.8 如何判断一个矩阵为正定

判定一个矩阵是否为正定，通常有以下几个方面：  

- 顺序主子式全大于0；  
- 存在可逆矩阵$C$使$C^TC$等于该矩阵；
- 正惯性指数等于$n$；
- 合同于单位矩阵$E$（即：规范形为$E$）
- 标准形中主对角元素全为正；
- 特征值全为正；
- 是某基的度量矩阵。



所有特征值是非负数的矩阵称为半正定，而所有特征值是负数的矩阵称为负定，所有特征值是非正数的矩阵称为半负定。

**正定性的用途**

- Hessian矩阵正定性在梯度下降的应用
  - 若Hessian正定,则函数的二阶偏导恒大于0，,函数的变化率处于递增状态，判断是否有局部最优解
- 在 svm 中核函数构造的基本假设





## 1.2 特征值和特征向量

### 1.2.1 特征值分解与特征向量  

**特征分解是使用最广的矩阵分解之一**，矩阵分解可以得到一组特征值(eigenvalues)与特征向量(eigenvectors)；



特征值表示的是这个特征到底有多重要，而特征向量表示这个特征是什么。  

如果说一个向量$\vec{v}$是方阵$A$的特征向量，将一定可以表示成下面的形式：

$$
A\nu = \lambda \nu
$$

$\lambda$为特征向量$\vec{v}$对应的特征值。

特征值分解是将一个矩阵分解为如下形式： 
    
$$
A=Q\sum Q^{-1}
$$

其中，$Q$是这个矩阵$A$的**特征向量组成的正交矩阵**，$\sum$是一个对角矩阵，**每一个对角线元素就是一个特征值**，里面的特征值是由大到小排列的，这些特征值所对应的特征向量就是描述这个矩阵变化方向（从主要的变化到次要的变化排列）。也就是说矩阵$A$的信息可以由其特征值和特征向量表示。

并非每个矩阵都可以分解成特征值和特征向量，但**每个实对称矩阵**都可以分解为实特征向量和实特征值。



### 1.2.2 奇异值分解

除了特征分解外，还有一种矩阵分解，称为**奇异值分解**（SVD)，将矩阵分解为奇异值和奇异向量。通过奇异值分解，可以得到和特征分解相同类型的信息，但是，奇异值分解有更广泛的应用，**每个实数矩阵都有一个奇异值分解，但不一定有特征分解，因为必须是方阵才有特征分解**。

在特征分解中，我们将 A 重新写作：
$$
A = Vdiag(\lambda)V^{-1}
$$
其中，V 是特征向量构成的矩阵，$\lambda$是特征值构成的向量，$diag(\lambda)$表示一个对角线都是特征值的对角矩阵。

奇异值分解的形式如下所示：
$$
A = U D V^T
$$
假如 A 是 $m\times n$ 的矩阵，则 U 是 $m\times m$的矩阵，D 是 $m\times n$ 的矩阵，V 是 $n\times n$ 的矩阵。并且，矩阵 U 和 V 是正交矩阵，D 是对角矩阵，且不一定是方阵。

**D 对角线上的元素就是 A 的奇异值，而 U 的列向量是左奇异向量，V 的列向量是右奇异向量。**

可以套用和 A 相关的特征分解来解释其奇异值分解，A 的左奇异向量就是 $AA^T$的特征向量，而右奇异向量就是$A^TA$ 的特征向量，A 的非零奇异值是$AA^T$特征值的平方根，也是$A^TA$特征值的平方根。



(来自深度学习 500 问的数学基础的内容)

> 那么奇异值和特征值是怎么对应起来的呢？我们将一个矩阵$A$的转置乘以$A$，并对$A^TA$求特征值，则有下面的形式：
>
> $$
> (A^TA)V = \lambda V
> $$
>
> 这里$V$就是上面的右奇异向量，另外还有：
>
> $$
> \sigma_i = \sqrt{\lambda_i}, u_i=\frac{1}{\sigma_i}AV
> $$
>
> 这里的$\sigma$就是奇异值，$u$就是上面说的左奇异向量。



奇异值$\sigma$跟特征值类似，在矩阵$\sum$中也是从大到小排列，而且$\sigma$的减少特别的快，**在很多情况下，前10%甚至1%的奇异值的和就占了全部的奇异值之和的99%以上了**。也就是说，我们也可以用前$r$（$r$远小于$m、n$）个的奇异值来近似描述矩阵，即部分奇异值分解：
$$
A_{m\times n}\approx U_{m \times r}\sum_{r\times r}V_{r \times n}^T
$$

右边的三个矩阵相乘的结果将会是一个接近于$A$的矩阵，在这儿，**$r$越接近于$n$，则相乘的结果越接近于$A$**。







# 2. 概率论

## 2.1 概率分布与随机变量

### 2.1.1 机器学习为什么要使用概率

事件的概率是衡量该事件发生的可能性的量度。虽然在一次随机试验中某个事件的发生是带有偶然性的，但那些可在**相同条件下大量重复的随机试验**却往往呈现出**明显的数量规律**。 

机器学习通常必须处理不确定量，有时候也需要处理随机量。几乎所有的活动都需要一些在不确定性存在的情况下进行推理的能力。

不确定性和随机性可能来自多个方面，不确定性有 3 种可能的来源：

1. **被建模系统内在的随机性**。比如纸牌游戏，假设纸牌被真正混洗成了随机顺序。
2. **不完全观测**。对于确定的系统，但是如果不能观测到所有驱动系统行为的变量时，该系统也会呈现随机性。比如让选手选择三扇门中的一个，并获得门后的奖品，每个门后的奖品是确定的，但是选手无法观测到，所以对于选手来说，结果是不确定的。
3. **不完全建模**。当采用一些必须舍弃某些信息的模型时，舍弃的信息可能导致模型的预测出现不确定性。

在很多情况下，采用简单而不确定的规则要比复杂而确定的规则更加的实用。

可以使用概率论来量化不确定性。 用概率来表示一种信任度，**概率直接和事件发生的频率相联系的被称为频率派概率**，比如说某件事发生的概率是 p，这表示如果反复试验无限次，有 p 的比例是发生这件事情；**而涉及确定性水平的称为贝叶斯概率**，比如说医生在对一个病人的诊断中判断其患某个病的概率是 p。

概率论在机器学习中扮演着一个核心角色，因为机器学习算法的设计通常依赖于对数据的概率假设。 

>例如在机器学习（Andrew Ng）的课中，会有一个朴素贝叶斯假设就是条件独立的一个例子。该学习算法对内容做出假设，用来分辨电子邮件是否为垃圾邮件。假设无论邮件是否为垃圾邮件，单词x出现在邮件中的概率条件独立于单词y。很明显这个假设不是不失一般性的，因为某些单词几乎总是同时出现。然而，最终结果是，这个简单的假设对结果的影响并不大，且无论如何都可以让我们快速判别垃圾邮件。



### 2.1.2 变量与随机变量有什么区别 

**随机变量**（random variable）是可以随机地取不同数值的变量。

它表示随机现象（在一定条件下，并不总是出现相同结果的现象称为随机现象）中各种结果的实值函数（一切可能的样本点）。例如某一时间内公共汽车站等车乘客人数，电话交换台在一定时间内收到的呼叫次数等，都是随机变量的实例。 
随机变量与模糊变量的不确定性的本质差别在于，后者的测定结果仍具有不确定性，即模糊性。

**变量与随机变量的区别：** 
当变量的取值的概率不是1时,变量就变成了随机变量；当随机变量取值的概率为1时,随机变量就变成了变量。

> 比如： 
> 当变量$x$值为100的概率为1的话,那么$x=100$就是确定了的,不会再有变化,除非有进一步运算.
> 当变量$x$的值为100的概率不为1,比如为50的概率是0.5,为100的概率是0.5,那么这个变量就是会随不同条件而变化的,是随机变量,取到50或者100的概率都是0.5,即50%。 



### 2.1.3 随机变量与概率分布的联系

一个随机变量仅仅表示一个可能取得的状态，还必须**给定与之相伴的概率分布**来制定每个状态的可能性。用来描述随机变量或一簇随机变量的每一个可能的状态的可能性大小的方法，就是概率分布(probability distribution)**.

随机变量可以分为离散型随机变量和连续型随机变量。

相应的描述其概率分布的函数是：

- **概率质量函数**(Probability Mass Function, PMF):描述离散型随机变量的概率分布，通常用大写字母 $P$表示。

- **概率密度函数**(Probability Density Function, PDF)：描述连续型随机变量的概率分布，通常用小写字母$p$表示。



### 2.1.4 离散型随机变量和概率质量函数

PMF 将随机变量能够取得的每个状态映射到**随机变量取得该状态的概率**。

- 一般而言，$P(x)$ 表示时$ X=x$的概率，概率为 1 表示 $ X=x$ 是确定的，概率是 0 表示 $ X=x$ 是不可能的；
- 有时候为了防止混淆，要明确写出随机变量的名称$P($x$=x)$ 
- 有时候需要先定义一个随机变量，然后制定它遵循的概率分布 x 服从$P($x$)$ 

PMF 可以同时作用于多个随机变量，即**联合概率分布**(joint probability distribution) $P(X=x,Y=y)$表示 $X=x$和$ Y=y$同时发生的概率，也可以简写成 $P(x,y)$.

如果一个函数$P$是随机变量 $X$ 的 PMF， 那么它必须满足如下三个条件：

- $P$的定义域必须是的所有可能状态的集合
- $∀x∈$x, $0 \leq P(x) \leq 1 $. 
- $∑_{x∈X} P(x)=1$. 我们把这一条性质称之为**归一化的**(normalized)，如果不满足这条性质，那么可能某件事情发生的概率会是大于 1。





### 2.1..5 连续型随机变量和概率密度函数

如果一个函数$p$是x的PDF，那么它必须满足如下几个条件

- $p$的定义域必须是x的所有可能状态的集合。
- $∀x∈X,p(x)≥0$. 注意，我们并不要求$ p(x)≤1$，因为此处 $p(x)$不是表示的对应此状态具体的概率，而是概率的一个相对大小(密度)。具体的概率，需要积分去求。
- $∫p(x)dx=1$, 积分下来，总和还是1，概率之和还是1.

注：PDF$p(x)$并没有直接对特定的状态给出概率，给出的是**密度**，相对的，它给出了落在面积为 $δx$的无线小的区域内的概率为$ p(x)δx$. 

由此，我们无法求得具体某个状态的概率，我们可以求得的是 某个状态 $x$ 落在 某个区间$[a,b]$内的概率为$ \int_{a}^{b}p(x)dx$.



### 2.1.6 举例理解条件概率 

条件概率公式如下：
$$
P(A|B) = P(A\cap B) / P(B)
$$
说明：在同一个样本空间$\Omega$中的事件或者子集$A$与$B$，如果随机从$\Omega$中选出的一个元素属于$B$，那么下一个随机选择的元素属于$A$ 的概率就定义为在$B$的前提下$A$的条件概率。

条件概率文氏图示意如图1.1所示。
![](https://cai-images-1257823952.cos.ap-beijing.myqcloud.com/conditional_probability.jpg)

图1.1 条件概率文氏图示意

根据文氏图，可以很清楚地看到在事件B发生的情况下，事件A发生的概率就是$P(A\bigcap B)$除以$P(B)$。 

举例：一对夫妻有两个小孩，已知其中一个是女孩，则另一个是女孩子的概率是多少？（面试、笔试都碰到过） 

**穷举法**：已知其中一个是女孩，那么样本空间为男女，女女，女男，则另外一个仍然是女生的概率就是1/3。 

**条件概率法**：$P(女|女)=P(女女)/P(女)$,夫妻有两个小孩，那么它的样本空间为女女，男女，女男，男男，则$P(女女)$为1/4，$P（女）= 1-P(男男)=3/4$,所以最后$1/3$。 

这里大家可能会误解，男女和女男是同一种情况，但实际上类似姐弟和兄妹是不同情况。 



### 2.1.7 联合概率与边缘概率联系区别  

**区别：** 
联合概率：联合概率指类似于$P(X=a,Y=b)$这样，包含多个条件，且所有条件同时成立的概率。联合概率是指在多元的概率分布中**多个随机变量分别满足各自条件的概率**。 

边缘概率：**边缘概率是某个事件发生的概率，而与其它事件无关**。边缘概率指类似于$P(X=a)$，$P(Y=b)$这样，仅与单个随机变量有关的概率。

**联系：** 

联合分布可求边缘分布，但若只知道边缘分布，无法求得联合分布。 



### 2.1.8 条件概率的链式法则 

由条件概率的定义，可直接得出下面的乘法公式： 
乘法公式 设$A, B$是两个事件，并且$P(A) > 0$, 则有 
$$
P(AB) = P(B|A)P(A)
$$
推广 
$$
P(ABC)=P(C|AB)P(B|A)P(A)
$$
一般地，用归纳法可证：若$P(A_1A_2...A_n)>0$，则有
$$
P(A_1A_2...A_n)=P(A_n|A_1A_2...A_{n-1})P(A_{n-1}|A_1A_2...A_{n-2})...P(A_2|A_1)P(A_1)\\
=P(A_1)\prod_{i=2}^{n}P(A_i|A_1A_2...A_{i-1})
$$
任何多维随机变量联合概率分布，都可以分解成只有一个变量的条件概率相乘形式。 



### 2.1.9 独立性和条件独立性

**独立性**
两个随机变量$x$和$y$，概率分布可以表示成两个因子乘积形式，一个因子只包含$x$，另一个因子只包含$y$，则可以说这两个随机变量相互独立(independent)**。 
条件有时为不独立的事件之间带来独立，有时也会把本来独立的事件，因为此条件的存在，而失去独立性。


举例：$P(XY)=P(X)P(Y)$, 事件$X$和事件$Y$独立。此时给定$Z$，
$$
P(X,Y|Z) \not = P(X|Z)P(Y|Z)
$$
**事件独立时，联合概率等于概率的乘积**。这是一个非常好的数学性质，然而不幸的是，无条件的独立是十分稀少的，因为大部分情况下，事件之间都是互相影响的。 

**条件独立性** 
给定$Z$的情况下,$X$和$Y$条件独立，当且仅当
$$
X\bot Y|Z \iff P(X,Y|Z) = P(X|Z)P(Y|Z)
$$
$X$和$Y$的关系依赖于$Z$，而不是直接产生。 

> **举例**定义如下事件： 
> $X$：明天下雨； 
> $Y$：今天的地面是湿的； 
> $Z$：今天是否下雨； 
> $Z$事件的成立，对$X$和$Y$均有影响，然而，在$Z$事件成立的前提下，今天的地面情况对明天是否下雨没有影响。 



### 2.1.10 常见公式

**概率基础的公式**

- $P(A+B) = P(A)+P(B)-P(AB)$
- $P(A-B)=P(A)-P(B)$
- $P(AB)=P(A)P(B|A)$

**全概率**

$P(A) = \sum_i P(B_i)P(A|B_i)$

**贝叶斯**

$P(B|A) = \frac{P(B)P(A|B)}{P(A)}$



### 2.1.11 应用

**抽球**

n 个球，对于有放回和无放回的抽取方式

- 有放回的抽取，抽取 m 个排成一列，求不同排列的数量：$n^m$

- 没有放回的抽取，抽取 m 个排成一列，求不同排列的数量：$\frac{n!}{(n-m)!}$





## 2.2 常见概率分布

### 2.2.1 均匀分布

离散随机变量的均匀分布：假设 X 有 k 个取值，则均匀分布的概率质量函数为：
$$
p(X=x_i) = \frac{1}{k},i=1,2,\cdots,k
$$
连续随机变量的均匀分布：假设 X 在 [a, b] 上均匀分布，则其概率密度函数为：
$$
p(X=x) = 

\begin{cases}
0,x\notin[a,b]\\
\frac{1}{b-a},x\in[a, b]
\end{cases}
$$


### 2.2.1 Bernoulli分布

**Bernoulli分布**(伯努利分布，0-1分布)是单个二值随机变量分布, 单参数$\phi$∈[0,1]控制,$\phi$给出随机变量等于1的概率. 主要性质有: 
$$
\begin{align*}
P(x=1) &= \phi \\
P(x=0) &= 1-\phi  \\
概率质量函数：P(x=x) &= \phi^x(1-\phi)^{1-x} \\
\end{align*}
$$
其期望和方差为：
$$
\begin{align*}
E_x[x] &= \phi \\
Var_x(x) &= \phi{(1-\phi)}
\end{align*}
$$
**适用范围**: **伯努利分布**适合对**离散型**随机变量建模.



**Multinoulli分布**也叫**范畴分布**, 是单个*k*值随机分布,经常用来表示**对象分类的分布**. 其中$k$是有限值.Multinoulli分布由向量$\vec{p}\in[0,1]^{k-1}$参数化,每个分量$p_i$表示第$i$个状态的概率, 且$p_k=1-1^Tp$.这里$1^T$表示元素全为1的列向量的转置，其实就是对于向量p中除了k的概率之和。可以重写为$p_k=1-\sum_{0}^{k-1}p_i$ 。

补充二项分布、多项分布：

二项分布，通俗点硬币抛多次。二项分布(Binomial distribution)是**n重伯努利试验**成功次数的离散概率分布。

定义成功 x 次的概率为：$f(x)=C_n^xp^x(1-p)^{n-x},x\in{0,1,\cdots,n}$。

**期望是 np， 方差是 np(1-p)**

多项式分布(Multinomial Distribution)是二项式分布的推广。二项式做n次伯努利实验，规定了每次试验的结果只有两个，如果现在还是做n次试验，只不过每次试验的结果可以有多m个，且m个结果发生的概率互斥且和为1，则发生其中一个结果X次的概率就是多项式分布。



### 2.2.3 高斯分布

**高斯也叫正态分布**(Normal Distribution), 概率度函数如下:  
$$
N(x;\mu,\sigma^2) = \sqrt{\frac{1}{2\pi\sigma^2}}exp\left ( -\frac{1}{2\sigma^2}(x-\mu)^2 \right )
$$
其中, $\mu$和$\sigma$分别是均值和标准差, 中心峰值x坐标由$\mu$给出, 峰的宽度受$\sigma$控制, 最大点在$x=\mu$处取得, 拐点为$x=\mu\pm\sigma$

正态分布中，±1$\sigma$、±2$\sigma$、±3$\sigma$下的概率分别是68.3%、95.5%、99.73%，这3个数最好记住。 

此外, 令$\mu=0,\sigma=1$高斯分布即简化为标准正态分布: 
$$
N(x;\mu,\sigma^2) = \sqrt{\frac{1}{2\pi}}exp\left ( -\frac{1}{2}x^2 \right )
$$
对概率密度函数高效求值: 
$$
N(x;\mu,\beta^{-1})=\sqrt{\frac{\beta}{2\pi}}exp\left(-\frac{1}{2}\beta(x-\mu)^2\right)
$$


其中，$\beta=\frac{1}{\sigma^2}$通过参数$\beta∈（0，\infty）$来控制分布精度。



### 2.2.4 何时采用正态分布

问: 何时采用正态分布? 

答: 缺乏实数上分布的先验知识, 不知选择何种形式时, **默认选择正态分布总是不会错的**, 理由如下: 

1. 中心极限定理告诉我们, **很多独立随机变量均近似服从正态分布**, 现实中很多复杂系统都可以被建模成正态分布的噪声, 即使该系统可以被结构化分解. 
2. 正态分布是具有相同方差的所有概率分布中, **不确定性最大的分布**, 换句话说, **正态分布是对模型加入先验知识最少的分布**.



正态分布的推广: 

正态分布可以推广到$R^n$空间, 此时称为**多位正态分布**, 其参数是一个正定对称矩阵$\Sigma$: 
$$
N(x;\vec\mu,\Sigma)=\sqrt{\frac{1}{(2\pi)^ndet(\Sigma)}}exp\left(-\frac{1}{2}(\vec{x}-\vec{\mu})^T\Sigma^{-1}(\vec{x}-\vec{\mu})\right)
$$
对多为正态分布概率密度高效求值: 
$$
N(x;\vec{\mu},\vec\beta^{-1}) = \sqrt{det(\vec\beta)}{(2\pi)^n}exp\left(-\frac{1}{2}(\vec{x}-\vec\mu)^T\beta(\vec{x}-\vec\mu)\right)
$$
此处，$\vec\beta$是一个精度矩阵。



### 2.2.5 指数分布

深度学习中, 指数分布用来描述在$x=0$点处取得边界点的分布, 指数分布定义如下:
$$
p(x;\lambda)=\lambda I_{x\geq 0}exp(-\lambda{x})
$$
指数分布用指示函数$I_{x\geq 0}$来使$x$取负值时的概率为零。



### 2.2.6 Laplace 分布（拉普拉斯分布）

一个联系紧密的概率分布是 Laplace 分布（Laplace distribution），它允许我们在任意一点 $\mu$处设置概率质量的峰值
$$
Laplace(x;\mu;\gamma)=\frac{1}{2\gamma}exp\left(-\frac{|x-\mu|}{\gamma}\right)
$$

期望是 $\mu$，方差是 $2\gamma^2$

拉普拉斯分布比高斯分布更加尖锐和狭窄，在正则化中通常会利用这个性质。

### 2.2.7 泊松分布

假设已知事件在单位时间（或者单位面积）内发生的平均次数为λ，则泊松分布描述了：事件在单位时间（或者单位面积）内发生的具体次数为 k 的概率。 概率密度函数：
$$
p(X=k;\lambda)=\frac{e^{-\lambda}\lambda^k}{k!}
$$
期望是 $\lambda$，方差是 $\lambda$.



### 2.2.8 Dirac分布和经验分布

Dirac分布可保证概率分布中所有质量都集中在一个点上. Diract分布的狄拉克$\delta$函数(也称为**单位脉冲函数**)定义如下: 
$$
p(x)=\delta(x-\mu), x\neq \mu
$$

$$
\int_{a}^{b}\delta(x-\mu)dx = 1, a < \mu < b
$$

Dirac 分布经常作为**经验分布**（empirical distribution）的一个组成部分出现
$$
\hat{p}(\vec{x})=\frac{1}{m}\sum_{i=1}^{m}\delta(\vec{x}-{\vec{x}}^{(i)})
$$
其中, m个点$x^{1},...,x^{m}$是给定的数据集, **经验分布**将概率密度$\frac{1}{m}$赋给了这些点.

当我们在训练集上训练模型时, 可以认为从这个训练集上得到的经验分布指明了**采样来源**.

**适用范围**: 狄拉克δ函数适合对**连续型**随机变量的经验分布.

关于经验分布的另一个重要观点是，它是训练数据的似然最大的那个概率密度函数。



### 2.2.9 混合分布

通过组合一些简单的概率分布来定义新的概率分布也是很常见的。

一种通用的组合方法就是**构造混合分布**。混合分布由一些组件分布构成。

一个混合分布的例子就是：实值变量的经验分布对于每一个训练实例来说，就是以 Dirac 分布为组件的混合分布。



**混合模型**是组合简单概率分布来生成更丰富的一种简单策略。一个非常强大且常见的混合模型就是**高斯混合模型**。

它的组件是高斯分布，每个组件有自己的参数，均值和协方差矩阵。





## 2.3 期望、方差、协方差、相关系数
### 2.3.1 期望 

函数 f(x) 关于某个分布 P(x) 的期望或者期望值是指，当 x 由 P 产生, f 作用于 x 的时候，f(x) 的平均值。

在概率论和统计学中，数学期望（或均值，亦简称期望）是试验中每次可能结果的概率乘以其结果的总和。**它反映随机变量平均取值的大小**。

- 线性运算： $E(ax+by+c) = aE(x)+bE(y)+c$ 
- 推广形式： $E(\sum_{k=1}^{n}{a_ix_i+c}) = \sum_{k=1}^{n}{a_iE(x_i)+c}$ 
- 函数期望：设$f(x)$为$x$的函数，则$f(x)$的期望为
    - 离散函数： $E(f(x))=\sum_{k=1}^{n}{f(x_k)P(x_k)}$
    - 连续函数： $E(f(x))=\int_{-\infty}^{+\infty}{f(x)p(x)dx}$

> 注意：
>
> - 函数的期望大于等于期望的函数（Jensen（詹森）不等式，即$E(f(x))\geqslant f(E(x))$ 
> - 一般情况下，乘积的期望不等于期望的乘积。 
> - 如果$X$和$Y$相互独立，则$E(xy)=E(x)E(y)$。 



### 2.3.2 方差

概率论中方差用来**度量随机变量和其数学期望（即均值）之间的偏离程度**。方差是一种特殊的期望。定义为：

$$
Var(x) = E((x-E(x))^2)
$$

> 方差性质： 
>
> 1）$Var(x) = E(x^2) -E(x)^2$ 
> 2）常数的方差为0; 
> 3）方差不满足线性性质; 
> 4）如果$X$和$Y$相互独立, $Var(ax+by)=a^2Var(x)+b^2Var(y)$  



### 2.3.3 协方差

**协方差是衡量两个变量线性相关性强度及变量尺度**。 两个随机变量的协方差定义为：
$$
Cov(x,y)=E((x-E(x))(y-E(y)))
$$

方差是一种特殊的协方差。当$X=Y$时，$Cov(x,y)=Var(x)=Var(y)$。

> 协方差性质： 
>
> 1）独立变量的协方差为0。 
> 2）协方差计算公式：

$$
Cov(\sum_{i=1}^{m}{a_ix_i}, \sum_{j=1}^{m}{b_jy_j}) = \sum_{i=1}^{m} \sum_{j=1}^{m}{a_ib_jCov(x_iy_i)}
$$

>
>3）特殊情况：

$$
Cov(a+bx, c+dy) = bdCov(x, y)
$$



### 2.3.4 相关系数

**相关系数是研究变量之间线性相关程度的量**。两个随机变量的相关系数定义为：
$$
Corr(x,y) = \frac{Cov(x,y)}{\sqrt{Var(x)Var(y)}}
$$

> 相关系数的性质： 
> 1）有界性。相关系数的取值范围是 [-1,1]，可以看成无量纲的协方差。 
> 2）值越接近1，说明两个变量正相关性（线性）越强。越接近-1，说明负相关性越强，当为0时，表示两个变量没有相关性。 



## 2.4 信息论

信息论主要研究的是对一个信号包含新的多少进行量化。

信息论的一个基本想法是一个不太可能发生的事件居然发生了，比一个非常可能发生的事件发生，能提供更多的信息。

如果想通过这种基本想法来量化信息，需要满足这个 3 个性质：

- 非常可能发生的事件信息论要比较少，并且极端情况下，确保能够发生的事件应该没有信息量；
- 较不可能发生的事件具有更高的信息量；
- 独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息，应该是投掷一次硬币证明朝上的信息量的两倍。

这里定义一个事件 x=$x$ 的**自信息**为：
$$
I(x) = -log P(x)
$$
自信息量只能处理单个的输出。可以用**香农熵**来对整个概率分布中的不确定性总量进行量化：
$$
H(x) = -E_{x\sim P}[I(x)] = -E_{x\sim P}[logP(x)]
$$
也记作 H(P)。这里的 E 表示的就是期望，也就是说一个分布的香农熵是指遵循这个分布的事件所产生的**期望信息总量**。

而如果对于一个随机变量有两个单独的概率分布 P(x) 和 Q(x)，那么可以使用**KL 散度**来衡量这两个分布的差异：
$$
D_{KL}(P||Q) = E_{x\sim P}[\frac{logP(x)}{logQ(x)}] = E_{x\sim P}[logP(x)-logQ(x)]
$$
举例：对于一个二值随机分布的香农熵，$H(x) =- (1-p)log(1-p)-plogp$

KL散度的性质有：

1. 非负的；
2. KL 散度为 0 的情况，当且仅当 P 和 Q 在离散型变量的情况下是相同的分布，或者在连续型变量的情况下是“几乎处处”相同的；
3. 常用作衡量分布之间的某种距离，但并不是真正的距离，因为它不是对称的。

一个和 KL 散度很相似的是**交叉熵**，即 $H(P,Q)=H(P)+D_{KL}(P||Q)$：
$$
H(P,Q)=-E_{x\sim P}logQ(x)
$$
针对 Q 最小化交叉熵等价于最小化 KL 散度，因为 Q 并不参与被省略的那一项。

在计算这些量的时候，经常会遇到 0log0 这个表达式，一般对这个的处理是 $lim_{x->0}xlogx = 0$









# 3. 数值计算

## 3.1 上溢和下溢

连续数学在计算机上的根本困难是：**需要通过有限数量的位模式来表示无限多的实数**。

这意味着在计算机中表示实数的时候几乎总是会引入一些近似误差，许多情况下这仅仅是**舍入误差**。但如果没有考虑最小化舍入误差的累积，可能就会导致算法的失效。这里存在两种比较严重的情况：

1. **下溢**：当接近 0 的数被四舍五入为零时发生下溢的情况。许多函数在参数为 0 而不是一个很小的正数的时候会表现出质的不同。例如，我们通常要避免被零除（有些软件会返回异常，或者会抛出一个非数字，比如 NaN 的占位符）或者避免取零的对数（这通常被视为负无穷）
2. **上溢**：当大量级的数被近似为 $\infin$ 或者 $-\infin$ 。进一步的运算会导致这些无限制变为非数字，比如 inf

必须对上溢和下溢进行数值稳定的一个例子是 **softmax 函数**，它经常用于预测和 Multinoulli 分布相关联的概率，也是 CNN 里常用的函数，其定义如下：
$$
softmax(x)_i = \frac{exp(x_i)}{\sum^n_{j=1}exp(x_j)}
$$
假如 x 是等于一个常数 c，那么输出应该是 $\frac{1}{n}$ ，但有两种情况需要注意：

1. c 是一个很小的负数，会发生下溢的情况；
2. c 是一个很大的正数，会发现上溢的情况

解决的办法，是先做一个简单的变换：
$$
z = x-max_i(x_i)
$$
然后计算 softmax(z) 。

也就是先让每个 x 减去最大值，那么这让exp 的参数最大值为 0（当 x 取最大值的时候），这排除了上溢的可能性；而分母中至少有一项是 1（x 取最大值，那么 exp(z)=1），这也排除了下溢的可能性，即分母不会等于0.

但这里还有另一个问题就是分子的下溢导致结果是 0，这种情况如果计算 log softmax(x) 会得到$-\infin$ 的结果。







##  3.2 导数和偏导数

### 3.2.1 导数和偏导计算

**导数定义**:

导数(derivative)代表了在自变量变化趋于无穷小的时候，函数值的变化与自变量的变化的比值。

- 几何意义是这个点的切线。
- 物理意义是**该时刻的（瞬时）变化率**。

*注意*：在一元函数中，只有一个自变量变动，也就是说只存在一个方向的变化率，这也就是为什么一元函数没有偏导数的原因。在物理学中有平均速度和瞬时速度之说。平均速度有
$$
v=\frac{s}{t}
$$

其中$v$表示平均速度，$s$表示路程，$t$表示时间。这个公式可以改写为

$$
\bar{v}=\frac{\Delta s}{\Delta t}=\frac{s(t_0+\Delta t)-s(t_0)}{\Delta t}
$$

其中$\Delta s$表示两点之间的距离，而$\Delta t$表示走过这段距离需要花费的时间。当$\Delta t$趋向于0（$\Delta t \to 0$）时，也就是时间变得很短时，平均速度也就变成了在$t_0$时刻的瞬时速度，表示成如下形式：

$$
v(t_0)=\lim_{\Delta t \to 0}{\bar{v}}=\lim_{\Delta t \to 0}{\frac{\Delta s}{\Delta t}}=\lim_{\Delta t \to 0}{\frac{s(t_0+\Delta t)-s(t_0)}{\Delta t}}
$$

实际上，上式表示的是路程$s$关于时间$t$的函数在$t=t_0$处的导数。一般的，这样定义导数：如果平均变化率的极限存在，即有

$$
\lim_{\Delta x \to 0}{\frac{\Delta y}{\Delta x}}=\lim_{\Delta x \to 0}{\frac{f(x_0+\Delta x)-f(x_0)}{\Delta x}}
$$

则称此极限为函数 $y=f(x)$ 在点 $x_0$ 处的导数。记作 $f'(x_0)$ 或 $y'\vert_{x=x_0}$ 或 $\frac{dy}{dx}\vert_{x=x_0}$ 或 $\frac{df(x)}{dx}\vert_{x=x_0}$。

通俗地说，导数就是曲线在某一点切线的斜率。



**偏导数**:

既然谈到偏导数(partial derivative)，**那就至少涉及到两个自变量**。以两个自变量为例，$z=f(x,y)$，从导数到偏导数，也就是从曲线来到了曲面。曲线上的一点，其切线只有一条。但是曲面上的一点，切线有无数条。而偏导数就是指多元函数沿着坐标轴的变化率。 


*注意*：直观地说，偏导数也就是函数在某一点上沿坐标轴正方向的的变化率。

设函数$z=f(x,y)$在点$(x_0,y_0)$的领域内有定义，当$y=y_0$时，$z$可以看作关于$x$的一元函数$f(x,y_0)$，若该一元函数在$x=x_0$处可导，即有

$$
\lim_{\Delta x \to 0}{\frac{f(x_0+\Delta x,y_0)-f(x_0,y_0)}{\Delta x}}=A
$$

函数的极限$A$存在。那么称$A$为函数$z=f(x,y)$在点$(x_0,y_0)$处关于自变量$x$的偏导数，记作$f_x(x_0,y_0)$或$\frac{\partial z}{\partial x}\vert_{y=y_0}^{x=x_0}$或$\frac{\partial f}{\partial x}\vert_{y=y_0}^{x=x_0}$或$z_x\vert_{y=y_0}^{x=x_0}$。

偏导数在求解时可以将另外一个变量看做常数，利用普通的求导方式求解，比如$z=3x^2+xy$关于$x$的偏导数就为$z_x=6x+y$，这个时候$y$相当于$x$的系数。

某点$(x_0,y_0)$处的偏导数的几何意义为曲面$z=f(x,y)$与面$x=x_0$或面$y=y_0$交线在$y=y_0$或$x=x_0$处切线的斜率。  



### 3.2.2 导数和偏导数有什么区别？  

导数和偏导没有本质区别，如果极限存在，都是当自变量的变化量趋于0时，函数值的变化量与自变量变化量比值的极限。  

> - 一元函数，一个$y$对应一个$x$，导数只有一个。  
> - 二元函数，一个$z$对应一个$x$和一个$y$，有两个导数：一个是$z$对$x$的导数，一个是$z$对$y$的导数，称之为偏导。  
> - 求偏导时要注意，对一个变量求导，则视另一个变量为常数，只对改变量求导，从而将偏导的求解转化成了一元函数的求导。



### 3.2.3 导数的四则运算



- $(u+v)^{'}=u^{'}+v^{'}$
- $(u-v)^{'}=u^{'}-v^{'}$
- $(uv)^{'} = u^{'}v+uv^{'}$
- $(u/v)^{'}=\frac{u^{'}v-uv^{'}}{v^2}$



### 3.2.4 常见的导数运算法则

- y=c(常数) ==> $y^{'}=0$
- $y=x^a$  ==> $y^{'}=ax^{a-1}$
- $y=a^x$ ==> $y^{'}=ln(a)a^x$
- $y=log_ax$==>$y^{'} = \frac{1}{xlna}$
- $y = lnx$ ==> $y^{'}=\frac{1}{x}$
- $y=sin(x)$ ==> $y^{'}=cos(x)$
- $y=cos(x)$ ==> $y^{'}=-sin(x)$
- $y=tan(x)$ ==> $y^{'}=\frac{1}{cos(x)^2}$



### 3.2.5 复合函数的运算法则

假设 g 在 x 处可导，且 f 在 g(x) 处可导，那么 y=f(g(x)) 的导数计算：
$$
y^{'} = f^{'}(g(x))\cdot g^{'}(x)
$$






## 3.3 基于梯度的优化方法

大多数深度学习算法都涉及某种形式的优化。**优化指的是改变 x 以最小化或者最大化某个函数 f(x) 的任务**。

通常我们都用最小化 f(x) 指代大多数最优化问题，最大化可以由最小化 -f(x) 来实现。

一般将需要优化的函数称为**目标函数(object function)或者准则(criterion)**，而如果是其进行最小化的时候，也称为**代价函数(cost function)、损失函数(loss function)或者误差函数(error function)**。

一般用一个上标 * 表示最小化或者最大化函数的 x 的取值，比如 $x^* = argmin f(x)$。

假设有一个函数 y=f(x)，其导数记作 $f^{'}(x)$或者 $\frac{dy}{dx}$

导数对于最小化是非常有用的，因为它告诉我们如何更改 x 来略微地改善 y。我们可以根据导数来决定对 x 的修改，是增大 x 还是减小 x，一般是移动 x 来走到导数为 0 的位置，从而最小化 y，这种做法就是**梯度下降**。

对于 $f^{'}=0$的点，称为**临界点或者驻点**。这个点可能是局部的极小点或者极大点，也可能是全局最小点或者最大点，也可能是一个鞍点，也就是既不是最大点也不是最小点。

理想情况下，当然是希望达到全局极小点，但这可能达不到，通常是达到局部极小点，但有的局部极小点非常接近全局最小点，有的极小点可能非常大，所以要尽量达到全局极小点或者接近的局部极小点。



**梯度**是相对一个向量求导的导数，f 的导数是包含所有偏导数的向量，记为 $\nabla_xf(x)$，梯度的第 i 个元素是 f 关于 $x_i$ 的偏导数。在多维情况下，临界点是梯度中所有元素都是 0 的点。

在负梯度上移动可以减少 f，这称为**最速下降法或者梯度下降**，它建议新的点为：
$$
x^{'}=x-\alpha \nabla_x f(x)
$$
这里 $\alpha$ 是学习率，是一个确定步长的正标量，通常其初始值是一个比较小的常数。





# 参考文献

1. Ian，Goodfellow，Yoshua，Bengio，Aaron...深度学习[M]，人民邮电出版，2017
2. 深度学习 500 问第一章数学基础：https://github.com/scutan90/DeepLearning-500-questions/tree/master/ch01_%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80
3. [Reflection_Summary--数学](https://github.com/sladesha/Reflection_Summary/tree/master/%E6%95%B0%E5%AD%A6)





