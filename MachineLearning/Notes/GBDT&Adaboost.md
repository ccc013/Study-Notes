# GBDT

参考文章：

- [[机器学习算法GBDT](https://www.cnblogs.com/bnuvincent/p/9693190.html)](https://www.cnblogs.com/bnuvincent/p/9693190.html)
- [机器学习-一文理解GBDT的原理-20171001](https://zhuanlan.zhihu.com/p/29765582)
- [机器学习（四）--- 从gbdt到xgboost](http://www.cnblogs.com/mfryf/p/5946815.html) 
- [机器学习常见算法个人总结（面试用）](http://kubicode.me/2015/08/16/Machine%20Learning/Algorithm-Summary-for-Interview/)
- [xgboost入门与实战（原理篇）](http://blog.csdn.net/sb19931201/article/details/52557382)



## 1. 简介

GBDT，即 Gradient Boosting Decision Tree，梯度提升决策树，也是 Boosting 算法的一种，在传统机器学习算法里是最常用也是效果最好的几种算法之一，并且经常应用于竞赛中，原因有这几个：

1. 性能很好；
2. 可以用作回归，也可以做分类
3. 可以筛选特征

GBDT是一个基于迭代累加的决策树算法，它通过构造一组弱的学习器（树），并把多颗决策树的结果累加起来作为最终的预测输出。



另外，面试的时候也很喜欢考这个算法，主要考核的问题会有以下几个：

- gbdt 的算法的流程？
- gbdt 如何选择特征 ？
- gbdt 如何构建特征 ？
- gbdt 如何用于分类？
- gbdt 通过什么方式减少误差 ？
- gbdt的效果相比于传统的LR，SVM效果为什么好一些 ？
- gbdt 如何加速训练？
- gbdt的参数有哪些，如何调参 ？
- gbdt 实战当中遇到的一些问题 ？
- gbdt的优缺点 ？



### 1.1 训练过程

GBDT 实际上是通过采用加法模型（即基函数的线性组合），以及不断减少训练过程中产生的残差来努力拟合训练数据，提升模型性能。其训练过程如下图所示：

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/gbdt_train_1.png" style="zoom:60%;" />

GBDT  通过多轮迭代，每轮迭代产生一个弱分类器，**每个分类器在上一轮分类器的残差基础上进行训练**。这里对弱分类器的要求一般是足够简单，而且是低方差和高偏差，因为 GBDT 的训练就是不断减少偏差来不断提高最终分类器的精度。

通常，弱分类器会选择 CART 树（分类回归树）。

计算公式如下所示：
$$
F^* = argminE_{x,y}[L(y,F(x))] \\
F(x; p_m, a_m) = \sum_{m=0}^M p_m h(x;a_m)
$$
上述公式中 $p_m$ 表示步长，我们可以在函数空间形式上使用梯度下降法求解，首先固定$x$，然后对$F(x)$求其最优解。下面先给出框架流程：

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/gbdt_train_2.png" style="zoom:50%;" />

我们需要做的是估计$g_m(x)$，它是梯度方向；通过使用决策树实现来逼近$g_m(x)$，使得两者之间的距离尽可能的近，而距离的衡量方式有多种，包括均方误差和`LogLoss`误差。

下面给出使用`LogLoss`损失函数的具体推导：
$$
L(y, F) = log(1+exp(-2yF)) \qquad y\in [-1,1]
$$
**Step1** 首先求解初始值$F_0$，令其偏导为0。（实现时是第1棵树需要拟合的残差）：

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/gbdt_train_3.png" style="zoom:50%;" />

**Step 2** 估计$g_m(x)$，并用决策树对其进行拟合。$g_m(x)$是梯度，实现时是第$m$棵树需要拟合的残差：

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/gbdt_train_4.png" style="zoom:50%;" />

**Step 3** 使用牛顿法求解下降方向步长。$r_{jm}$是拟合的步长，实现时是每棵树的预测值。（通常实现中这一步是被省略的，改为使用**Shrinkage**的策略通过参数设置步长，避免过拟合。

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/gbdt_train_5.png" style="zoom:50%;" />

**Step 4** 预测时只需要把每棵树的预测值乘以缩放因子然后相加即可得到最终的预测值：
$$
p = predict(0) + \sum_{m=1}^M shrinkage * predict(d_m)
$$
若需要预测值输出区间在$[0,1]$，可作如下转换：
$$
probability = \frac{1}{1+e^{-2 * predict}}
$$
**GBDT中的树是回归树，不是分类树。**



### 1.2 如何选择特征

GBDT 如何选择特征其实就是如何生成 CART 树的过程，这里有一个前提，默认弱分类器选择的是 CART 树，但也可以选择其他分类器，但是必须是低方差和高偏差。

那么 CART 树是如何生成呢？

假设目前总共有 M 个特征，第一步是先从中选择一个特征 j，作为二叉树的第一个节点，然后对特征 j 的值选择一个切分点 m，如果样本的特征 j 的数值小于 m，则分为一类，否则就是另一类，这样就构建了CART 树的一个节点，其他节点的生成都是一样的。这里的主要问题是每轮迭代中特征 j 和其切分点 m 的选择。

原始的 gbdt 是暴力的做法，遍历每个特征，然后对每个特征遍历它所有可能的切分点，找到最优特征 j 的最优切分点 m。

这里代码的实现如下：

```python
def findLossAndSplit(x,y):
    # 我们用 x 来表示训练数据
    # 我们用 y 来表示训练数据的label
    # x[i]表示训练数据的第i个特征
    # x_i 表示第i个训练样本

    # minLoss 表示最小的损失
    minLoss = Integet.max_value
    # feature 表示是训练的数据第几纬度的特征
    feature = 0
    # split 表示切分点的个数
    split = 0

    # M 表示 样本x的特征个数
    for j in range(0,M):
        # 该维特征下，特征值的每个切分点，这里具体的切分方式可以自己定义
        for c in range(0,x[j]):
            L = 0
            # 第一类
            R1 = {x|x[j] <= c}
            # 第二类
            R2 = {x|x[j] > c}
            # 属于第一类样本的y值的平均值
            y1 = ave{y|x 属于 R1}
            # 属于第二类样本的y值的平均值
            y2 = ave{y| x 属于 R2}
            # 遍历所有的样本，找到 loss funtion 的值
            for x_1 in all x
                if x_1 属于 R1：
                    L += (y_1 - y1)^2
                else:
                    L += (y_1 - y2)^2
            if L < minLoss:
               minLoss = L
               feature  = i
               split = c
    return minLoss,feature ,split
```

这个实现是找到让下面公式最小的特征 j 和其切分点 m：
$$
min_j[min_{c_1}\sum(y_i-c_i)^2+min_{c_2}\sum(y_i-c_2)^2]
$$






## 2. RF 与 GBDT 对比

（1）RF中树的棵树是**并行**生成的；GBDT中树是**顺序**生成的；两者中过多的树都会过拟合，但是**GBDT更容易过拟合；**

（2）RF中每棵树分裂的特征**比较随机**；GBDT中前面的树优先分裂对大部分样本区分的特征，后面的树分裂对小部分样本区分特征；

（3）RF中主要参数是**树的棵数**；GBDT中主要参数是**树的深度，一般为1**；



## 3. Shrinkage

**Shrinkage**认为，每次走一小步逐步逼近的结果要比每次迈一大步逼近结果更加容易避免过拟合。
$$
y(1\sim i) = y(1 \sim i-1) + step * y_i
$$



## 4. 优缺点

### 4.1 优点

1. 精度高
2. 能处理非线性数据
3. 能处理多特征类型
4. 适合低维稠密数据
5. 模型可解释性好
6. 不需要做特征的归一化，可以自动选择特征
7. 能适应多种损失函数，包括均方误差和`LogLoss`等



### 4.2 缺点

1. **boosting**是个串行的过程，所以并行麻烦，需要考虑上下树之间的联系
2. 计算复杂度大
3. 不适用高维稀疏特征



### 4.3 为什么不适合高维稀疏特征？

参考：https://www.zhihu.com/question/400741447

低维稠密数据，可以看作特征是经过精心筛选后的数据，拥有较高的质量。低维就对应着变量个数少，稠密对应着数据几乎没有缺失值和outlier。

此时GDBT就可以非常精确地拟合这些特征和response的【非线性】与【交互】协变。二叉树是可以拟合非线性关系的有效工具，但由于显著性检验的准则不太明确，在医学经济学领域倒不是很常用，希望有统计学家攻克这一难点。



稠密数据与稀疏数据对立，稀疏数据是数据有效值占比低于5%，将n个样本样本，m个特征看作n*m的数据矩阵，统计其中有效值个数/n*m，即可得到该指标。稠密数据同理评估。

GBDT的底层采用CART树，核心是分裂节点的选择，也就是特征选择。**如果数据稀疏，意味着特征有效取值很少，计算Gini指数都会非常低，即此方法无法区分性的选择出良好特征以进行分支建树**。

这很好理解，数据稀疏，特征表征性不够，传统机器学习算法模型效果都不会好到哪里去。所以要特征工程，进行数据筛选和降维。



## 5. 调参

1. 树的个数 100~10000
2. 叶子的深度 3~8
3. 学习速率 0.01~1
4. 叶子上最大节点树 20
5. 训练采样比例 0.5~1
6. 训练特征采样比例 $\sqrt(n)$





------

# Adaboost

参考：

- [12_Adaboost_GBDT_XGBoost算法原理 ](https://github.com/GYee/CV_interviews_Q-A/blob/master/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/12_Adaboost_GBDT_XGBoost%E7%AE%97%E6%B3%95%E5%8E%9F%E7%90%86.md)
- 《统计学习方法》



## 简介 

**特点：**

1. 不改变所给的训练数据，而不断改变训练数据权值的分布，使得训练数据在基本分类器的学习中起不同的作用
2. 利用基本分类器的线性组合构建最终的分类器

假设给定一个二类分类的训练数据集：
$$
T=\{(x_1,y_1),(x_2,y_2),...,(x_N,y_N)\}
$$
其中每个样本点由实例与标记组成。实例 $x_i \in \chi \subseteq R^n$ ，标记 $y_i \in \Upsilon = \{-1,1\}$ ，$\chi$ 是实例空间，$\Upsilon$ 是标记集合。Adaboost 利用以下算法，从训练数据中学习一系列弱分类器，并将这些弱分类器线性组合成为一个强分类器。

(一) 初始化训练数据的权值分布：
$$
D_1 = (w_{11},...,w_{1i},...,w_{1N}),\quad w_{1i}=\frac{1}{N},\quad i=1,2,...,N
$$

> 假设训练数据集具有均匀的权值分布，即每个训练样本在基本分类器的学习中作用相同。这一假设保证第１步能够在原始数据上学习基本分类器 $G_1(x)$ 。

(二) 一共需要学习 M 个基本分类器，则在每一轮 $m = 1,2,...,M$ 顺次地执行下列操作：

1. **使用当前分布 $D_m$ 加权的训练数据集，得到基本分类器：**
   $$
   G_m(x):\chi\rightarrow \{-1,+1\}
   $$

2. **计算 $G_m(x)$ 在训练数据集上的分类误差率：**
   $$
   e_m = \sum_{i=1}^{N}P(G_m(x_i)\not = y_i) = \sum_{G_{mi}\not = y_i}w_{mi}
   $$
   $w_{mi}$ 表示第 m 轮中第 i 个实例的权值，$\sum_{i=1}^{N}w_{mi} = 1$ 。这表明，  $G_{m}(x)$  在加权的训练数据集上的分类误差率是被 $G_{m}(x)$  误分类的样本的权值之和。

3. **计算 $G_{m}(x)$ 的系数：**
   $$
   \alpha_m = \frac{1}{2}ln \frac{1-e_m}{e_m}
   $$
   $\alpha_m$ 表示 $G_{m}(x)$ 在最终分类器中的重要性。根据式子可知，$e_m \le \frac{1}{2}$ 时，$\alpha_m \ge 0$ ，并且 $\alpha_m$ 随着 $e_m$ 的减小而增大，所以分类误差率越小的基本分类器在最终分类器中的作用越大。**这里注意所有 $\alpha_m$ 加起来的和并不等于 1 。** （<u>注意 $e_m$ 是有可能比 $\frac{1}{2}$ 大的，也就是说 $\alpha_m$ 有可能小于 0，《统计学习方法》没有讨论这种情况的处理方法，但在西瓜书中的处理方法是抛弃该分类器，且终止学习过程，哪怕学习到的分类器个数远远没有达到预设的 M</u>）

4. **更新训练数据集的权值分布：**
   $$
   D_{m+1} = (w_{m+1,1},...,w_{m+1,i},...,w_{m+1,N}) \\
   w_{m+1,i} = \frac{w_{mi}}{Z_m}exp(-\alpha_my_iG_m(x_i)), \quad i=1,2,...,N \\
   其中 Z_m是规范因子，Z_m = \sum_{i=1}^{N}w_{mi}exp(-\alpha_my_iG_m(x_i))
   $$
   $w_{m+1,i}$ 也可以写成分段函数的形式：
   $$
   w_{m+1,i}= \begin {cases} 
   \frac{w_{mi}}{Z_m}e^{-\alpha_m}, & G_m(x_i) = y_i \\
   \frac{w_{mi}}{Z_m}e^{\alpha_m}, & G_m(x_i) \not = y_i
   \end {cases}
   $$
   也就是说被基本分类器 $G_m(x)$ 误分类样本的权值得以增大，而被正确分类的样本的权值却变小。两者相比可知误分类样本的权值被放大 $e^{2\alpha_m} = \frac{1-e_m}{e_m}$ 倍。因此，误分类样本在下一轮学习中起更大的作用。

(三) 经过以上过程后可以得到 M 个基本分类器，构建基本分类器的线性组合：
$$
f(x) = \sum_{m=1}^{M}\alpha_m G_m(x)
$$
得到最终分类器：
$$
G(x) = sign(f(x)) = sign(\sum_{m=1}^{M}\alpha_m G_m(x))
$$
线性组合 $f(x)$ 实现 M 个基本分类器的加权表决，$f(x)$ 的符号决定了实例 $x$ 的类，$f(x)$ 的绝对值表示分类的确信度。



引自《机器学习》

> 对无法接受带权样本的基分类器学习方法，则可通过“**重采样法**”来处理，即在每一轮学习中，根据样本分布对训练集重新采样，再用重采样而得的样本集对基分类器进行训练。上面也说到了，AdaBoost模型学习过程中，当出现一个基分类器的误分类误差大于 0.5 即比随机猜测还要差时，处理方法是将该分类器丢弃，且终止学习过程，此种情形下，初始设置的学习轮次 M 也许还未达到，可能会导致最终集成中只包含很少的基分类器而性能不佳。若采用“重采样法”，则可获得“重启动”机会以避免训练过程过早停止，即在抛弃不满足条件的当前基分类器之后，可根据当前分布重新对训练样本进行采样，再基于新的采样结果重新训练出基分类器，从而使得学习过程可以持续到预设的 M 轮完成。



## Adaboost 的另一种解释

> 可以认为 AdaBoost 算法是模型为加法模型、损失函数为指数函数、学习算法为前向分布算法时的二类分类学习方法。

首先需要介绍一下**前向分布算法：**

假设加法模型：
$$
f(x) = \sum_{m=1}^{M}\beta_mb(x;\gamma_m)
$$
其中，$b(x;\gamma_m)$ 为基函数，$\gamma_m$ 为基函数的参数，$\beta_m$ 为基函数的系数。

在给定训练数据及损失函数 $L(y,f(x))$ 的条件下，学习加法模型 $f(x)$ 成为经验风险极小化即损失函数极小化问题：
$$
min_{\beta_m,\gamma_m}\sum_{i=1}^{N}L(y_i,\sum_{m=1}^{M}\beta_mb(x_i;\gamma_m))
$$
这是一个复杂的优化问题，前向分布算法求解这一问题的想法是：因为学习的是加法模型，如果能够从前往后，每一步只学习一个基函数及其系数，逐步逼近以上优化函数式，那么就可以简化优化的复杂度。具体地，每一步只需要优化如下损失函数：
$$
min_{\beta,\gamma}\sum_{i=1}^{N}L(y_i,\beta b(x_i;\gamma))
$$
**下面使用前向分步算法推导出 AdaBoost**

此时的模型是由基本分类器组成的加法模型：
$$
f(x ) = \sum_{m=1}^{M}\alpha_mG_m(x)
$$
其损失函数为**指数损失函数**：$L(y,f(x)) = exp(-yf(x))$ (书中还用了一大段来证明前向分步算法的损失函数是指数损失函数)

在第 m 轮迭代中可以得到 $\alpha_m$，$G_m(x)$ 和 $f_m(x)$。
$$
f_m(x) = f_{m-1}(x)+\alpha_mG_m(x)
$$
目标是使前向分步算法得到的 $\alpha_m$ 和 $G_m(x)$ 使 $f_m(x)$ 在训练集上的指数损失最小，即：
$$
(\alpha_m,G_m(x)) = arg min_{\alpha,G}\sum_{i=1}^{N}exp[-y_i(f_{m-1}(x_i)+\alpha G(x_i))]
$$
也可以表示为：
$$
(\alpha_m,G_m(x)) = arg min_{\alpha,G}\sum_{i=1}^{N}\hat{w}_{mi}exp[-y_i\alpha G(x_i)]
$$
其中 $\hat{w}_{mi}=exp[-y_if_{m-1}(x_i)]$ 。因为 $\hat{w}_{mi}$ 既不依赖于 $\alpha$ 也不依赖于 G ，所以与最小化无关。但 $\hat{w}_{mi}$ 依赖于 $f_{m-1}(x)$ ，随着每一轮迭代而发生改变。

可以证明使上面式子达到最小的 $\alpha_m^*$ 和 $G_m^*(x)$ 就是 AdaBoost 算法所得到的 $\alpha_m$ 和 $G_m(x)$ 。

