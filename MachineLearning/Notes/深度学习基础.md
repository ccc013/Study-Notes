深度学习的一些知识点，包括参考的文章，论文或者博客等。

---

# 基础理论


## 1. 深度学习的实践层面


### 1.1 训练、验证和测试集


应用深度学习是一个典型的迭代过程，需要多次循环往复，才能为应用程序找到一个称心的神经网络，**因此循环该过程的效率是决定项目进展速度的一个关键因素**，**而创建高质量的训练数据集，验证集和测试集也有助于提高循环效率。**


一般的训练过程是这样的，收集数据，划分好训练集、验证集、测试集，在训练集上训练算法，然后通过验证集或者简单交叉验证集选择最好的模型，然后在测试集上进行评估，也是为了无偏评估算法的运行情况。


对于数据集的划分，一般会划分训练集、验证集和测试集，其中**验证集的目的是验证不同的算法，检验哪种算法更有效；而测试集的主要目的就是正确评估分类器的性能。**


划分比例的做法如下：


1. 在深度学习发展之前，机器学习的**小数据量时代**，**常见的做法就是将数据按照 7:3的比例划分训练集和测试集，或者就是 6:2:2的比例分为训练集、验证集和测试集**，这也是之前机器学习领域普遍认可的最好的实践方法；
1. 但在大数据时代，比如数据有上百万，验证集和测试集则是小于数据总量的 20% 或者 10%。



另外，深度学习的一个趋势就是越来越多的人在训练和测试集分布不匹配的情况下进行训练，比如训练一个识别猫的模型，训练集可能是来自网上的图片，通常都可能是精修过的图片，非常清晰明亮，分辨率很高，也没有复杂背景、遮挡等干扰因素，但是测试集就是用户上传的猫的图片，像素可能比较低，分辨率不高、比较模糊等，这就可能导致训练时候模型很好，但实际应用效果很差。


因此，最好保证验证集和测试集的数据来自同一分布，这样才能很好的反映模型真实应用的情况，更好的优化模型性能。对应到上述例子，就是，你的验证集和测试集也应该收集类似用户上传的图片，分辨率不高、模糊、复杂背景等。


另外，没有测试集也是可以的，这种情况就是训练集和验证集，或者说验证集就是测试集了，因为在实际应用中，很多人也只是把测试集当成简单交叉验证集使用，并没有完全实现该术语的功能，**因为他们会把验证集数据过度拟合到测试集中**。实际上如果不需要无偏评估算法性能，是可以不需要测试集的。


### 1.2 偏差，方差（Bias / Variance）


偏差和方差也是深度学习里很重要的概念，它们通常和另一对术语相关联，也就是**过拟合和欠拟合。**


#### 二维数据集


这里用一个例子来介绍这两者的区别。


![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/bias_variance_0.png)


首先，有一个数据集如上图所示，这是一个二维数据集，我们可以绘制数据进行可视化，通常分类结果情况如下图所示，有 3 种情况。


如果给这个数据集拟合一条曲线，如果不能很好拟合该数据，那么这是**高偏差(high bias)** 的情况，也叫做**欠拟合(underfitting)**。比如下图第一幅图的情况；


相反，如果使用了一个非常复杂的分类算法，比如深度神经网络，那么得到的曲线就可能会非常的拟合数据，这种情况就是**高方差(high variance)**，也叫做**过拟合(overfitting)**。比如下图第三幅图的情况


当然最好的情况，应该是复杂程度适中，数据拟合适度的的分类算法，数据拟合情况如下图第二幅图的情况，这种就是**适度拟合(just right)**，这种也就是我们训练模型的目标，不会欠拟合也不过拟合。


![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/bias_variance_1.png)


#### 多维数据集


上述例子是一个二维数据集，可以绘制数据，将偏差和方差可视化。但在多维空间数据中，绘制数据和可视化分割边界是无法实现的，但这里可以有其他的指标来判断偏差和方差的情况。


第一个指标就是看误差，**训练集误差和验证集误差**。


假设训练集误差是 1%，而验证集误差是 11%，**这种情况就是模型过拟合了训练集，高方差的情况**。因为训练集的性能很好，但验证集的性能较差。


第二个指标是**结合人眼辨别的错误率**，一般来说，**最优误差也被称为贝叶斯误差**。


通常来说，**判断是否高偏差是看训练集误差和贝叶斯误差的差距**，如果两者相差不大，比如贝叶斯误差是 1%，训练集误差也是 1%，那么这种情况偏差就不高，如果这种情况下验证集误差是2%，那么方差也不高，也就是非常理想的情况，拟合数据适中。


而如果人眼都无法很好分辨的数据集，比如图片都很模糊，这种情况下，最优误差也会很高。那么就只能单纯看训练集误差和验证集误差来判断是否高偏差和高方差了。


上述分析的前提都是训练集和验证集数据来自相同分布，并且假设基本误差很小，如果没有这些前提，分析过程就更加复杂了。


### 1.3 训练网络的技巧


吴恩达老师在 deeplearning.ai 课程给出的训练技巧：


1. 首先需要判断网络的偏差是否很高，通过训练集来评估性能，如果确实很高，甚至无法拟合训练集，可以考虑的做法：
   - **更换新的网络**，比如含有更多隐藏层或者隐藏单元的网络，更大的网络通常是可以很好拟合训练集的；
   - **花费更久的时间来训练网络**，增加时间不一定有用，但也可以尝试；
   - **尝试更先进的优化算法**
2. 其次就是评估方差问题，这个就是看验证集的性能，如果方差很高，可以选择的做法有：
   - **最好的解决办法是增加数据**，但不一定能实现；
   - **正则化也可以减少过拟合**；
3. 需要注意这两点：
   - **高偏差和高方差是两种不同的情况，对应的解决方法也不相同**，并且解决办法也不是通用的，比如增加训练数据并不能解决高偏差问题，所以首先需要明确问题是高偏差还是高方差；
   - 在机器学习的阶段，很多方法可以增加偏差，减少方差，或者减少偏差，增加方差；而目前的深度学习和大数据时代，通过正则化的办法，可以在训练更大网络的时候，减少方差而不会增加太多方差，即可以减少方差或者偏差的时候，不会对另一方产生过多的不良影响，当然前提是网络是比较规范化的。



### 1.4 正则化(Regularization)


正则化是一种有效解决过拟合的办法，很多时候没法增加数据量的情况，这就是首选的解决过拟合的办法。


正则化通常有两种，L1 和 L2 正则化。


#### L2 正则

L2 正则化的公式如下所示
$$
minJ(W,b)=\frac{1}{m}\sum^m_{i=1}L(\hat y^i,y^i)+\frac{\lambda}{2m}||W||^2_2
$$

其中 $J(W,b)$ 表示要代价函数，右边公式的第一项是原来的损失函数，第二项就是 L2 正则项，其中参数 $\lambda$ 用于控制正则化的程度，**通常通过验证集或者交叉验证集来配置这个参数**，L2 正则化也被称为**权重衰减(weight decay)**。

对于逻辑回归模型， W 是一个向量，即维度和特征向量一样，其正则化项就是：
$$
||W||^2_2=\sum_{j=1}^{dimension}W_j^2
$$
而对于包含多层的神经网络模型，网络层之间有多个参数矩阵，每个矩阵 W 的维度是 $(n^{[l]},n^{[l-1]})$，这里 l 表示的是 第 l 层，而 $n^l$ 表示第  l 层里神经元的数量，所以 L2 正则化部分可以写为：
$$
\frac{\lambda}{2m}\sum_{l=1}^L||W^l||_2^2 \\
||W^l||_2^2=\sum_{i=1}^{n^{[l-1]}}\sum_{j=1}^{n^{[l]}}(W^l_{ij})^2
$$


#### L1 正则

L1 正则化则是：
$$
minJ(W,b)=\frac{1}{m}\sum^m_{i=1}L(\hat y^i,y^i)+\frac{\lambda}{2m}||W^l||\\
||W^l||=\sum_{i=1}^{n^{[l-1]}}\sum_{j=1}^{n^{[l]}}W^l_{ij}
$$
当使用 L1 正则化的时候，参数 W 将会变得稀疏，即 W 向量中会包含很多 0，**从理论上来说这种做法有利于压缩模型，因为存储模型所占用的内存更少，但实际上并没有降低太多存储的内存**。




#### 正则化如何预防过拟合呢？


正则化如何预防过拟合呢？


在神经网络中，过拟合的原因主要是网络参数过多，当数据不足的时候，就很容易完全拟合训练集，而正则化的作用，直观上的理解就是**当参数 $\lambda$ 设置得足够大，权重矩阵 W 被设置为接近于 0 的数值，也就是说将很多神经元的权重设置为 0，也就基本上是消除了这些神经元的许多影响**，当然实际上并不能这么做，这种做法就会导致高偏差的问题，实际上就是参数 ![](https://g.yuque.com/gr/latex?%5Clambda#card=math&code=%5Clambda) 会存在一个中间值，使得网络可以处于适度拟合的状态。


#### dropout


常用的正则化方法除了 L2 正则化，在深度学习更加常用且实用的是 **dropout(随机失活)**。


假设我们采用下面这个神经网络进行训练：


![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/dropout_0.png)


该网络出现了过拟合的情况，所以我们加入 **dropout** 来解决过拟合的情况，而 **dropout** 的做法是会在设置该方法的网络层中，根据预设的概率（通常是 0.5），让该网络层中的神经元以这样的概率随机保留或者消除，这样就会得到一个神经元比较少的网络，然后进行训练，其实现效果如下所示：


![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/dropout_1.png)


##### 反向随机失活(inverted dropout)


如何实现 dropout 呢，方法有几种，最常用的是 **反向随机失活(inverted dropout)**。下面是这方法实现的过程。

一般来说，对于一个网络层，假设有 ![](https://g.yuque.com/gr/latex?n#card=math&code=n) 个神经元，设置保留神经元的概率是 ![](https://g.yuque.com/gr/latex?p#card=math&code=p)，那么每次前向传播会保留使用的神经元数量是 ![](https://g.yuque.com/gr/latex?n%20%5Ctimes%20p#card=math&code=n%20%5Ctimes%20p)，而删除的数量则是 ![](https://g.yuque.com/gr/latex?(1-p)%20%5Ctimes%20n#card=math&code=%281-p%29%20%5Ctimes%20n)，这里有一个更生动的例子图片，如下两种情况，分别是设置 ![](https://g.yuque.com/gr/latex?p%3D1.0#card=math&code=p%3D1.0) 和 p = 0.8 的情况，下图中 `keep Probability = p` ：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/dropout_2.png)




上图中第二张图的第二层使用了 **dropout**，并且设置 p=0.8 ，所以原本是 5 个神经元，在这次前向传播中就有两个神经元是被删除，并没有使用。但这种做法是有副作用的，就是第三层的数值 $z^{[3]}=W^{[3]}a^{[2]}+b^{[3]}$，将会因为有部分的神经元被删除，即置为 0 没有使用而变小了，为了不让 $z^{[3]}$ 的期望值降低，这里需要做一个调整，对 $a^{[2]}$ 需要这样处理：$a^{[2]}:=\frac{a^{[2]}}{p}$，也就是除以概率 p ，这样可以修正或者弥补被删除的那部分神经元。

注意，在测试阶段不需要使用 **dropout** ，因为测试时候并不希望输出结果是随机的，这会影响预测结果，所以测试时候就是使用网络的所有神经元和网络参数进行预测结果。




##### 理解 dropout


**dropout** 可以随机删除网络中的神经单元，它是如何实现正则化的效果的呢？


直观上的理解如下：


> 不要依赖于任何一个特征，因为该单元的输入可能随时被清除，因此该单元通过这种方式传播下去，并为单元的四个输入增加一点权重，通过传播所有权重，**dropout **将产生收缩权重的平方范数的效果，和之前讲的正则化类似；
> 实施**dropout**的结果是它会压缩权重，并完成一些预防过拟合的外层正则化；
> 对不同权重的衰减是不同的，它取决于激活函数倍增的大小。



总结一些，**dropout** 的作用和 L2 正则化类似，不同的是应用方式不同，而且更适用于不同的输入范围。


这里还有两个应用 **dropout** 的技巧：


1. **dropout** 可以对不同网络层设置不同的 **keep-prob** 值，比如担心某些层更容易发生过拟合比如拥有更多的网络参数），那么可以设置更低的 **keep-prob** 值，但缺点是**为了使用交叉验证，需要搜集更多的参数**；
1. 可以在一些网络层使用 **dropout**，一些层不用使用。



目前在计算机视觉领域中使用 **dropout** 会比较频繁，这也是由于输入是图像，一般像素都会非常多，网络参数也很多，加上用的网络层也会越来越深，所以也需要应用正则化技术，但需要记住它只是一种正则化方法，是用来防止过拟合的，如果没有过拟合，那么可以不需要使用。


当然 **dropout** 也是存在一个缺点，就是导致代价函数 J 不再被明确定义，因为每次迭代都会随机删除一些神经元，如果再三检查梯度下降的性能，实际上是很难进行复查的，这导致调试会比较困难。

解决办法可以是先关闭 **dropout** 函数，设置 **keep-prob** 为 1，运行代码，保证 J 单调递减，然后再打开 **dropout** 函数，希望使用它的过程中没有引入 bug。



##### dropout 概率的选择

1. 经过交叉验证，隐含节点 dropout 率等于0.5的时候效果最好，**原因是0.5的时候dropout随机生成的网络结构最多**。
2. dropout 也可以被用作一种**添加噪声**的方法，直接对 input 进行操作、输入层设为更更接近 1 的数。使得输入变化不会太大（0.8）
3. 对参数w的训练进行**球形限制(max-normalization)**，对 dropout 的训练非常有用。
4. 使用 pretrain 方法也可以帮助 dropout 训练参数，在使用dropout时，要将所有参数都乘以1/p。




#### 早停(Early Stopping)


还有一种正则化方法，就是早停，即在观察到模型的训练集误差和验证集误差变得越来越大的时候，提早终止训练过程，从而防止陷入过拟合，如下所示：


![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/early_stopping.png)


当然这种做法也有一个缺点，不能同时得到更小的 J 和避免过拟合，因为提早停止意味着停止优化代价函数 J ，这导致其数值可能不够小，但如果不提早停止则可能会出现过拟合，所以不能做到两全其美。


当然，早停机制的优点就是只需要运行一次梯度下降，就可以找出 w 的较小值、中间值和较大值，并不需要尝试 L2 正则化超级参数 $\lambda$ 的很多值。


#### 数据扩增(Data augmentation)


另一种防止过拟合的办法，数据扩增，也就是增加训练数据，因为有时候直接增加数据的代价很大，甚至无法实现，这时候可以采用数据扩增的方法，比如水平翻转、旋转、随机裁剪图片等，来增加更多的数据量。


这种数据扩增方法就是人工合成假数据的，这种额外的假数据无法包含像全新数据那么多的信息，但是代价几乎为零，除了一些对抗性代价，基本没有花费，比较廉价。



### 1.5 归一化输入(Normalizing inputs)


**训练神经网络的一个加速训练方法，就是归一化输入。**


归一化分为两步：


1. 零均值；
1. 归一化方差。

我们希望训练集和测试集都是通过相同的均值 $\mu$ 和方差 $\sigma^2$ 定义的数据转换，也就是相同的数据分布，而这两个参数可以通过训练集得到。

第一步，零均值化，计算公式如下所示，这是一个向量，其中 $x=x-\mu$，表示移动训练集，直到它完成零均值化。
$$
\mu=\frac{1}{m}\sum_{i=1}^mx^{(i)}
$$
第二步，归一化方差。
$$
\sigma^2=\frac{1}{m}\sum_{i=1}^m(x^{(i)})^2
$$

这里需要用在训练集数据得到的方差和均值来调整训练集和测试集，保证它们服从相同的数据分布。


归一化输入的好处在于避免输入数据的取值范围过大，如下图所示，当不同的特征的取值范围相差太大，在进行梯度下降法的时候，需要花费更多的迭代次数才能找到最小值。

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/normalize_input.png)




### 1.6 梯度消失/ 梯度爆炸(Vanishing / Exploding gradients)


梯度消失或者爆炸也是训练神经网络，特别是深度神经网络，会面临的一个问题。通常在没有很好初始化权重的时候，可能会导致梯度消失或者爆炸。


这里以一个有 L 层的神经网络为例，为了简单起见，假设每个网络层的偏置值 b=0，即只有权重 w，然后激活函数是 $g(z)=z$，另外每层的权重   的数值是相同的，比如都取值为 1.5。

基于这个简化版的假设，网络的输出为：
$$
y=W^{[l]}W^{[l-1]}\ldots W^{[3]}W^{[2]}W^{[1]}x
$$

因为权重值是 1.5 > 1，那么最终对于一些元素可以得到 $1.5^L$，这就会导致梯度爆炸的情况；相反，如果权重值小于 1.0，比如 0.5，那么就会存在某些梯度消失，因为 $0.5^L$ 就会变得非常小。


这都是因为在深度网络中，权重是以指数级递增或递减的原因。

**梯度消失或者爆炸都会让网络训练变得非常困难，因此初始化权重是非常重要的。**




### 1.7 神经网络的权重初始化


如何避免梯度消失或者爆炸呢？就是要合理的初始化权重，但是怎么进行合理的初始化呢，下面有几种做法。


**当然，实际上现在很多框架都已经实现了很多权重初始化的方法，直接调用即可。**


#### 使用较小的初始数值


比如，对于权重参数  W ，我们可以乘以一个比较小的数值，如 0.01，保证得到的初始化参数比较小：


```python
W = numpy.random.randn(shape) * 0.01
```


这种做法的原因是，如果你才有的是 `sigmoid` 激活函数，当初始化数值非常大的时候，梯度就会变得很小。


#### 神经元越多，权重越小

这种做法就是当神经元数量越多，我们需要将权重的初始化数值设置得更小来防止梯度消失或者爆炸。如下图所示，![](https://g.yuque.com/gr/latex?z%3Dw_1x_1%2Bw_2x_2%2B...%2Bw_nx_n#card=math&code=z%3Dw_1x_1%2Bw_2x_2%2B...%2Bw_nx_n) ，这里就希望每项数值更小。

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/weight_init.png)


所以权重初始化公式如下所示：


```python
W = numpy.random.randn(shape) * numpy.sqrt(1/n[l-1])
```


公式中第二项也就是$\sqrt{\frac{1}{n^{[l-1]}}}\cdot n^{[l-1]}$   表示前一层的神经元数量。


如果是采用 **Relu** 激活函数，那么推荐使用$\sqrt{\frac{2}{n^{[l-1]}}}$ 。



#### Xavier Initialization

**Xavier** 也是目前非常常用的权重初始化方法，当采用 **tanh** 激活函数的时候，可以使用 **Xavier** 使用的 $\sqrt{\frac{1}{n^{[l-1]}}}$ 或者 $\sqrt{\frac{2}{n^{[l-1]}}+n^{[l]}}$。



### 1.8 梯度检验的注意事项


下面是在神经网络中实施梯度检验的实用技巧和注意事项：


1. 不要在训练中使用，它只用于调试；
1. 如果算法的梯度检验失败了，要检查所有项，逐个检查，并试着找出 **bug**；
1. 实施梯度检验的时候，如果使用了正则项，请注意正则项；
1. 梯度检验不能和 **dropout** 同时使用，因为 **dropout** 会在每次迭代过程中随机删除神经元，难以计算 **dropout** 在梯度下降上的代价函数 J ；
1. 比较微妙的一点，在现实中几乎不会出现这种情况，如果随机初始化权重的数值比较小，反复训练网络后，再重新运行梯度检验。



### 1.9 Batch size 相关

#### 为什么需要 Batch_Size？

Batch 的选择，首先**决定的是下降的方向**。

如果数据集比较小，可采用全数据集的形式，好处是：

1. 由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向。
2. 由于不同权重的梯度值差别巨大，因此选取一个全局的学习率很困难。Full Batch Learning 可以使用 Rprop 只基于梯度符号并且针对性单独更新各权值。

对于更大的数据集，假如采用全数据集的形式，坏处是：

1. **随着数据集的海量增长和内存限制**，一次性载入所有的数据进来变得越来越不可行。
2. 以 Rprop 的方式迭代，会由于各个 Batch 之间的采样差异性，各次梯度修正值相互抵消，无法修正。这才有了后来 RMSProp 的妥协方案。



#### 值的选择

假如每次只训练一个样本，即 Batch_Size = 1。线性神经元在均方误差代价函数的错误面是一个抛物面，横截面是椭圆。对于多层神经元、非线性网络，在局部依然近似是抛物面。此时，每次修正方向以各自样本的梯度方向修正，横冲直撞各自为政，难以达到收敛。

既然 Batch_Size 为全数据集或者Batch_Size = 1都有各自缺点，可不可以选择一个适中的Batch_Size值呢？

此时，可采用批梯度下降法（Mini-batches Learning）。因为如果数据集足够充分，那么用一半（甚至少得多）的数据训练算出来的梯度与用全部数据训练出来的梯度是几乎一样的。



#### 在合理范围内，增大Batch_Size有何好处？

1. **内存利用率提高了**，大矩阵乘法的并行化效率提高。
2. 跑完一次 epoch（全数据集）所需的迭代次数减少，**对于相同数据量的处理速度进一步加快**。
3. 在一定范围内，**一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小**。

####   

#### 盲目增大 Batch_Size 有何坏处？

1. 内存利用率提高了，但是**内存容量可能撑不住了**。
2. 跑完一次 epoch（全数据集）所需的迭代次数减少，**要想达到相同的精度，其所花费的时间大大增加了**，从而对参数的修正也就显得更加缓慢。
3. Batch_Size 增大到一定程度，**其确定的下降方向已经基本不再变化**。







---

## 2. 优化算法


### 2.1 Mini-batch 梯度下降


当我们有一个数据量很大的数据集，如果全部一起训练的话，那么完整训练一个 epoch 就会需要很长的时间。但我们可以将数据集划分成多个子集，然后依次训练每个子集，计算每个子集的损失和梯度，更新权重。

这样的每个子集，就叫做 **mini-batch**，如下所示，展示一个训练集被划分成多个 **mini-batch**，其中 m 表示训练集的数量：

$[X^{(1)},\ldots,X^{(1000)}],[X^{(10001)},\ldots,X^{(2000)}],\ldots,[\ldots,X^{(m)}]$


对于 **mini-batch** 的梯度下降实现过程如下所示：


```python
for t=(1, ..., Batches):
  对第 t 个 batch 的样本进行前向传播；
  计算该样本的 loss；
  进行反向传播，计算第 t 个 batch 的梯度并更新网络的权重
```


这个训练过程中，loss 的下降趋势会更加平滑，对比直接用整个训练集来计算梯度和 loss 的过程，如下图所示：


![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/mini_batch_loss.png)


对于 **mini-batch**，很重要的选择就是 **batch** 的大小：


- 如果设置 **batch=m**，m 是训练集的数量，那么这就是**批量梯度下降(Batch Gradient Descent)**，也就是直接用所有训练集数据计算梯度；
- 如果设置 **batch=1**，那么就是**随机梯度下降(Stochastic Gradient Descent)**。



实际应用中，选择的 **batch** 都是在 1 和 m 之间，根据 m 的大小，有这么几种选择：


- 如果 m 小于 2000，这个数据集就是一个比较小的数据集，可以直接用批量梯度下降法；
- 如果 m 大于 2000，那么应该用的就是 **mini-batch** 梯度下降，而经典的 **batch** 大小设置一般是 64，128，256 等，也就是通常是 2 的 n 次方，这样主要是考虑内存设置和使用方式；



对于上述三种梯度下降法，其下降趋势如下图所示：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/mini_batch_gradient.png)


对于这三种梯度下降法，各有各的优缺点：


1. **批量梯度下降法**：
   - 优点：
   - 缺点：每次迭代需要计算所有的训练数据，如果训练数据数量太大，单次迭代耗时太长；
2. **mini-batch 梯度下降**
   - 优点：
   - 缺点：
3. **随机梯度下降**：
   - 优点：
   - 缺点：每次只处理一个样本，效率过低；



### 2.2 动量梯度下降(Gradient descent with Momentum)


动量梯度下降法的运行速度**几乎总是快于标准的梯度下降法**，**其基本思想就是计算梯度的指数加权平均数，并利用该梯度更新你的权重。**


动量梯度下降法的实现步骤是这样的，在每次迭代 t 中：


1. 计算每个 mini-batch 里的权重和偏置值的微分： dW，db；
1. 参数的更新公式如下所示：

$$
V_{dW}=\beta V_{dW} + (1-\beta)dW\\
V_{db}=\beta V_{db} + (1-\beta)db\\
W:=W-\alpha V_{dW}\\
b:=b-\alpha V_{db}
$$




这里两个超参数，分别是学习率 $\alpha$，以及控制指数加权平均数的参数 ，$\beta$ 最常用的 $\beta$ 数值是 0.9。

而 $V_{dW}$ 表示的是权重过去梯度历史的信息，如果设置 $\beta=0.9$，它将是平均前面十次迭代的梯度，也就是会考虑前面10次迭代的梯度来更新本次迭代的权重。如果这个参数设置为 0.999，那么考虑的就是前面 1000 次迭代了。




### 2.3 RMSprop


**RMSprop**，全程是 **root mean square prop** 算法，它也可以加速梯度下降。


其实现的步骤如下所示，在每次迭代 t 中：


1. 计算当前 batch 的中网络参数的微分 $dW, db$；
1. 参数更新的公式如下所示：

$$
S_{dW}=\beta S_{dW} + (1-\beta)(dW)^2\\
S_{db}=\beta V_{db} + (1-\beta)(db)^2\\
W:=W-\alpha \frac{dW}{\sqrt{S_{dW}}+\epsilon}\\
b:=b-\alpha \frac{db}{\sqrt{S_{db}}+\epsilon}
$$




参数中 $\epsilon$  是为了数值稳定，防止  $S_{dW}$ 趋近于 0，一般这个参数会设置为非常小的数值，比如 $10^{-8}$。

**RMSprop **跟 **Momentum** 有很相似的一点，可以**消除梯度下降中的摆动**，包括 **mini-batch** 梯度下降，并允许你使用一个更大的学习率，从而加快你的算法学习速度。




### 2.4 Adam


**Adam** 优化算法，全称是 **Adaptive Moment Estimation**, 基本上就是将 **Momentum** 和 **RMSprop** 结合在一起，其实现如下。

首先，初始化参数，$V_{dW}=0,S_{dW}=0,V_{db}=0,S_{db}=0$, 然后每次迭代 t 中：


1. 计算微分 $dW, db$;
1. 计算 **Momentum**

$$
V_{dW}=\beta_1V_{dW}+(1-\beta_1)dW\\
V_{db}=\beta_1V_{db}+(1-\beta_1)db
$$

计算 **RMSprop**
$$
S_{dW}=\beta_2S_{dW}+(1-\beta_2)(dW)^2\\
S_{db}=\beta_2S_{db}+(1-\beta_2)(db)^2
$$
偏差修正
$$
V^{correct}_{dW}=\frac{V_{dW}}{1-\beta_1^t}\\
V^{correct}_{db}=\frac{V_{db}}{1-\beta_1^t}\\
S^{correct}_{dW}=\frac{S_{dW}}{1-\beta_2^t}\\
S^{correct}_{db}=\frac{S_{db}}{1-\beta_2^t}\\
$$
更新参数
$$
W:=W-\alpha \frac{V_{dW}^{correct}}{\sqrt{S^{correct}_{dW}}+\epsilon}\\
b:=b-\alpha \frac{V_{db}^{correct}}{\sqrt{S^{correct}_{db}}+\epsilon}
$$


一般使用 **Adam** 算法的时候，要计算偏差修正，具体的介绍可以观看吴恩达老师 deeplearning.ai 课程优化算法中指数加权平均修正这节课程，课程视频地址：




youtube：[https://www.youtube.com/watch?v=lWzo8CajF5s](https://www.youtube.com/watch?v=lWzo8CajF5s)

b站：




这个做法目的是让指数加权平均操作更加精确。

通常，公式里使用到的超参数的默认设置是这样的，$\beta_1=0.9，\beta_2=0.999,\epsilon=10^{-8}$。


而学习率  $\alpha$  则需要通过调试来确定，当然更好的做法是采用学习率下降策略来自动调整学习率

**Adam **算法结合了 **Momentum** 和 **RMSprop** 梯度下降法，并且是一种极其常用的学习算法，被证明能有效适用于不同神经网络，适用于广泛的结构。




### 2.5 学习率衰减策略


为了加快训练速度，可以考虑随时间慢慢减小学习率，这称为学习率衰减策略。

假设训练时候如果采用的是固定的学习率，那么其 loss 的下降过程如下图所示，波动会非常大，并且迭代过程中会有噪音，而且并不会精确地收敛，算法最后会在最小值附近摆动。主要也是因为用的是一个固定的学习率，而不同的 **mini-batch** 中会有噪音。

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/learning_rate_decay_methods.png)


因此需要一种学习率衰减策略，并且学习率衰减策略的本质应该是在学习初期可以承受较大的下降，但开始收敛的时候，较小的学习率可以让下降速度减慢，即开始学习率可以下降较多，让 loss 尽快下降到最小值附近，然后通过较小的学习率来逐渐逼近最小值，并达到最小值。

**最直接的学习率衰减策略就是根据训练迭代次数的下降**，也就是 epoch （或者是总的训练迭代次数也可以），其下降公式如下所示：
$$
\alpha=\frac{1}{1+DecayRate * EpochNumber}\alpha_0
$$



其中 DecayRate 是衰减率，$\alpha_0$  是初始学习率，而 EpochNumber 是训练 epoch 次数。这里衰减率就是需要我们调整的一个超参数。


举个例子，初始学习率 $\alpha_0=0.2$ ，然后衰减率 DecayRate 是 1.0，那么每个 epoch 的学习率如下所示：

| Epoch | $\alpha$ |
| :---: | :---: |
| 1 | 0.1 |
| 2 | 0.067 |
| 3 | 0.05 |
| 4 | 0.04 |
| 5 | ... |



当然还有其他的学习率衰减策略：


1. **指数衰减**：$\alpha=0.95^{EpochNumber}\alpha_0$
1. **基于 epoch 的衰减**：$\alpha=\frac{k}{EpochNumber}\alpha_0$    
1. **基于 mini-batch 的衰减**：![](https://g.yuque.com/gr/latex?%5Calpha%20%3D%20%5Cfrac%7Bk%7D%7Bt%7D%5Calpha_0#card=math&code=%5Calpha%20%3D%20%5Cfrac%7Bk%7D%7Bt%7D%5Calpha_0)，t 就是 batch 的大小
1. **离散下降（discrete stair cease）**：如下图所示：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/discrete_stair_cease.png)


5. **手动衰减**：也就是先用一个学习率训练模型，观察训练速度，如果训练速度变慢了，可以将学习率减小一些；这种方法也是有效的，但是前提是模型数量很小的时候。



---

## 3. 超参数调试


### 3.1 超参数调试方法


深度学习网络中有不少超参数需要进行调试，比如学习率、batch的大小等，通常有两种调试超参数的做法：


1. 计算资源不足的时候，只能一次照看一个模型，每天观察它的表现，耐心地调试学习率，如下左图就是这种情况；
1. 有足够的计算资源，可以同时试验多种模型，这种情况就可以调试不同的超参数或者选择对同一个超参数试验不同的取值，比如下面右图就是同时试验了 5 种模型。

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/hyper_parameter_tuning.png)


所以，计算资源不足的时候，就需要考虑哪些超参数更加重要，需要优先作为调试的参数，下面是吴恩达老师推荐的不同超参数的优先级等级：

| 优先级 | 超参数 |
| :---: | :---: |
| 1 | 学习率$\alpha$ |
| 2 | $\beta_1, \beta_2, \epsilon$ （Momentum 和 RMSprop 的超参数） |
| 2 | 隐藏层的数量 |
| 2 | batch 大小 |
| 3 | 网络层的数量 |
| 3 | 学习率衰减的数量 |



注意，一般 Momentum 和 RMSprop 的三个参数默认值是 $\beta_1=0.9,\beta_2=0.99,\epsilon=10^{-8}$


确定调试的参数的优先级，接下来就是如何选择参数的取值，更高效的找到参数的最优数值呢？



#### 随机均匀的取值

这种方法主要是用于网络层数量和神经元数量的调试，假设网络层数量的取值范围是 [2, 6]，那么可以逐个取2，3，4，5，6 来进行调试，或者神经元数量取值范围是 [50, 100]，那么可以按照 50，60，70，80，90，100 这样取值来调试，也就是均匀的取值，如下图所示：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/hyper_parameter_tuning_units_and_layers.png)





#### 采用对数标尺取值


但如果取值范围比较大，跨越数量级的，比如对于学习率的取值范围可能是 [0.0001, 1]，即$[10^{-4},10^0]$ ，如果按照上述随机均匀取值的话，那这种做法就很低效了。

所以更好的做法是采用对数标尺，即令$\alpha=10^r, r\in[-4,0]$，然后分别让 r 取 -4，-3，-2，-1，0 即可，这样就对应学习率分别取值为 0.0001，0.001, 0.01, 0.1 和 1 了，这种做法相当于做了一个变换，在对数轴上进行均匀取值了。

如果是 Momentum 和 RMSprop 的超参数 $\beta_1, \beta_2$，则可以这么进行转换操作：


令 $1-\beta=10^r=>\beta=1-10^r, r\in[-3,-1]$


结果可以如下所示：

| $\beta$ | 0.9 | 0.99 | 0.999 |
| :---: | :---: | :---: | :---: |
| $1-\beta$ | 0.1 | 0.01 | 0.001 |
| r | -1 | -2 | -3 |



上述的 $\alpha$ 和 $\beta_1, \beta_2$ 的例子如下图所示：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/hyper_parameter_tuning_alpha_and_beta.png)





### 3.2 Batch Normalization

Batch Normalization（BN) 是非常常用的一种正则化方法，它可以加快训练的速度，其实现如下所示：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/batch_normalization.png)




其中有：
$$
z^{[l]} = w^{[l]}a^{[l-1]} + b^{[l]}\\
a^{[l]}=f(z^{[l]})
$$



f() 表示激活函数，w，b 就是权重和偏置值。


究竟是在激活函数之前、还是之后进行`batch normalization`， 这个问题在文献中有一些争论。

**实践中，通常都是在激活函数之前进行的。**




具体实现细节，对于每个网络层 l 有：

$$
\mu = \frac{1}{m}Z^i \\
\delta^2 = \frac{1}{m}(Z^{(i)}-\mu)\\
Z^{(i)}_{normalized} = \alpha \frac{Z^{(i)}-\mu}{\sqrt \delta^2+\epsilon}+\beta
$$



这里 $\alpha, \beta$ 都是可学习的参数。


在测试阶段，并不会计算 $\mu, \delta$，因为可能只有一个测试样本。那么这种情况的做法就是**将 $\mu, \delta$  设置为训练阶段收集的运行均值（或者是指数加权均值）。**


#### BN 工作原理


1. 首先是和归一化输入特征的作用一样，`BN` 也是完成了同样的工作，对每层的输出归一化，使其取值范围限制在较小的范围内；
1. `BN` 表现良好的一个解释是：**内部协方差偏移**`Internal Covariate Shift:ICS` 会对训练产生负面影响，`BN` 能够减少`ICS`。
1. 内部协方差偏移：**低层网络的更新对当前层输入分布造成了改变**。



统计学习中一个经典假设是源空间`source domain` 和目标空间`target domain` 的数据分布一致。协方差偏移`covariate shift` 就是分布不一致假设之下的一个分支问题。它指的是：源空间和目标空间的条件概率是一致的，但是其边缘概率不同。即：对所有的 $x\in X$, 有 $P_s(Y|X=x)=P_t(Y|X=x)$，但是 $P_s(X)\neq P_t(X)$。


在神经网络中经过各层的作用，**各层输出与其输入分布会不同**。这种差异随着网络深度的增大而增大，但是它们能够“指示”的样本标记仍然不变，这就符合`covariate shift` 的定义。


由于是对层间信号的分析，这就是`internal` 的由来。


`ICS` 带来的问题是：**各个神经元的输入数据不再是独立同分布的**。


- 上层参数需要不断适应新的输入数据分布，降低了学习速度。
- 下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止。
- 每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。



3. 论文`《How Does Batch Normalization Help Optimization》 2018 Shibani Santurkar etc.` 说明`BN` 对训练带来的增益与`ICS` 的减少没有任何联系，或者说这种联系非常脆弱。研究发现：`BN` 甚至不会减少`ICS` 。



论文说明 `BN` 的成功的真正原因是：**它使得优化问题的解空间更加平滑了。这确保梯度更具有预测性，从而允许使用更大范围的学习率，实现更快的网络收敛**。


4. **`BN` 还有轻微的正则化效果。**因为 `BN` 的计算是对每个 mini-batch 进行计算均值和方差，并不是整个数据集，所以均值和方差会有一些小的噪声，这种做法就给神经元添加了噪音，使得每个神经元都不过分依赖其他的神经元，但这个效果非常轻微，而且如果使用较大的 mini-batch，那么这种效果还会别减弱。



#### BN 性质


1. `BN` 独立地归一化每个输入维度，它要求每个`mini batch` 的统计量是整体统计量的近似估计。



因此`BN` 要求每个`mini-batch` 都比较大，而且每个`mini-batch` 的数据分布比较接近。所以在训练之前，需要对数据集进行充分混洗，否则效果可能很差。


2. 当验证或者测试的`batch size` 较小时（如：只有一个测试样本），此时无法得到`mini batch` 的统计量，或者`mini batch` 统计量无法作为整体统计量的近似估计。
此时的做法是：先通过训练集上得到的所有 `mini batch` 的统计量的移动平均值，然后将它们作为验证或者测试时的`mini batch` 的统计量。
但是当训练数据、验证数据、测试数据的数据分布存在差别时（如：训练数据从网上爬取的高清图片，测试数据是手机拍照的图片），训练集上预先计算好的`mini batch` 的统计量的移动平均值并不能代表验证集、测试集的相应的统计量。这就导致了训练、验证、测试三个阶段存在不一致性。
2. `BN` 存在两个明显不足：

- **高度依赖于 mini-batch 的大小**，**它要求 batch 比较**大，更好发挥它的效果，所以不适合 batch 较小的场景，比如在线学习（batch=1）
- **不适合 RNN 网络**。因为不同样本的 `sequence` 的长度不同，因此`RNN` 的深度是不固定的。同一个`batch` 中的多个样本会产生不同深度的`RNN`，因此很难对同一层的样本进行归一化。



### 3.3 softmax和 softmax loss


参考：


- [Softmax 和 Softmax-loss的推演](https://blog.csdn.net/xg123321123/article/details/52716890)
- [简单谈谈Cross Entropy Loss](https://blog.csdn.net/xg123321123/article/details/80781611)
- [详解softmax函数以及相关求导过程](https://zhuanlan.zhihu.com/p/25723112)
- [Softmax回归 - Ufldl](http://deeplearning.stanford.edu/wiki/index.php/Softmax%E5%9B%9E%E5%BD%92)



### 3.4 深度学习框架


目前有很多深度学习框架，比如：


- TensorFlow
- PyTorch/Torch
- Keras
- Caffe/Caffe2
- CNTK
- DL4J
- mxnet
- PaddlePaddle
- Theano



面对这些框架的选择标准有这几个：


1. **便于开发编程**。包括对神经网络的开发和迭代，也包括为产品进行配置；
1. **运行速度**。特别是训练大数据集时，可以高效运行和训练模型；
1. **是否真的开放**。不仅需要开源，而且需要良好的管理。

---

## 4. 机器学习策略


首先给一个启发的例子，比如你正在训练一个猫分类器，经过一段时间的训练，分类准确率达到 90%，但还达不到预设的目标或者业务的需求，那么就需要继续优化你的模型，优化的方法有不少，比如：


1. 数据方面：可以增加数量，或者增加数据的多样性，比如收集更多不同姿势的猫的图片，或者增加多样化的负样本；
1. 优化算法方面：使用更久的梯度下降法进行训练，或者换用 Adam ；
1. 网络结构规模方面：使用更大的或者更小的网络模型，比如从 resent50 换成 resent101，或者是 resnet18；
1. 正则化角度：采用 dropout 或者 L2 正则化；
1. 网络结构方面：修改激活函数，改变网络层中神经元的数量，或者是更换损失函数。



这些方法可能有效果，但也可能提升不明显，甚至效果更差，所以这需要能快速判断哪些方法是靠谱的，可以值得一试的，或者提出新的更好的方法，这就是有这个机器学习策略的原因，这些策略可以帮助你们快速找到有效的，值得一试的方法。


### 1. 错误分析


进行错误分析，看看错误样本中假阳性和假阴性各自占的比例，然后看看错误划分的原因，归纳出错误类型，统计属于不同错误类型的错误数量，以及各自的占比，这样可以找到占比较大的错误问题，也就是需要优先解决的问题。


### 2. 清除标注错误的数据


在数据集中可能存在标注错误的样本，一般来说如果是随机的错误，**深度学习算法对于随机错误是相当健壮的**，但是如果**是系统性的错误那就没那么健壮**，也就是说如果标记错误的样本都是将同一类错误标记，比如将所有的白色的狗标记为猫，那么算法就会把白色的狗当做猫这个类别。


如果标注错误是造成算法准确率较差的主要原因，那么就需要修正标注错误的样本，这里有几个方针和原则。


1. **必须同时在训练集、验证集和测试集上修改标注错误的数据**。要保证它们都是同一个数据分布的；
1. **同时检查算法判断正确和错误的样本**。错误的样本里有标注错误的，但算法判断正确的样本也可能存在这种情况。
1. **可以考虑不修正训练集的数据标签，选择修正验证集和测试集的样本**。一般来说训练集数据量会更多，而验证集和测试集会小得多，选择不修正训练集的可以节省精力和时间，并且验证集和测试集来自同一个分布很重要，但训练集可以来自稍微不同的分布。



### 3. 数据不匹配问题


当训练集和验证集、测试集分布不一样的时候，可能存在**数据不匹配**的问题，即训练集误差是 1%，而验证集误差是 10%，这种情况如果两者分布一致，那是方差问题，也就是算法对训练集过拟合了；但如果分布不一致，那就是数据不匹配问题，比如训练集的数据比较简单，算法很容易识别分类，而验证集数据比较难。


那么如何解决数据不匹配问题呢，可以这么做：


1. 做错误分析，或者看训练集和验证集，看看两者数据分布有什么不同；
1. **收集更多看起来像验证集的数据作为训练集**，这里可以考虑人工合成数据，但需要注意可能只是从所有可能性的空间选择了很小的一部分进行模拟数据。



### 4. 迁移学习


迁移学习，简单来说就是从一个领域中学到知识，然后应用到另一个领域中。比如图像分类中常见的，采用在 ImageNet 上训练好的模型，然后通过微调，得到适用当前图片数据分类的模型。


什么时候适用迁移学习呢？


**假如在 A 领域有大量数据，并训练得到模型，然后需要解决一个新的 B 领域问题或者优化 算法在 B 领域的性能，以及两者有相同的输入，比如都是图像，但该领域只得到少量的数据，这个时候就是迁移学习可以发挥作用的时候了**。


### 5. 多任务学习


多任务学习，顾名思义就是需要学习多项任务，在图像分类中就是多标签分类一样，即对同一张图片，输出的不只是单个类别，而是多个类别的情况，比如给定一张衣服的图片，不只是输出图片中衣服是上衣，裤子还是鞋子，同时还需要判断是什么颜色，纹理，材料等。


什么时候适用多任务学习呢？


1. **这些任务都可以共用低层次特征**。
1. **每个任务的数据加起来都比单个任务的数据量要更多**，但这个不是必须满足的条件，这个条件只是让其他任务的知识可以帮助改善单独的一个任务；
1. **可以训练一个足够大的神经网络，并且能同时做好所有的工作**。多任务学习会降低性能的唯一情况，和训练单个神经网络相比性能更低的情况就是你的神经网络还不够大。



不过，相比迁移学习，多任务学习的使用频率还是很少，主要是第三点的问题，多数情况我们很难训练出一个足够大的网络，来保证比单独为每个任务训练一个网络的性能要更好。


### 6. 端到端的深度学习


端到端的深度学习就是将多个步骤都融合到同一个神经网络中，一个网络就可以实现这些步骤，常见的就是目标检测里面 one-stage 和 two-stage 的区别了。


端到端的深度学习的优点：


1. **端到端学习真的只是让数据说话**。当有足够多的数据的时候，就可以训练一个足够大的神经网络来完成任务；
1. **需要手工设计的组件更少**，可以简化设计工作流程。



缺点则是：


1. **需要大量的数据**。
1. **可能会排除有用的手工设计组件**。



另外，有的时候之所以分步完成任务，也是分步实现的效果会更好：


1. 拆分的几个问题实际上会更加简单
1. 单独的子任务的训练数据更多。



### 7. 数据增强


类别不平衡学习笔记：


[https://www.yuque.com/u164072/wnatqs/tgey2f](https://www.yuque.com/u164072/wnatqs/tgey2f)


数据增强学习笔记：


[https://www.yuque.com/u164072/wnatqs/wb3ogp](https://www.yuque.com/u164072/wnatqs/wb3ogp)


# 激活函数


学习笔记：[https://www.yuque.com/u164072/wnatqs/nsqq9o](https://www.yuque.com/u164072/wnatqs/nsqq9o)


# CNN 介绍


学习笔记：


[https://www.yuque.com/u164072/wnatqs/kemt8t](https://www.yuque.com/u164072/wnatqs/kemt8t)


# RNN&LSTM


学习笔记：[https://www.yuque.com/u164072/wnatqs/vexanq](https://www.yuque.com/u164072/wnatqs/vexanq)


# GAN


学习笔记：


[https://www.yuque.com/u164072/wnatqs/ozqgh3](https://www.yuque.com/u164072/wnatqs/ozqgh3)


# 经典的分类模型


学习笔记：


1. [https://www.yuque.com/u164072/wnatqs/tkhkfx](https://www.yuque.com/u164072/wnatqs/tkhkfx)
1. Inception 介绍：[https://www.yuque.com/u164072/wnatqs/opcbn0](https://www.yuque.com/u164072/wnatqs/opcbn0)
1. ResNet：[https://www.yuque.com/u164072/wnatqs/qp374t](https://www.yuque.com/u164072/wnatqs/qp374t)
1. NASNet 学习笔记：[https://www.yuque.com/u164072/wnatqs/yc11om](https://www.yuque.com/u164072/wnatqs/yc11om)
1. 轻量化网络：https://www.yuque.com/u164072/wnatqs/ms5elk





---

# 图像分类


## 多标签图像分类


学习笔记：

[https://www.yuque.com/u164072/wnatqs/axh96r](https://www.yuque.com/u164072/wnatqs/axh96r)



## Siamese 网络



## Triplet loss






# 目标检测


学习笔记：


1. [https://www.yuque.com/u164072/wnatqs/chhw6g](https://www.yuque.com/u164072/wnatqs/chhw6g)
1. 场景文本检测识别学习笔记：[https://www.yuque.com/u164072/wnatqs/wtstlp](https://www.yuque.com/u164072/wnatqs/wtstlp)





# 人脸识别





# 神经风格转换


## 内容代价函数（Content cost function）



## 风格代价函数（Style cost function）





# 模型压缩&加速&移动端部署

https://www.yuque.com/u164072/wnatqs/rysabi





---

# 参考


1. 吴恩达老师的 deeplearning.ai 课程
2. [deeplearning.ai 学习笔记](http://www.ai-start.com/dl2017/)
3. [https://createmomo.github.io/2018/01/23/Super-Machine-Learning-Revision-Notes/](https://createmomo.github.io/2018/01/23/Super-Machine-Learning-Revision-Notes/)
4. [http://www.huaxiaozhuan.com/](http://www.huaxiaozhuan.com/)
5. https://mp.weixin.qq.com/s/bsfYCaOKHNsbRcH83970fQ



