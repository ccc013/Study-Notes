### 1. 机器学习中为什么需要梯度下降

梯度下降是机器学习中常见优化算法之一，梯度下降法有以下几个作用：

1. 梯度下降是一种求解函数局部极小值的迭代优化算法，可以用于求解最小二乘问题。
2. 在求解机器学习算法的模型参数，即无约束优化问题时，主要有梯度下降法（Gradient Descent）和最小二乘法。
3. 在求解损失函数的最小值时，可以通过梯度下降法来一步步的迭代求解，得到最小化的损失函数和模型参数值。
4. 如果我们需要求解损失函数的最大值，可通过梯度上升法来迭代。梯度下降法和梯度上升法可相互转换。
5. 在机器学习中，梯度下降法主要有随机梯度下降法和批量梯度下降法。





### 2. 梯度下降法缺点

梯度下降法缺点有以下几点：

（1）靠近极小值时收敛速度减慢。

（2）直线搜索时可能会产生一些问题。

（3）可能会“之字形”地下降。

梯度概念也有需注意的地方：

（1）梯度是一个向量，即有方向有大小。 

（2）梯度的方向是最大方向导数的方向。 

（3）梯度的值是最大方向导数的值。



### 3. 梯度下降法直观理解

梯度下降法经典图示如下图2.7所示：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E7%A4%BA%E4%BE%8B.png)

​								

形象化举例，由上图所示，假如最开始，我们在一座大山上的某处位置，因为到处都是陌生的，不知道下山的路，所以只能摸索着根据直觉，走一步算一步，在此过程中，每走到一个位置的时候，都会求解当前位置的梯度，沿着梯度的负方向，也就是当前最陡峭的位置向下走一步，然后继续求解当前位置梯度，向这一步所在位置沿着最陡峭最易下山的位置走一步。不断循环求梯度，就这样一步步地走下去，一直走到我们觉得已经到了山脚。当然这样走下去，有可能我们不能走到山脚，而是到了某一个局部的山势低处。


由此，从上面的解释可以看出，梯度下降不一定能够找到全局的最优解，有可能是一个局部的最优解。当然，如果损失函数是凸函数，梯度下降法得到的解就一定是全局最优解。

**核心思想归纳**：

1. 初始化参数，随机选取取值范围内的任意数；

2. 迭代操作：
	- 计算当前梯度；
	- 修改新的变量；
	- 计算朝最陡的下坡方向走一步；
	- 判断是否需要终止，如否，重新开始计算梯度，然后继续重复这个步骤；

3. 得到全局最优解或者接近全局最优解。



### 4. 梯度下降法算法描述

梯度下降法算法步骤如下：

（1）确定优化模型的假设函数及损失函数。
举例，对于线性回归，假设函数为：
$$
  h_\theta(x_1,x_2,...,x_n)=\theta_0+\theta_1x_1+...+\theta_nx_n
$$

其中，$\theta_i,x_i(i=0,1,2,...,n)$分别为模型参数、每个样本的特征值。

对于假设函数，损失函数为：
$$
  J(\theta_0,\theta_1,...,\theta_n)=\frac{1}{2m}\sum^{m}_{j=0}(h_\theta (x^{(j)}_0
  	,x^{(j)}_1,...,x^{(j)}_n)-y_j)^2
$$

（2）相关参数初始化。

主要初始化${\theta}_i$、算法迭代步长${\alpha} $、终止距离${\zeta} $。初始化时可以根据经验初始化，即${\theta} $初始化为0，步长${\alpha} $初始化为1。当前步长记为${\varphi}_i $。当然，也可随机初始化。

（3）迭代计算。

1）计算当前位置时损失函数的梯度，对${\theta}_i $，其梯度表示为：
$$
\frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{2m}\sum^{m}_{j=0}(h_\theta (x^{(j)}_0
	,x^{(j)}_1,...,x^{(j)}_n)-y_j)^2
$$

2）计算当前位置下降的距离。
$$
{\varphi}_i={\alpha} \frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)
$$

3）判断是否终止。
确定是否所有${\theta}_i$梯度下降的距离${\varphi}_i$都小于终止距离${\zeta}$，如果都小于${\zeta}$，则算法终止，当然的值即为最终结果，否则进入下一步。

4）更新所有的${\theta}_i$，更新后的表达式为：
$$
{\theta}_i={\theta}_i-\alpha \frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)
$$

$$
\theta_i=\theta_i - \alpha \frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{(j)}_0
	,x^{(j)}_1,...,x^{(j)}_n)-y_j)x^{(j)}_i
$$

5）令上式$x^{(j)}_0=1$，更新完毕后转入1)。

由此，可看出，当前位置的梯度方向由所有样本决定，上式中 $\frac{1}{m}$、$\alpha \frac{1}{m}$ 的目的是为了便于理解。



### 5. 如何对梯度下降法进行调优

实际使用梯度下降法时，各项参数指标不能一步就达到理想状态，对梯度下降法调优主要体现在以下几个方面：

#### 5.1 学习率$\alpha$选择

在算法参数初始化时，有时根据经验将学习率初始化为1。实际取值取决于数据样本。可以从大到小，多取一些值，分别运行算法看迭代效果，如果损失函数在变小，则取值有效。如果取值无效，说明要增大步长。但步长太大，有时会导致迭代速度过快，错过最优解。步长太小，迭代速度慢，算法运行时间长。如下图所示分别是不同的学习率情况：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%8E%87.png)

- a 图的学习率最优，模型成功收敛到最小值；
- b 图的学习率太小，需要花费更多时间，但最终是收敛到最小值；
- c 图的学习率高于最优值，也是以较慢速度来收敛；
- d 图的学习率过大，所以会过度偏离，远离了最小值，无法收敛。

对于不同的学习率，随着训练次数的增加，loss 增加的情况如下所示：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95%E5%AD%A6%E4%B9%A0%E7%8E%872.png)







#### 5.2 参数的初始值选择

初始值不同，获得的最小值也有可能不同，梯度下降有可能得到的是局部最小值。**如果损失函数是凸函数，则一定是最优解**。

由于有局部最优解的风险，需要多次用不同初始值运行算法，关键损失函数的最小值，选择损失函数最小化的初值。

一般对于不带约束条件的优化问题，可以将初始值设置为 0，或者随机数。而对于神经网络，则一般设置为随机数。



#### 5.3 标准化处理

由于样本不同，特征取值范围也不同，导致迭代速度慢。为了减少特征取值的影响，可对特征数据标准化，使新期望为0，新方差为1，**可节省算法运行时间**。



### 6. 随机梯度和批量梯度区别

随机梯度下降（SGD）和批量梯度下降（BGD）是两种主要梯度下降法，其目的是增加某些限制来加速运算求解。
下面通过介绍两种梯度下降法的求解思路，对其进行比较。
假设函数为：
$$
h_\theta (x_0,x_1,...,x_3) = \theta_0 x_0 + \theta_1 x_1 + ... + \theta_n x_n
$$
损失函数为：
$$
J(\theta_0, \theta_1, ... , \theta_n) = 
			\frac{1}{2m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
	,x^{j}_1,...,x^{j}_n)-y^j)^2
$$
其中，$m$为样本个数，$j$为参数个数。



1、**批量梯度下降的求解思路如下：**
a) 得到每个$ \theta $对应的梯度：
$$
\frac{\partial}{\partial \theta_i}J({\theta}_0,{\theta}_1,...,{\theta}_n)=\frac{1}{m}\sum^{m}_{j=0}(h_\theta (x^{j}_0
	,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i
$$
b) 由于是求最小化风险函数，所以按每个参数 $ \theta $ 的梯度负方向更新 $ \theta_i $ ：
$$
\theta_i=\theta_i - \frac{1}{m} \sum^{m}_{j=0}(h_\theta (x^{j}_0
	,x^{j}_1,...,x^{j}_n)-y^j)x^{j}_i
$$
c) 从上式可以注意到，它得到的虽然是一个全局最优解，但每迭代一步，都要用到训练集所有的数据，**如果样本数据很大，这种方法迭代速度就很慢**。

相比而言，随机梯度下降可避免这种问题。



2、**随机梯度下降的求解思路如下：**
a) 相比批量梯度下降对应所有的训练样本，随机梯度下降法中损失函数对应的是**训练集中每个样本的粒度**。

损失函数可以写成如下这种形式，
$$
J(\theta_0, \theta_1, ... , \theta_n) = 
			\frac{1}{m} \sum^{m}_{j=0}(y^j - h_\theta (x^{j}_0
			,x^{j}_1,...,x^{j}_n))^2 = 
			\frac{1}{m} \sum^{m}_{j=0} cost(\theta,(x^j,y^j))
$$
b）对每个参数 $ \theta$ 按梯度方向更新 $ \theta$：
$$
\theta_i = \theta_i + (y^j - h_\theta (x^{j}_0, x^{j}_1, ... ,x^{j}_n))
$$
c) 随机梯度下降是通过每个样本来迭代更新一次。

随机梯度下降伴随的一个问题是噪音较批量梯度下降要多，使得随机梯度下降并不是每次迭代都向着整体最优化方向。



**小结：**
随机梯度下降法、批量梯度下降法相对来说都比较极端，简单对比如下：

|     方法     | 特点                                                         |
| :----------: | :----------------------------------------------------------- |
| 批量梯度下降 | a）采用所有数据来梯度下降。<br/>b）批量梯度下降法在样本量很大的时候，训练速度慢。 |
| 随机梯度下降 | a）随机梯度下降用一个样本来梯度下降。<br/>b）训练速度很快。<br />c）随机梯度下降法仅仅用一个样本决定梯度方向，导致解有可能不是全局最优。<br />d）收敛速度来说，随机梯度下降法一次迭代一个样本，导致迭代方向变化很大，不能很快的收敛到局部最优解，效率会比较低。 |



下面介绍能结合两种方法优点的小批量梯度下降法。

3、 **小批量（Mini-Batch）梯度下降的求解思路如下**

对于总数为$m$个样本的数据，根据样本的数据，选取其中的$n(1< n< m)$个子样本来迭代。其参数$\theta$按梯度方向更新$\theta_i$公式如下：
$$
\theta_i = \theta_i - \alpha \sum^{t+n-1}_{j=t}
		( h_\theta (x^{j}_{0}, x^{j}_{1}, ... , x^{j}_{n} ) - y^j ) x^{j}_{i}
$$



首先，如果训练集较小，直接使用 **batch** 梯度下降法，这里的少是说小于 2000 个样本。一般的 **mini-batch** 大小为 64 到 512，考虑到电脑内存设置和使用的方式，如果 **mini-batch** 大小是 2 的𝑛次方，代码会运行地快一些。

对于调节 Batch 大小对模型的影响：

- Batch_Size 太小，模型表现效果极其糟糕(error飙升)。
- 随着 Batch_Size 增大，处理相同数据量的速度越快。
- 随着 Batch_Size 增大，达到相同精度所需要的 epoch 数量越来越多。
- 由于上述两种因素的矛盾， Batch_Size 增大到某个时候，**达到时间上的最优**。
- 由于最终收敛精度会陷入**不同的局部极值**，因此 Batch_Size 增大到某些时候，**达到最终收敛精度上的最优**。



### 7. 各种梯度下降法性能比较

下表简单对比随机梯度下降（SGD）、批量梯度下降（BGD）、小批量梯度下降（Mini-batch GD）、和Online GD的区别：

|                |    BGD     |   SGD    | Mini-batch GD |   Online GD    |
| :------------: | :--------: | :------: | :-----------: | :------------: |
|     训练集     |    固定    |   固定   |     固定      |    实时更新    |
| 单次迭代样本数 | 整个训练集 | 单个样本 | 训练集的子集  | 根据具体算法定 |
|   算法复杂度   |     高     |    低    |     一般      |       低       |
|     时效性     |     低     |   一般   |     一般      |       高       |
|     收敛性     |    稳定    |  不稳定  |    较稳定     |     不稳定     |

BGD、SGD、Mini-batch GD，前面均已讨论过，这里介绍一下Online GD。

Online GD 与 Mini-batch GD/SGD的区别在于，**所有训练数据只用一次，然后丢弃**。这样做的优点在于**可预测最终模型的变化趋势**。

Online GD 在互联网领域用的较多，比如搜索广告的点击率（CTR）预估模型，网民的点击行为会随着时间改变。用普通的BGD算法（每天更新一次）一方面耗时较长（需要对所有历史数据重新训练）；另一方面，无法及时反馈用户的点击行为迁移。而 Online GD 算法可以**实时的依据网民的点击行为进行迁移**。



### 8. 面临的问题

梯度下降法在实现的时候也会遇到一些问题，比较典型的就是局部极小值和鞍点问题。

局部极小值是指有的函数可能有多个局部极小值，如下图所示：

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/%E5%B1%80%E9%83%A8%E6%9E%81%E5%B0%8F%E5%80%BC.png)

上图存在 3 个局部极小值，但 A 才是全局极小值，但是梯度下降可能会在 B 或者 C 点就停止了。



鞍点是指梯度为0，Hessian矩阵既不是正定也不是负定，即不定的点。如下图所示：

<img src="https://gitee.com/lcai013/image_cdn/raw/master/notes_images/%E9%9E%8D%E7%82%B9%E5%9B%BE%E7%A4%BA.png" style="zoom:50%;" />

在鞍点的位置，梯度下降法可能认为已经找到了全局最小值，然后停止了迭代。

怎么解决上述两个问题呢，在后续的优化算法中有了一些解决办法，比如 Adam、AdaDelta等方法。这部分在后续优化算法中会提到。





------

### 参考

1. 深度学习 500 问：https://github.com/scutan90/DeepLearning-500-questions
2. Andrew Ng 在 Coursea 上的 [机器学习课程](https://www.coursera.org/learn/machine-learning)
3. [梯度下降算法的工作原理](https://mp.weixin.qq.com/s/2hO5LHOOVgxRvmJpAF1lvA)
4. [推荐收藏 | Dropout、梯度消失/爆炸、Adam优化算法，神经网络优化算法看这一篇就够了](https://mp.weixin.qq.com/s/HbQXB4NajXAAfUeglDe8XA)
5. [一文读懂梯度下降法](https://mp.weixin.qq.com/s/vPraRVs5Y6qzIe8-YogNfQ)

