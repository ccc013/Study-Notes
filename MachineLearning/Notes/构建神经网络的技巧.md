这里记录构建神经网络/卷积神经网络时候的一些经验、技巧等。

# 常用框架

来自：https://mp.weixin.qq.com/s/llpex8u9PQNbzxeVXc62hA

编程语言：python（⭐⭐⭐⭐⭐）

炼丹框架：Pytorch（⭐⭐⭐⭐⭐）、Keras(⭐⭐⭐⭐)、Mxnet(⭐⭐⭐)、Tensorflow（⭐）

必备框架：Apex（⭐⭐⭐⭐⭐）、Numpy、Opencv、PIL、Scikit-learn、albumentations、imgaug等

- Pytorch的易用性其实是已经超越了另外几个框架，搭建/魔改模型都非常方便；

- `Apex`可以让你只写几行代码，就可以轻松使用`float16`或者**混合精度**来训练模型，显存减少将近一半的情况下，训练速度也得到大幅度提升。

- 自写数据增强库，推荐使用`Opencv`；如果是封装好的数据增强库，推荐`albumentations`或`imgaug`（或`torchvison.transforms`），基本想得到的transform方式都包含。

## Apex 使用例子

```python
from apex import amp
import apex
...

model = resnet18()
optimizer = Adam(....)
model, optimizer = amp.initialize(model, optimizer, opt_level="O1", verbosity=0)
...
logits = model(inputs)
train_loss = criterion(logits, truth)
with amp.scale_loss(train_loss, optimizer) as scaled_loss:
    scaled_loss.backward()
optimizer.step()
```





# 通用 Tips

- **使用 ADAM 优化器**。它真的很好用。比起传统的优化方法，如原始的梯度下降，我更喜欢它。注意：如果要保存和恢复权重，记得在设置好`AdamOptimizer`之后设置`Saver` ，因为 ADAM 也有需要恢复的状态(即每个权重的学习率)。
- **ReLU 是最好的非线性(激活函数)**。这有点像说 Sublime 是最好的文本编辑器。但实际上，ReLUs 是快速的、简单的，而且令人惊讶的是，它们能够工作，并且没有梯度衰减的问题。虽然 sigmoid 是一种常见的教科书式激活函数，但它不能很好地通过 DNNs 传播梯度。
- **不要在输出层使用激活函数**。这应该是显而易见的，但这是一个很容易犯的错误，如果你用一个共享函数构建每个层：一定要在输出处*关闭*激活函数。
- **一定要在每一层添加一个偏差**。这是 ML 101：偏差本质上是将飞机转换成最佳位置。在`y=mx+b`中，b 是偏差，允许直线向上或向下移动到“最合适”的位置。
- **使用 variance-scaled 初始化**。在 Tensorflow 中，就像`tf.contrib.layers.variance_scaling_initializer()`。在我们的经验中，这比常规的高斯分布、截断的正太分布和 Xavier 更能泛化/缩放。粗略地说， variance scaling 初始化根据每一层的输入或输出的数量来调整初始随机权重的方差(TensorFlow 中的默认值是输入的数量)，从而帮助信号更深入地传播到网络中，而不需要额外的“技巧”，比如 clipping 或 batch normalization。Xavier 是很相似的方法，但是 Xavier 的所有层的方差几乎相同，在那些层的形状变化很大的网络(通常是卷积网络)中，可能不能很好地处理每一层相同的变化。
- **白化(归一化)你的输入数据**。训练时，减去数据集的均值，然后除以其标准差。你需要向各个方向拉伸和拉伸的幅度越少，你的网络学习就会越快、越容易。保持输入数据的均值以不变的方差为中心有助于解决这个问题。你还必须对每个测试输入执行相同的标准化，因此要确保你的训练集与真实数据相似。
- **以合理保留其动态范围的方式缩放输入数据**。这与归一化有关，但应该在归一化之前进行。例如，实际范围为[0,140000000]的数据“x”通常可以用`tanh(x)`或`tanh(x/C)`来处理，其中 `C`是某个常数，它拉伸曲线以适应 tanh 函数动态的、倾斜的部分中的更多输入范围。特别是在输入数据的一端或两端可能是无界的情况下，神经网络在(0,1)之间可以更好地学习。
- **不要费心降低学习速度(通常)**。学习率下降在 SGD 中更为常见，但 ADAM 自然地处理了这个问题。如果你绝对想要榨干每一盎司的表现：在训练结束后短时间内降低学习速度，你可能会看到一个突然的，非常小的误差下降，然后它会再次变平。
- **如果你的卷积层有 64 或 128 个滤波器，那可能就足够了**。特别是对于深度网络。实际上，128 已经很多了。如果你已经有了大量的滤波器，那么添加更多的滤波器可能不会改善性能。
- **池化用于转换不变性**。池化本质上是让网络学习图像“那部分”的“大意”。例如，最大池可以帮助卷积网络对图像中特征的平移、旋转和缩放变得健壮。

------

# 调试神经网络

如果你的网络没有学习(意思是：在训练过程中，损失没有收敛，或者你没有得到你期望的结果)，试试下面的建议：

- **过拟合** ！如果你的网络没有在学习，首先要做的就是在单个数据样本上让网络过拟合。这样的话，准确度应该是 100%或 99.99%，或者接近于 0 的误差。如果你的神经网络不能对单个数据点进行过拟合，那么可能是体系结构出现严重问题，但问题可能很微妙。如果你可以过拟合一个数据点，但是在更大的集合上的训练仍然不收敛，请尝试以下建议。
- **降低学习率**。你的网络学习速度会变慢，但它可能会进入一个以前无法进入的最小值，因为之前它的步长太大了。(直觉上，当你真正想进入沟底时，你的错误是最低的，想象一下跨过路边的水沟。)
- **提高学习率**。这将加快训练，帮助收紧反馈回路，这意味着你会更早知道你的网络是否在工作。虽然网络应该更快地收敛，但它的结果可能不会很好，而且“收敛”的过程实际上可能会跳来跳去。(使用 ADAM 的时候，我们发现~0.001 是一个非常好的值，在许多实验中都是这样。)
- **减小 minibatch 大小**。将 minibatch 大小减少到 1 可以提供与权重更新相关的更细粒度的反馈，你可以使用 TensorBoard(或其他调试/可视化工具)报告这些更新。
- **去掉 batch normalization**。随着批大小减少到 1，这样做可以梯度消失或梯度爆炸。几个星期以来，我们的网络都没有收敛，当我们删除了 batch normalization 之后，我们意识到在第二次迭代时输出都是 NaN。Batch norm 的作用是给需要止血带的东西贴上创可贴。它有它用的位置，但只有在你网络是没有 bug 的情况下才可以用。
- **增加 minibatch 大小**。更大的 minibatch — 如果可以的话，使用整个训练集 — 减少梯度更新中的方差，使每次迭代更精确。换句话说，让权重更新的方向是正确的。但是！它的有用性有一个有效的上限，物理内存的限制。通常，我们发现这不如前两个建议那么有用，这两个建议将 minibatch 大小减少到 1 并删除 batch normalization。
- **检查一下 reshaping**。剧烈的 reshaping(比如改变图像的 X、Y 维度)会破坏空间的局部性，使得网络更难学习，因为它也必须学习 reshaping。(自然景观变得支离破碎。自然特征在空间上是局部的，这就是为什么 conv 网如此有效的原因。如果使用多个图像/通道进行 reshape，要特别小心，使用`numpy.stack()`进行适当的对齐。
- **仔细检查你的损失函数**。如果使用复合函数，尝试将其简化为 L1 或 L2。我们发现 L1 对异常值的敏感度较低，当遇到有噪声的批处理或训练点时，L1 的调整幅度较小。
- **仔细检查你的可视化效果，如果适用的话**。你的可视化库(matplotlib, OpenCV 等)是调整数值的比例，还是剪切它们？还可以考虑使用一种感觉上一致的配色方案。



------

# 竞赛技巧

## 实验 pipeline(baseline)

1. 建议**baseline**模型从resnet18/34 or efficientnet-B0，小模型迭代快，实验进程也可以快速推进；
2. Adam优化器，SGD（可选，但是SGD没有Adam那么好调，所以baseline可以不选，后面细致调参的时候再转SGD也行。）；
3. loss function: 多分类（cross entropy），多标签分类（binary cross entropy），语义分割（binary cross entropy，如果是多类语义分割，可选cross entropy）；
4. metric: 这个就有点难度，一般根据比赛评测指标来选择；
5. 数据增强：可以为空，因为是baseline，需要数据增强后面是可以逐渐试错找到有效的再添加上去。

## 如何提升搭建baseline的能力

> 对于参赛者而言，往往具有一个好的baseline，或有着一套属于自己风格的pipeline，其实是已经成功了一半。好的baseline等价于一个好的起点，后面的改进遇到的阻碍相对也会少非常多。但是，要把baseline写好，其实是需要不少场比赛下来的经验或者是已经有过相关项目的工作经验。

1. 对于初学者，每场比赛，都会有许多kagglers将自己的baseline开源到kernel上，你所要做的是，不是直接copy，而是去学习，从多个kernel中取出其比较精妙的部分，再组合成自己的baseline；
2. 在以前相关类型的比赛里，模仿top solution codes，整理出一个baseline。
3. 多次实践下来，你基本掌握了自己动手写baseline的能力。

## 调参技巧

> 调参是比赛环节里最重要的一步（日常工作也都是一样离不开调参的），好的learning rate和learning rate schedule对比不合适的，得到的结果也是千差万别，optimizer的选取或许多比较fancy的finetune技巧也是类似的结果。

1. `Adam: init_lr=5e-4(3e-4)`（⭐⭐⭐⭐⭐），`3e-4`号称是Adam最好的初始学习率；SGD就更考验调参功力。

2. **lr schedule**

   - ReduceLROnPlateau，patience=4（5），gamma=0.1，这是我常用的一套组合，并不是最好的；
	- StepLR，个人比较喜欢用这个，自己设定好在哪个epoch进行学习率的衰减，个人比较喜欢用的衰减步骤是[5e-4(3e-4), 1e-4, 1e-5, 1e-6]，至于衰减位置，就需要自己有比较好的直觉，或者就是看log调参，对着2.1上训练的valid loss走势，valid loss不收敛了，咱就立刻进行衰减；
   - CosineAnnealingLR+Multi cycle,这个相较于前两个，就不需要太多的调参，可以训练多个cycle，模型可以找到更多的局部最优，一般推荐min_lr=1e-6，至于每个cycle多少epoch这个就说不准了，不同数据不太一样。
   
3. **finetune**，微调也是有许多比较fancy的技巧，在这里不做优劣比较，针对分类任务说明。

   - 微调方式一，最常用，只替换掉最后一层fc layer，改成本任务里训练集的类别数目，然后不做其余特殊处理，直接开始训练；
   
   - 微调方式二，在微调一的基础上，freeze backbone的参数，只更新（预训练）新的fc layer的参数（更新的参数量少，训练更快）到收敛为止，之后再放开所有层的参数，再一起训练；

   - 微调方式三，在微调方式二预训练fc layer之后或者直接就是微调方式一，可选择接上差分学习率（discriminative learning rates）即更新backbone参数和新fc layer的参数所使用的学习率是不一致的，一般可选择差异10倍，理由是backbone的参数是基于imagenet训练的，参数足够优秀同时泛化性也会更好，所以是希望得到微调即可，不需要太大的变化。
   
   - ```python
     optimizer = torch.optim.Adam([{'params': model.backbone.parameters(), 'lr': 3e-5},
      {'params': model.fc.parameters(), 'lr': 3e-4},   ])
     ```
   
   - 微调方式四，freeze浅层，训练深层（如可以不更新resnet前两个resnet block的参数，只更新其余的参数，一样是为了增强泛化，减少过拟合）。
   
4. **Find the best init_lr**，前面说到3e-4在Adam是较优的init_lr，那么如何寻找最好的init_lr？

   - 出自fastai, lr_find()，其原理就是选取loss function仍在明显降低的较大的学习速率，优劣性其实也是相对而言，不一定都是最好的。

5. **learing rate warmup**，理论解释可以参 zhihu.com/question/3380

6. 如果模型太大的同时你的GPU显存又不够大，那么设置的batch size就会太小，如何在有限的资源里提升多一点？

   - **梯度累计**（gradient accumulation），其实就是**积累多个batch的梯度之后，再进行梯度的回传做参数的更新，变相的增大了训练的batch size，但缺点是对 Batch Normalization没影响的。**
   - 如果你卡多，**这时可以使用多卡并行训练**，但要使用`syncbn（跨卡同步bn）`，即增大了`batch size`，又对`Batch Normalization`起到相同的作用。

## 图像分类技巧

### 1. label smoothing

分类任务的标签是`one-hot`形式，交叉熵会不断地去拟合这个真实概率，在**数据不够充足**的情况下拟合`one-hot`容易形成**过拟合**，因为`one-hot`会鼓励正确类别与所属类别之间的差异性尽可能大，但其实有不少类别之间是极为相似的。`label smoothing`的做法其实就是将`hard label`变成`soft label`。

### 2. topk-loss(OHEM)

`OHEM` 最初是在目标检测上提出来的，但其实思想是所有领域任务都通用的:

> 提取当前`batch`里`top k`大的`loss`的均值作为当前batch的loss，进行梯度的计算和回传。

其insight也很简单，就是一种 `hard mining` 的方法，一个batch里会有 easy sample 和 hard sample，easy sample对网络的更新作用较小（loss值小，梯度也会小），而hard sample的作用会更大（loss值大，梯度值也会大），**所以 topk-loss 就是提取hard sample**。

```python
loss = criterion(logits, truth)
loss,_ = loss.topk(k=..)     
loss = loss.mean()
```

### 3. weighted loss

weighted loss 其实也算是一种 `hard mining`的方法，只不过这种是**人为地认为哪种类别样本更加hard**，哪种类别样本更加easy。也就是说**人为对不同类别的loss进行进行一个权重的设置**，比如0,1类更难，设置权重为1.2，2类更容易，设置权重为0.8。。

```python
weights = [1.2, 1.2, 0.8]
class_weights = torch.FloatTensor(weights).to(device)
criterion = torch.nn.CrossEntropyLoss(weight=class_weights)
```

### 4. dual pooling

这种是在**模型层**进行改造的一种小trick了，常见的做法：`global max/average pooling + fc layer`，这里试`concat(global max-pooling, global average pooling) + fc layer`，其实就是为了**丰富特征层，**max pooling更加关注重要的局部特征，而average pooling试更加关注全局的特征。不一定有效，但不少人喜欢这样用。

```python
class res18(nn.Module):
    def __init__(self, num_classes):
        super(res18, self).__init__()
        self.base = resnet18(pretrained=True)
        self.feature = nn.Sequential(
            self.base.conv1,
            self.base.bn1,
            self.base.relu,
            self.base.maxpool,
            self.base.layer1,
            self.base.layer2,
            self.base.layer3,
            self.base.layer4
        )
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)
        self.reduce_layer = nn.Conv2d(1024, 512, 1)
        self.fc  = nn.Sequential(
            nn.Dropout(0.5),
            nn.Linear(512, num_classes)
            )
    def forward(self, x):
        bs = x.shape[0]
        x = self.feature(x)
        x1 = self.avg_pool(x).view(bs, -1)
        x2 = self.max_pool(x).view(bs, -1)
        x = torch.cat([avgpool_x, maxpool_x], dim=1)
        x = self.reduce_layer(x)
        logits = self.fc(x)
        return logits

```

### 5. margin-based softmax

- 在人脸识别领域，**基于margin的softmax loss其实就是对softmax loss的一系列魔改**（large margin softmax、NormFace、AM-softmax、CosFace、ArcFace等等），增加类间 margin，当然也有其它的特点，如weight norm和基于余弦角度的优化等等。其共同目标都是为了获得一个更加具有区分度的feature，不易过拟合。

- 一个比较多同学忽略的点是，如果使用了`margin-based softmax`，往往连同开源repo里默认的超参数也一起使用了，比如s=32.0，m=0.5，但其实这两个参数的设定都是有一定的缘由，比如s值象征着超球体的体积，**如果类别数较多，那么s应该设置大点。**如果你没有很好的直觉，那**grid search**一波，搜索到适合的s和m值也不会花很多时间。

### 6. Lovasz loss

- 这个loss本来是出于分割任务上的，其优化的是IOU，但你如果仔细观察lovasz传入的logit和truth，可以发现是和multi label classification类似，logit和truth都是由多个1值的one-hot形式。
- 所以在多标签分类任务上，其实是可以用lovasz loss来进行优化的，出自（Bestfitting）(kaggle.com/c/human-prot)

## 分类赛技巧（openset/检索）

### 1.  **BNNeck**
(出自罗浩博士的Bag of Tricks and A Strong Baseline for Deep Person Re-identification )，知乎链接一个更加强力的 ReID Baseline， **其实就是在feature层和fc layer之间增加一层Batch Normalization layer，然后在retrieval的时候，使用BN后的feature再做一个l2 norm，也就是retrieval with Cosine distance**。

```python
class res50(torch.nn.Module):
    def __init__(self, num_classes):
        super(res50, self).__init__()
        resnet = resnet50(pretrained=True)
        self.backbone = torch.nn.Sequential(
                        resnet.conv1,
                        resnet.bn1,
                        resnet.relu,
                        resnet.layer1,
                        resnet.layer2,
                        resnet.layer3,
                        resnet.layer4
        )
        self.pool = torch.nn.AdaptiveMaxPool2d(1)
        self.bnneck = nn.BatchNorm1d(2048)
        self.bnneck.bias.requires_grad_(False)  # no shift
        self.classifier = nn.Linear(2048, num_classes, bias=False)
    def forward(self, x):
        x = self.backbone(x)
        x = self.pool(x)
        feat = x.view(x.shape[0], -1)
        feat = self.bnneck(feat)
        if not self.training:
            return nn.functional.normalize(feat, dim=1, p=2)
        x = self.classifier(feat)
        return x

```

### 2. margin-based softmax

上面图像分类技巧介绍过了

### 3.triplet loss + softmax loss

结合metric learning，对feature进行多个loss的优化，triplet loss也是可以有很多的花样，Batch Hard Triplet Loss，是针对triplet loss的一种hard mining方法。

### 4. IBN

切换带有IBN block的backbone，搜图（open-set）往往test和train是不同场景下的数据，IBN block当初提出是为了提高针对不同场景下的模型泛化性能，提升跨域（cross domain）能力，在reid下的实验，IBN表现优异。

### 5. center loss

### 6. Gem

generalized mean pooling，出自 `Fine-tuning CNN Image Retrieval with No Human Annotation`，提出的是一种可学习的`pooling layer`，可提高检索性能。

```python
class GeM(nn.Module):
    def __init__(self, p=3, eps=1e-6):
        super(GeM,self).__init__()
        self.p = Parameter(torch.ones(1)*p)
        self.eps = eps
    def forward(self, x):
        return LF.gem(x, p=self.p, eps=self.eps)
    def __repr__(self):
        return self.__class__.__name__ + '(' + 'p=' + '{:.4f}'.format(self.p.data.tolist()[0]) + ', ' + 'eps=' + str(self.eps) + ')'
```

### 7.global feature + local features 

**将全局特征和多个局部特征一起融合，其实就是一种暴力融合特征的方法，对提升精度有一定的帮助，就是耗时相对只使用global feature来说很多点**，此种方法可参考在 `reid` 常用的 

- `PCB(Beyond Part Models: Person Retrieval with Refined Part Pooling)`
- `MGN(Learning Discriminative Features with Multiple Granularities for Person Re-Identification)`



### 8. re-ranking

是一种在首次获取检索图的候选图里做一次重新排序，获得更加精准的检索，相对比较耗时间，不适合现实场景，适合比赛刷精度。

------

## 通用技巧

> 大部分都是牺牲推断速度来提高精度的方法，适合比赛，不适合现实场景。

### 1. TTA（Test Time Augmentation）

 一种暴力测试的方法，将有效的增强方式得到多个不同的input，然后进行infer，得到多个结果进行融合，一般会比原始input会高不少。这种方法的缘由就是希望通过对input进行不同的变换方式获取多个不同的但重要的特征，然后可以得到多个具有差异性的预测结果。

### 2. 多尺度训练，融合 

在训练期间，随机输入多种尺度的图像进行训练，如（128\*128，196\*196，224\*224，256\*256，384\*384等等）然后测试的时候可适当的选取其中某几个尺度表现优异的预测结果出来融合，这种方法其实就是为了提升模型对尺度的变换的鲁棒性，不易受尺度变换的影响。

### 3. Ensemble

- **Snapshot Ensembles**，这个方法常在与cycle learning rate的情况下使用，在不同cycle下，模型会产出多个不同的snapshot weight（多个不同的局部最优，具有差异性），这时可以将这几个snapshot model一起进行推断，然后将预测结果进行平均融合。
- **SWA, Stochastic Weight Averaging，随机权重平均**，其实现原理当模型在训练时收敛到一定程度后，开始追踪每次epoch后得到的模型的平均值，有一个计算公式和当前的模型权重做一个平均得到一个最终的权重，提高泛化性能。
- **stacking**，在分类任务里，stacking是作为一种2nd level的ensemble方法，将多个“准而不同”的基分类器的预测集成与一身，再扔进去一个简单的分类器（mlp、logit regression、simple cnn，xgboost等）让其自己学习怎么将多个模型融合的收益做到最高。一般数据没有问题的话，stacking会更加稳定，不易过拟合，融合的收益也会更高。

### 4. 设计metric loss 

许多小伙伴会有这样一个疑惑，比赛的评测metric往往和自己训练时所使用的loss优化方向不是那么一致。比如多标签分类里的metric是fbeta_score，但训练时是用了bce loss，经常可以看到val loss再收敛后会有一个反弹增大的过程，但此时val fbeta_score是还在继续提升的。这时就可以针对metric来自行设计loss，比如fbeta loss就有。

### 5. semi-supervised learning

#### recurssive pseudo-label（伪标签）

伪标签现在已经是kaggle赛里一个必备工具了，但是这是个非常危险的操作，**如果没有筛选好的伪标签出来，容易造成模型过拟合伪标签里的许多噪声**。比较安全的方法是：

1. 筛选预测置信度高的样本作为伪标签，如分类里，在 test 里的预测概率是大于0.9的，则视为正确的预测，此时将其作为伪标签来使用。
2. 帮第一次的伪标签扔进去训练集一起训练后，得到新的模型，按相同的规则再次挑一次伪标签出来。
3. 如此不断循环多次，置信度的阈值可以适当作调整。

#### mean teacher
在这里给涛哥在Recursion Cellular Image Classification第三名的方案做个广告，end2end semi-supervised learining pipeline。

#### knowledge distillation（知识蒸馏)

此方法有助于提高小模型（student）的性能，将大模型（teacher）的预测作为soft label（用于学习teacher的模型信息）与truth（hard label）扔进去给小模型一起学习，当然两个不同label的loss权重需要调一调。当然，蒸馏的方法有很多种，这只是其中一种最简单的方法。蒸馏不一定用于训练小模型，大模型之间也是可以一同使用的。



------

## 数据增强与预处理

> 数据增强往往都是调出来的，可以先在本地里对图像施加不同的变换方式，用肉眼观察其是否有效（肉眼观察和模型学习到的不一定对等），之后再扔进去网络里训练验证其是否有效。

### h/v flip(水平垂直翻转)

95%的情况下都是有效的，因为不怎么破坏图像空间信息。

### random crop/center crop and resize

在原图进行crop之后再resize到指定的尺度。模型的感受野有限，有时会看不到图像中一些分布比较边缘或者是面积比较小的目标物体，crop过后其占比有更大，模型看到的机会也会更多。适用性也是比较大的。

### random cutout/erasing(随机擦除)

其实就是为了随机擦除图像中局部特征，模型根据有限的特征也可以判断出其属性，可提高模型的泛化性。

### AutoAugment

自己设定一些规则policy，让模型自己寻找合适的数据增强方式，需要消耗比较多的计算资源。

### mixup

一种与数据无关的数据增强方式，即特征之间的线性插值应导致相关标签之间的线性插值，扩大训练分布。意思是两个不同的label的样本进行不同比例的线性插值融合，那么其label也应该是相同比例关系进行线性融合。（上图）

### Class balance

主要就是针对**数据不平衡**的情况下进行的操作，一般是针对采样方法，或者在loss上做处理，如focal loss、weighted loss等。

### 图像预处理

许多看似有效的预处理操作，但是并不一定有效，如在医学领域的图像，许多肉眼观察良好的预处理的方式，实际上是破坏了原图真实类别关联的特征，这种方面需要相关领域知识。



## 网络搭建&训练技巧

参考：

- 深度学习 500 问--第十二章 网络搭建及训练



### 网络搭建

#### 新手原则

刚入门的新手不建议直接上来就开始搭建网络模型。比较建议的学习顺序如下：

- 1.了解神经网络工作原理，熟悉基本概念及术语。
- 2.阅读经典网络模型论文+实现源码(深度学习框架视自己情况而定)。
- 3.找数据集动手跑一个网络，可以尝试更改已有的网络模型结构。
- 4.根据自己的项目需要设计网络。

#### 深度优先原则

通常增加网络深度可以提高准确率，但同时会牺牲一些速度和内存。但深度不是盲目堆起来的，一定要在浅层网络有一定效果的基础上，增加深度。深度增加是为了增加模型的准确率，如果浅层都学不到东西，深了也没效果。



#### 卷积核size一般为奇数

卷积核为奇数有以下好处：

- 保证锚点刚好在中间，方便以 central pixel为标准进行滑动卷积，避免了位置信息发生偏移 。
- 保证在填充（Padding）时，在图像之间添加额外的零层，图像的两边仍然对称。



#### 卷积核不是越大越好

AlexNet中用到了一些非常大的卷积核，比如11×11、5×5卷积核，之前人们的观念是，卷积核越大，感受野越大，看到的图片信息越多，因此获得的特征越好。但是大的卷积核会导致计算量的暴增，不利于模型深度的增加，计算性能也会降低。于是在VGG、Inception网络中，利用2个3×3卷积核的组合比1个5×5卷积核的效果更佳，同时参数量（3×3×2+1=19<26=5×5×1+1）被降低，因此后来3×3卷积核被广泛应用在各种模型中。



### 网络训练

#### 合适的数据集

 - 1 没有明显脏数据(可以极大避免Loss输出为NaN)。
 - 2 样本数据分布均匀。



#### 合适的预处理方法

关于数据预处理，在Batch Normalization未出现之前预处理的主要做法是减去均值，然后除去方差。在Batch Normalization出现之后，减均值除方差的做法已经没有必要了。对应的预处理方法主要是数据筛查、数据增强等。



#### 网络的初始化

网络初始化最粗暴的做法是参数赋值为全0，这是绝对不可取的。因为如果所有的参数都是0，那么所有神经元的输出都将是相同的，那在back propagation的时候同一层内所有神经元的行为也是相同的，这可能会直接导致模型失效，无法收敛。吴恩达视频中介绍的方法是将网络权重初始化均值为0、方差为1符合的正态分布的随机数据。



#### 小规模数据试练

在正式开始训练之前，可以先用小规模数据进行试练。原因如下：

  - 1 可以验证自己的训练流程对否。
  - 2 可以观察收敛速度，帮助调整学习速率。
  - 3 查看GPU显存占用情况，最大化batch_size(前提是进行了batch normalization，只要显卡不爆，尽量挑大的)。



#### 设置合理Learning Rate

参考：

- [炼丹手册——学习率设置](https://zhuanlan.zhihu.com/p/332766013)

对于学习率，一般有这几种情况：

1. 太大。Loss爆炸、输出NaN等。
2. 太小。收敛速度过慢，训练时长大大延长。
3. 可变的学习速率。比如当输出准确率到达某个阈值后，可以让Learning Rate减半继续训练。



学习率的设置，一般分为两种，人工调整或策略调整。

**人工调整学习率**一般是根据我们的经验值进行尝试，首先在整个训练过程中学习率肯定不会设为一个固定的值，原因如上图描述的设置大了得不到局部最优值，设置小了收敛太慢也容易过拟合。通常我们会尝试性的将初始学习率设为：0.1，0.01，0.001，0.0001等来观察网络初始阶段epoch的loss情况：

- 如果训练初期loss出现梯度爆炸或NaN这样的情况（暂时排除其他原因引起的loss异常），说明初始学习率偏大，可以将初始学习率降低10倍再次尝试；
- 如果训练初期loss下降缓慢，说明初始学习率偏小，可以将初始学习率增加5倍或10倍再次尝试；
- 如果训练一段时间后loss下降缓慢或者出现震荡现象，可能训练进入到一个局部最小值或者鞍点附近。如果在局部最小值附近，需要降低学习率使训练朝更精细的位置移动；如果处于鞍点附件，需要适当增加学习率使步长更大跳出鞍点。
- 如果网络权重采用随机初始化方式从头学习，有时会因为任务复杂，初始学习率需要设置的比较小，否则很容易梯度飞掉带来模型的不稳定(振荡)。**这种思想也叫做Warmup**，在预热的小学习率下，模型可以慢慢趋于稳定，等模型相对稳定后再选择预先设置的学习率进行训练,使得模型收敛速度变得更快，模型效果更佳。

- 如果网络基于预训练权重做的finetune，由于模型在原数据集上以及收敛，有一个较好的七点，可以将初始学习率设置的小一些进行微调，比如0.0001。



**策略调整学习率**包括固定策略的学习率衰减和自适应学习率衰减，由于学习率如果连续衰减，不同的训练数据就会有不同的学习率。当学习率衰减时，在相似的训练数据下参数更新的速度也会放慢，就相当于减小了训练数据对模型训练结果的影响。为了使训练数据集中的所有数据对模型训练有相等的作用，通常是以epoch为单位衰减学习率。



**固定学习率衰减包括：**

分段减缓：每N轮学习率减半或者在训练过程中不同阶段设置不同的学习率，便于更精细的调参。TF的接口函数为：

```python
global_step_op = tf.train.get_or_create_global_step()
base_learning_rate = 0.01
decay_boundaries = [2000, 4000] # 学习率衰减边界；
learning_rate_value = [base_learning_rate, base_learning_rate/10., base_learning_rate/100.] # 不同阶段对应学习率。
learning_rate = tf.train.piecewise_constant_decay(global_step_op, boundaries=decay_boundaries,values=learning_rate_value)
```


分数减缓：将学习率随着epoch的次数进行衰减，$\alpha=\frac{1}{(1+decayRate*epoch)}* lr$ ，其中  $\alpha$ 表示学习率，decayRate 表示衰减率(可尝试设为0.1，根据数据集/迭代次数调整)，  epoch 表示迭代次数， lr  表示初始学习率。

指数减缓：与分数减缓类似，只是采用指数形式做了表达， $\alpha=\gamma^{epoch}*lr$ ，其中 $\gamma$ 表示指数的底（通常会设置为接近于1的数值，如0.95），随着训练批次epoch的增加，学习率呈指数下降。TF的接口函数为：

```python
global_step_op = tf.train.get_or_create_global_step()
base_learning_rate = 0.01
decay_rate = 0.98
decay_steps = 2000
learning_rate_no_stair = tf.train.exponential_decay(learning_rate=base_learning_rate,
                                                        decay_rate=decay_rate,
                                                        decay_steps=decay_steps,
                                                        staircase=True,
                                                        global_step=global_step_op,
                                                        name="exponential_decay_use_stair")
```

余弦周期减缓：**余弦周期减缓也叫余弦退火学习率**，不同于传统的学习率，随着epoch的增加，学习率先急速下降，再陡然提升，然后不断重复这个过程。其目的在于跳出局部最优点。

![img](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/cosine_learning_rate.png)

之前介绍的几种学习率调节方式在神经网络训练过程中学习率会逐渐减小，所以模型逐渐找到局部最优点。这个过程中，因为一开始的学习率较大，模型不会踏入陡峭的局部最优点，而是快速往平坦的局部最优点移动。随着学习率逐渐减小，模型最终收敛到一个比较好的最优点。如下图所示：

![img](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/learning_rate_schedule.png)

而余弦退火学习率由于急速下降，所以模型会迅速踏入局部最优点（不管是否陡峭），并保存局部最优点的模型。⌈快照集成⌋中⌈快照⌋的指的就是这个意思。保存模型后，学习率重新恢复到一个较大值，逃离当前的局部最优点，并寻找新的最优点。因为不同局部最优点的模型则存到较大的多样性，所以集合之后效果会更好。如下图所示：

![img](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/cosine_learning_rate_2.png)

两种方式比较起来，可以理解为模型训练的“起点”和“终点”是差不多的。不同的是，余弦退火学习率使得模型的训练过程比较“曲折”。TF的接口函数为：

```python
# total_decay_step = 15000          总的学习率衰减步数
# base_learning_rate = 0.01         基学习率
# warmup_learning_rate = 0.0001     warm-up 学习率
# warmup_steps = 2000               warm-up 迭代次数
# hold_base_rate_steps_2000 = 2000  保持基学习率的步数
# hold_base_rate_steps_0 = 0
# alpha = 0.00001                   最小学习率
global_step_op = tf.train.get_or_create_global_step()
learning_rate = cosine_decay_with_warmup(learning_rate_base=base_learning_rate,
                                             total_decay_steps=total_decay_step,
                                             alpha = alpha,
                                             warmup_learning_rate=warmup_learning_rate,
                                             warmup_steps=warmup_steps,
                                             hold_base_rate_steps=hold_base_rate_steps_2000,
                                             global_step=global_step_op)
def cosine_decay_with_warmup(global_step,
                             learning_rate_base,
                             total_decay_steps,
                             alpha = 0.0,
                             warmup_learning_rate=0.0,
                             warmup_steps=0,
                             hold_base_rate_steps=0):
  """Cosine decay schedule with warm up period.
  In this schedule, the learning rate grows linearly from warmup_learning_rate
  to learning_rate_base for warmup_steps, then transitions to a cosine decay
  schedule."""
  def eager_decay_rate():
    """Callable to compute the learning rate."""
    learning_rate = tf.train.cosine_decay(learning_rate=learning_rate_base,
                                          decay_steps=total_decay_steps - warmup_steps - hold_base_rate_steps,
                                          global_step= global_step - warmup_steps - hold_base_rate_steps,
                                          alpha=alpha)
    if hold_base_rate_steps > 0:
      learning_rate = tf.where(
          global_step > warmup_steps + hold_base_rate_steps,
          learning_rate, learning_rate_base)
    if warmup_steps > 0:
      if learning_rate_base < warmup_learning_rate:
        raise ValueError('learning_rate_base must be larger or equal to '
                         'warmup_learning_rate.')
      slope = (learning_rate_base - warmup_learning_rate) / warmup_steps
      warmup_rate = slope * tf.cast(global_step,
                                    tf.float32) + warmup_learning_rate
      learning_rate = tf.where(global_step < warmup_steps, warmup_rate,
                               learning_rate)
    return tf.where(global_step > total_decay_steps, alpha, learning_rate,
                    name='learning_rate')

  if tf.executing_eagerly():
    return eager_decay_rate
  else:
    return eager_decay_rate()
```

**自适应学习率衰减包括：**

AdaGrad、 RMSprop、 AdaDelta等。此处更偏向于优化算法，暂时不在该篇介绍。



#### 损失函数

损失函数主要分为两大类:分类损失和回归损失

>1.回归损失：
>
>> - 1 均方误差(MSE 二次损失 L2损失)
>>   它是我们的目标变量与预测值变量差值平方。
>> - 2 平均绝对误差(MAE L1损失)
>>   它是我们的目标变量与预测值变量差值绝对值。
>>   关于MSE与MAE的比较。MSE更容易解决问题，但是MAE对于异常值更加鲁棒。更多关于MAE和MSE的性能，可以参考[L1vs.L2 Loss Function](https://rishy.github.io/ml/2015/07/28/l1-vs-l2-loss/)


>2.分类损失：
>
>> - 1 交叉熵损失函数。
>>   是目前神经网络中最常用的分类目标损失函数。
>> - 2 合页损失函数
>>   合页损失函数广泛在支持向量机中使用，有时也会在损失函数中使用。缺点:合页损失函数是对错误越大的样本施以更严重的惩罚，但是这样会导致损失函数对噪声敏感。









------

# 参考

1. [构建神经网络的一些实战经验和建议](https://mp.weixin.qq.com/s/hBghRDNuexWCutuDSUa4WQ)
2. [本科生晋升GM记录 & Kaggle比赛进阶技巧分享](https://mp.weixin.qq.com/s/llpex8u9PQNbzxeVXc62hA)