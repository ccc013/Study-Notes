# 降维

参考：

- 周志华--《机器学习》

- 葫芦娃--《百面机器学习》

- [机器学习--主成分分析(PCA)算法的原理及优缺点](https://www.cnblogs.com/lsm-boke/p/11760224.html)

- [线性判别分析LDA原理总结](https://www.cnblogs.com/pinard/p/6244265.html) https://www.cnblogs.com/pinard/p/6244265.html

  



**降维就是用低维度的向量来表示原始高维度的特征。**

例如：三维空间中分布在同一个平面上的一些点，用x,y,z三个轴来表示，就需要三个维度；而实际上，因为这些点是分布在一个平面上的，所以可以通过坐标系的旋转变换使得只需要x,y两个轴来表示这些点的数据关系，而且不会有任何损失，从而达到数据降维的目的。

**降维的作用**：	

1. 增大样本密度，可以缓解维数灾难
2. 减小计算开销
3. 去噪



------

## 1. PCA

### 简介

主成分分析（Components Analysis，PCA）是机器学习中最经典的降维方法，**旨在找到数据中的主成分，并利用这些主成分来表征原始数据**。简单地说，就是将n维的特征映射到k维上（k<n），这k维的正交特征，就是主成分。

PCA是一种线性的、无监督的、全局的降维算法。

PCA的应用也很广泛，这里列举其中几项：

1. 数据降维 
2. 去噪
3. 高位数据集的可视化
4. 数据压缩
5. 图像分析



对于 PCA，周志华的《机器学习》书上的话来理解就是：对于正交特征空间中的样本点，如何用一个超平面（直线的高维推广）来对所有的样本进行恰当的表达？如果存在这样的超平面（即由k维特征重构出的主成分），那么它应该具有这样的性质：

**①最大可分性：样本点在这个超平面上的投影尽可能的分开（最大化方差）**

**②最近重构性：样本点到这个超平面的距离都足够近（最小化平方误差）**

如何理解最大可分性（最大方差）和最近重构性（最小平方误差）这两种性质呢？以及怎样才能找到这个k维的主成分呢？下面分别展开分析。



### PCA之最大可分性（最大方差）

在信号处理中认为，**信号具有较大的方差，噪声具有较小的方差，两者的比值称之为信噪比**。

**信噪比越大意味着数据的质量越好**，因此，我们很容易想到**PCA的优化目标，就是最大化投影方差。换种说法就是，让数据在某个超平面（主轴）上投影的方差最大**。

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/PCA_fig1.png)

理解了最大方差的含义和PCA的优化目标，接下来将是具体的公式推导。

#### 最大化方差公式推导

1. 给定一组样本点 $\{v_{1},v_{2},...,v_{n}\}$，首先将其中心化后表示为 $\{x_{1},x_{2},...,x_{n}\}=\{v_{1}-\mu,v_{2}-\mu,...,v_{n}-\mu\}$，其中，$\mu=\frac{1}{n}\sum_{i=1}^{n}v_{i}$。

2. 因为一个向量 $x_{i}$ 在 $\omega$（单位方向向量）上的投影可以表示为两者的内积$<x_{i},\omega>=x_{i}^{T}\omega$，而**PCA的目标就是找到一个投影方向$\omega$，使得所有的数据$\{x_{1},x_{2},...,x_{n}\}$在$\omega$上的投影方差尽可能地大**，因此：

3. 投影后的方差可以表示为：

$$
D(x)=\frac{1}{n}\sum_{i=1}^{n}(x_{i}^{T}\omega)^2
\ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \  
\\=\frac{1}{n}\sum_{i=1}^{n}(x_{i}^{T}\omega)^{T}(x_{i}^{T}\omega)
\\=\frac{1}{n}\sum_{i=1}^{n}\omega^{T}x_{i}x_{i}^{T}\omega
\ \ \ \ \ \ \ \\  \ 
=\omega^{T}\left(\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}^{T}\right)\omega
$$
4. 然后可以发现，上面大括号内的$\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}^{T}$就是**原始样本的协方差矩阵**，令其等于$\Sigma$。

> 补充理解：协方差公式形式：
> $$
> Cov(x,y)=\frac{1}{n-1}\sum_{i=1}^{n}(x_{i}-\mu_x)(y_{i}-\mu_{y})
> $$
> 在均值 $\mu=0$ 时，（当n足够大时，n-1可以约等于n），于是有：
> $$
> Cov(x,y)=\frac{1}{n}\sum_{i=1}^{n}x_{i}y_{i}
> $$
> 对于步骤③中 $x_{i}$，是原始样本 $v_{i}$ 中心化后的，因此可以说 $\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}^{T}$ 就是**原始样本的协方差矩阵**。

5. 因此，上面的最大化方差D(x)的优化问题可以转化为

$$
\begin{cases}max\{\omega^{T}\Sigma\omega\},\\s.t. \ \ \omega^{T}\omega=1.\end{cases}\\
其中，\omega是单位向量，因此有\ \omega^{T}\omega=1.
$$

6. 对于上面的优化目标，可以构造拉格朗日函数来解决：

$$
L(\omega)=\omega^{T}\Sigma\omega+\lambda(1-\omega^{T}\omega)\\
对\ \omega\ 求导并令其等于0，可得\ \Sigma\omega=\lambda\omega
$$

> 补充一点矩阵微分的知识，有助于理解上式的求导过程：（非常有用的公式！！！可以记住！）
> $$
> ①\ \frac{\partial x^{T}a}{\partial x}=\frac{\partial a^{T}x}{\partial x}=a\ \ \ \ \\②\ \frac{\partial x^{T}Ax}{\partial x}=(A+A^{T})x
> $$
> 因此，对拉格朗日函数的求导便很容易理解了：
> $$
> \frac{\partial L(\omega)}{\partial\omega}=(\Sigma+\Sigma^{T})\omega-\lambda(I+I^{T})\omega\\
> 其中 I为单位矩阵
> $$
> 还记得$\Sigma$是什么吗？由上面定义可知，$\Sigma=\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}^{T}$，很显然，$\Sigma$的转置$\Sigma^{T}$=$\Sigma$，因此上面可化简为：
> $$
> \frac{\partial L(\omega)}{\partial\omega}=2\Sigma\omega-2\lambda\omega\\
> 令其等于0，便可得\ \Sigma\omega=\lambda\omega
> $$

7. 由此，最终可以得到最大方差：

$$
D(x)=\omega^{T}\Sigma\omega=\lambda\omega^{T}\omega=\lambda
$$
至此，公式推导已经完成，现在我们不难看出，**x 投影后的方差就是协方差矩阵的特征值**，理解了这一点一切就很清晰了。**因此，我们要找到最大的方差，也就是相当于要求协方差矩阵的最大特征值，而最佳投影方向就是最大特征值所对应的特征向量。**



#### PCA求解过程总结

1. 对原始样本进行中心化处理，即零均值化
2. 求出样本的协方差矩阵 $\Sigma=\frac{1}{n}\sum_{i=1}^{n}x_{i}x_{i}^{T}$
3. 求解协方差矩阵的特征值和特征向量
4. 将特征值由大到小排列，取出前 k 个特征值对应的特征向量
5. 将 n 维样本映射到 k 维，实现降维处理。
$$
x_{i}^{'}=\begin{bmatrix}\omega_{1}^{T}x_{i}\\\omega_{2}^{T}x_{i}\\\vdots \\\omega_{k}^{T}x_{i} \end{bmatrix}\\
新的x_{i}^{'}的第k维就是x_{i}在第k个主成分\omega_{k}方向上的投影.
$$



### PCA之最近重构性（最小平方误差）

如何理解最近重构性或最小平方误差呢？我们先回顾一下前面讲的最大化方差方法：对于二维空间中的样本点，最大化方差的思想是找到一条直线，使得样本点投影到该直线上的方差最大。因此，也很容易想到，**我们可以找到一条直线来更好的拟合这些样本点。从这个角度来理解，求解PCA的问题就可以转化为一个回归问题了。**

上面说的是二维空间，可以用直线来拟合，那对于高维空间呢？当然也是可以的。超平面是直线在高维空间的推广，因此，最大化方差就是寻找一个超平面使得样本点在超平面上的投影方差最大，而**最小平方误差就是寻找一个超平面使得样本点到这个超平面的距离平方和最小，也就是最近重构性**。

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/PCA_fig2.png)

下面给出最小化平方误差的优化目标，具体推导就不展开啦~（一般熟悉最大化方差的推导面试就够用了，最小化平方误差的推导作为了解，如果有需要可以参考《百面机器学习》这本书）

#### 最小化平方误差优化目标

1. 假设超平面D由k个标准正交基$W=\{\omega_{1},\omega_{2},...,\omega_{k}\}$构成，$\widehat{x_{d}}$是样本点$x_{d}$（中心化后）在超平面D上的投影向量，则每个样本点到 k 维超平面D的距离为： 

$$
dist(x_{i},D)=||x_{d}-\widehat{x_{d}}||_{2}\\其中，投影向量\ \widehat{x_{d}}可以通过k维正交基线性表示为\ \widehat{x_{d}}=\sum_{i=1}^{k}(\omega_{i}^{T}x_{d})\omega_{i}，而\ \omega_{i}^{T}x_{d}\ 是x_{d}在\omega_{i}方向上的投影长度
$$
2. 则PCA的优化目标为：

$$
\begin{cases}\underset{\omega_{1},...,\omega_{k}}{arg\ min}\sum_{d=1}^{n}||x_{d}-\widehat{x_{d}}||_{2}^{2},\\s.t.\ \ \underset{\forall i,j}{\omega_{i}^{T}\omega_{j}}=\delta_{i,j}=\begin{cases}1,\ i=j;\\0,\ i\neq j. \end{cases} \end{cases}\\
\ \\ \  \\ 通过进一步化简，可得优化目标为：\ \ \begin{cases}\underset{W}{arg\ max}\ tr(W^{T}XX^{T}W),\\s.t.\ \ W^{T}W=I. \end{cases}
$$

因此，可以发现，最小化方差方法求解问题的形式，和最大化方差方法是一致的，因此也同样可以通过求解协方差的特征值所对应的特征向量，从而得到降维后的主成分。

重要的事情不妨多说一遍，因此这里再把求解过程写一遍：



### PCA的优缺点

**优点：** 	
1. 它是无监督学习算法，完全无参数限制。
2. 降维，减小计算开销
3. 可以去除噪声
4. 使得数据集更容易使用，让结果更加容易理解

**缺点：**	 
1. 特征值分解有一些局限性，比如变换的矩阵必须是方阵
2. 如果用户对观测对象有一定的先验知识，掌握了数据的一些特征，却**无法通过参数化等方法对处理过程进行干预**，可能会得不到预期的效果，效率也不高
3.  在非高斯分布情况下，PCA方法得出的主元可能并不是最优的。





------

## 2. LDA

线性判别分析（Linear Discriminant Analysis，LDA）是机器学习中常用的降维方法之一，是一种线性的、有监督的降维方法，即每个样本都有对应的类别标签。

它的主要思想是给定训练样本集，设法将样本投影到一条直线上，使得同类的样本的投影尽可能的接近、异类样本的投影尽可能地远离（即**最小化类内距离和最大化类间距离**）。



![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/LDA_fig1.png)

![](https://gitee.com/lcai013/image_cdn/raw/master/notes_images/LDA_fig2.png)

● 为什么要将最大化类间距离和最小化类内距离同时作为优化目标呢？

先看上面第二张图的左图（a），对于两个类别，只采用了最大化类间距离，其结果中两类样本会有少许重叠；而对于右图（b），同时最大化类间距离和最小化类内距离，可见分类效果更好，同类样本的投影分布更加集中了。当然，对于二维的数据，可以采用将样本投影到直线上的方式，对于高维的数据，则是投影到一个低维的超平面上，这应该很好理解。



### LDA算法优化目标

由上面的介绍我们知道，LDA算法的思想就是最大化类间距离和最小化类内距离，其优化目标就很直观了，那怎么用数学方式来表示呢？要解决这个问题，就得先看看怎么描述类间距离和类内距离。

**● 类间距离**（以二分类为示例）

假设有$C_{1}$、$C_{2}$两类样本，其均值分别为 $\mu_{1}=\frac{1}{N}\sum_{x\in C_{1}}x$ 和  $\mu_{2}=\frac{1}{N}\sum_{x\in C_{2}}x$ 。很显然，要使得两类样本类间距离最大，则 $\mu_{1}$ 、$\mu_{2}$ 的距离应尽可能地大，则类间距离可描述为
$$
||\omega^{T}\mu_{0}-\omega^{T}\mu_{1}||_{2}^{2},\ \ 其中，\omega为投影方向
$$
**● 类内距离** 

要使得样本在同类中距离最小，也就是最小化同类样本的方差，假设分别用 $D_{1}$、 $D_{2}$ 表示两类样本的投影方差，则有：
$$
D_{1} = \sum_{x\in C_{1}}(\omega^{T}x-\omega^{T}\mu_{1})^{2}=\sum_{x\in C_{1}}\omega^{T}(x-\mu_{1})(x-\mu_{1})^{T}\omega \\
D_{2} = \sum_{x\in C_{2}}(\omega^{T}x-\omega^{T}\mu_{2})^{2}=\sum_{x\in C_{2}}\omega^{T}(x-\mu_{2})(x-\mu_{2})^{T}\omega
$$
因此，要使得类内距离最小，就是要最小化 $D_{1}+D_{2}$。

**● 优化目标** 

由上面分析，最大化类间距离和最小化类内距离，因此可以得到最大化目标：
$$
J(\omega) = \frac{||\omega^{T}\mu_{0}-\omega^{T}\mu_{1}||_{2}^{2}}{D_{1}+D_{2}}\\\qquad\qquad\qquad\quad
=\frac{||\omega^{T}\mu_{0}-\omega^{T}\mu_{1}||_{2}^{2}}{\sum_{x\in C_{i}}\omega^{T}(x-\mu_{i})(x-\mu_{i})^{T}\omega}
$$
为了化简上面公式，给出几个定义：

**● 类间散度矩阵：** 
$$
S_{b}=(\mu_{1}-\mu_{2})(\mu_{1}-\mu_{2})^{T}
$$
**● 类内散度矩阵：** 
$$
S_{\omega}=\Sigma_{1}+\Sigma_{2}=\sum_{x\in C_{1}}(x-\mu_{1})(x-\mu_{1})^{T}+\sum_{x\in C_{2}}(x-\mu_{2})(x-\mu_{2})^{T}
$$
因此最大化目标可以简写为：
$$
J(\omega) = \frac{\omega^{T}S_{b}\omega}{\omega^{T}S_{\omega}\omega}
$$

> 这是一个广义瑞利商，可以对矩阵进行标准化操作（具体证明就不展开啦），因此，通过标准化后总可以得到 $\omega^{T}S_{\omega}\omega=1$，又由于上面优化目标函数分子分母都是二次项，其解与 $\omega$ 的长度无关，只与方向有关，因此上面优化目标等价于以下最小化目标：

转化为最小化目标：
$$
\underset{\omega}{min}\quad-\omega^{T}S_{b}\omega \\
s.t. \quad \omega^{T}S_{\omega}\omega=1
$$
由拉格朗日法，上式可得：
$$
S_{b}\omega=\lambda S_{\omega}\omega \\
即有，S_{\omega}^{-1}S_{b}\omega=\lambda \omega
$$
至此，我们的**优化目标就转化成了求矩阵 $S_{\omega}^{-1}S_{b}$ 的特征值，而投影方向就是这个特征值对应的特征向量**。

> 由于 $(\mu_{1}-\mu_{2})^{T}\omega$ 是个标量（因为 $\mu_{1}-\mu_{2}$ 和  $\omega$ 同向时才能保证类间距离最大），
>
> 所以，对于 $S_{b}\omega=(\mu_{1}-\mu_{2})(\mu_{1}-\mu_{2})^{T}\omega$ 而言，可以看出 $S_{b}\omega$ 始终与 $(\mu_{1}-\mu_{2})$ 的方向一致

因此，如果只考虑 $\omega$ 的长度而不考虑方向，则由：
$$
S_{\omega}^{-1}S_{b}\omega=\lambda \omega \qquad => \qquad  \omega=S_{\omega}^{-1}(\mu_{1}-\mu_{2})
$$
**也就是说，我们只需求出样本的均值和类内的散度矩阵（即类内方差），即可求出投影方向。**



### LDA算法流程(推广至高维)

1.计算每类样本的均值向量 $\mu_{i}$。

2.计算类间散度矩阵  $S_{\omega}$ 和类内散度矩阵 $S_{b}$ 。

3.求矩阵 $S_{\omega}^{-1}S_{b}$ 的特征值即对应的特征向量，从大到小排序。

4.将特征值由大到小排列，取出前 k 个特征值对应的特征向量。

5.将 n 维样本映射到 k 维，实现降维处理。
$$
x_{i}^{'}=\begin{bmatrix}\omega_{1}^{T}x_{i}\\\omega_{2}^{T}x_{i}\\\vdots \\\omega_{k}^{T}x_{i} \end{bmatrix}\\
$$

### 优缺点

**优点**

1. 善于对有类别信息的数据进行降维处理
2. 是线性模型，对噪声的鲁棒性较好
3. 可以实现降维
4. 在样本分类信息依赖均值而不是方差的时候，LDA算法优于PCA算法



**缺点**

1. 由于模型简单，对数据特征的表达能力不足
2. 对数据的分布做了一些很强的假设，比如每个类别都是高斯分布、各个类别的协方差相等，实际中这些假设很难完全满足
3. LDA不适合对非高斯分布样本进行降维，PCA也有这个问题。
4. LDA在样本分类信息依赖方差而不是均值的时候，降维效果不好。
5. LDA降维最多降到类别数k-1的维数，如果我们降维的维度大于k-1，则不能使用LDA。
6. LDA可能过度拟合数据。



### PCA与LDA的比较

#### 相同点

1. 均是降维方法
2. 降维时均使用了矩阵特征分解的思想
3.  两者都假设数据符合高斯分布




#### 不同点

1. PCA是无监督的降维方法，而LDA是有监督的降维方法
2. LDA除了可以降维，还可以用于分类
3. LDA降维最多降到类别数 `k-1`的维数（k是样本类别的个数），而PCA没有这个限制。
4. LDA选择的是分类性能最好的投影方向，而PCA选择样本点投影具有最大方差的方向

> ● 关于第4点，可以这样理解：
>
> LDA在降维过程中最小化类内距离，即同类样本的方差尽可能小，同时最大化类间距离，即异类样本尽可能分离，这本身是也为分类任务服务的；
>
> 而PCA是无监督的降维方法，其假设方差越大，信息量越多，因此会选择样本点投影具有最大方差的方向。







------



