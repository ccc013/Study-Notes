# 神经网络学习笔记1--Multi Layer Perceptron

参考文章：

1. [A Quick Introduction to Neural Networks](https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/)
2. [this Quora answer by Hemanth Kumar](https://www.quora.com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/Hemanth-Kumar-Mantri)

这是简单介绍神经网络的知识，并介绍一种特别的神经网络--**多层感知器(Multi Layer Perceptron,MLP)**，参考自 [A Quick Introduction to Neural Networks](https://ujjwalkarn.me/2016/08/09/quick-intro-neural-networks/) 。

#### 1. 神经元

**神经元(neuron)**是神经网络的基本计算单元，也被称作**节点(node)**或者**单元(unit)**。它可以接受来自其他神经元的输入或者是外部的数据，然后计算一个输出。每个输入值都有一个**权重(weight)**,权重的大小取决于这个输入相比于其他输入值的重要性。然后在神经元上执行一个特定的函数**f**,定义如下图所示，这个函数会该神经元的所有输入值以及其权重进行一个操作。

![](http://7xrluf.com1.z0.glb.clouddn.com/ANN1.png)

由上图可以看到，除了权重外，还有一个输入值是1的偏置值**bias**。这里的函数**f**就是一个被称为**激活函数**的非线性函数。它的目的是给神经元的输出引入非线性。因为在现实世界中的数据都是非线性的，因此我们希望神经元都可以学习到这些非线性的表示。

下面是一些比较常见的激活函数：

* **Sigmoid**:  $\sigma(x) =\frac{1}{1+e^{-x}}$，输出范围是[0,1]
* **tanh:**  $tanh(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}=2\sigma(2x)-1$,输出范围是[-1,1]
* **ReLU:**  $f(x) = max(0,x)$

下面给出上述激活函数的图像：

![](http://7xrluf.com1.z0.glb.clouddn.com/ANN2.png)

这里需要简单说下偏置值**bias**的作用，它可以提供给每个神经元一个可训练的常量值。具体可以看下**Stackoverflow**上的这个[回答](http://stackoverflow.com/questions/2480650/role-of-bias-in-neural-networks)。

#### 2. 前向神经网络

前向神经网络是第一个也是最简单的一种人工神经网络了。下面给出一个前向神经网络的简单例子示意图。

![](http://7xrluf.com1.z0.glb.clouddn.com/ANN3.png)

如上图所示，这个神经网络分为3个网络层，分别是输入层，隐藏层和输出层，每个网络层都包含有多个神经元，每个神经元都会跟相邻的前一个层的神经元有连接，这些连接其实也是该神经元的输入。根据神经元所在层的不同，前向神经网络的神经元也分为三种，分别为：

1. 输入神经元：位于输入层，主要是传递来自外界的信息进入神经网络中，比如图片信息，文本信息等，这些神经元不需要执行任何计算，只是作为传递信息，或者说是数据进入隐藏层。
2. 隐藏神经元：位于隐藏层，隐藏层的神经元不与外界有直接的连接，它都是通过前面的输入层和后面的输出层与外界有间接的联系，因此称之为隐藏层，上图只是有1个网络层，但实际上隐藏层的数量是可以有很多的，远多于1个，当然也可以没有，那就是只有输入层和输出层的情况了。隐藏层的神经元会执行计算，将输入层的输入信息通过计算进行转换，然后输出到输出层。
3. 输出神经元：位于输出层，输出神经元就是将来自隐藏层的信息输出到外界中，也就是输出最终的结果，如分类结果等。

前向网络中，信息是从输入层传递到输出层，只有前向这一个方向，没有反向传播，也不会循环（不同于**RNN**，它的神经元间的连接形成了一个循环）。

下面是两个前向神经网络的例子：

1. 单层感知器——最简单的前向神经网络，并且不含任何隐藏层。
2. 多层感知器——拥有1个或多个隐藏层。

#### 3. 多层感知器(MLP)

单层感知器只有输入层和输出层，所以只能学习线性函数，而多层感知器拥有1个或多个隐藏层，因此也就可以学习非线性函数了。下面是拥有一个隐藏层的**MLP**的例子示意图：

![](http://7xrluf.com1.z0.glb.clouddn.com/ANN4.png)

上图显示该**MLP**有两个输入值**X1**和**X2**，隐藏层有两个神经元，然后有两个输出值**Y1**和**Y2**。图中还展示了其中一个隐藏层神经元的计算——$f(summation) = f(w_0 * 1  w_1 * X1 + w_2 * X2)$。

#### 4. 反向传播算法

**MLP**要能够获得更好的结果，单单通过前向传播计算是不够的，神经网络除了前向传播计算，还有反向传播算法，通过反向传播来更新权值，从而获得更好的训练结果。下面给出来自[this Quora answer by Hemanth Kumar](https://www.quora.com/How-do-you-explain-back-propagation-algorithm-to-a-beginner-in-neural-network/answer/Hemanth-Kumar-Mantri)]对反向传播的解释：

> **反向传播误差**，也被简称为反向传播，是用于训练神经网络的方法之一，它是一个有监督学习方法，也就是说它是从训练数据的标签来进行学习，即相当于有一个监督者负责指导它的学习。
>
> 简单来说，反向传播是**“从错误中进行学习”**，监督者会在神经网络出错的时候修正它。
>
> 学习的目的主要是为了给隐藏层的每个节点的连接的权重分配一个正确的数值。当给定一个输入向量，这些权重可以决定输出的向量。
>
> 在监督学习中，训练集是有标签的，这意味着，对于给定的一些输入，我们是可以知道一些期望的输出值，也就是标签是什么的。

> **反向传播算法：**
>
> 在初始阶段，所有权重都是随机分配的。对于训练集的每个输入值，经过神经网络的前向计算后，得到的输出值将会与期望的输出进行比较，然后得到的误差会传回给前面的网络层。这个误差会被记下，然后权重会进行相应的调整。这个过程会不断重复，知道输出的误差低于一个设定好的阈值。
>
> 当上述算法结束，我们就会得到一个训练好的网络，可以应用于一些新的输入数据。

下图展示了一个**MLP**的反向传播过程：(图片来自[excellent visual explanation of the backpropagation algorithm](https://github.com/rasbt/python-machine-learning-book/blob/master/faq/visual-backpropagation.md))

![](http://7xrluf.com1.z0.glb.clouddn.com/ANN5.png)

图中展示的是一个给定的例子的训练数据，训练数据如下：

| 学习时间 | 中期考试成绩 | 期末考试结果  |
| ---- | ------ | ------- |
| 35   | 67     | 1（Pass） |
| 12   | 75     | 0（Fail） |
| 16   | 89     | 1（Pass） |
| 45   | 56     | 1（Pass） |
| 10   | 90     | 0（Fail） |

要预测的例子如下：

| 学习时间 | 中期考试成绩 | 期末考试结果 |
| ---- | ------ | ------ |
| 25   | 70     | ？      |

这个例子中输入数据，或者说输入的特征是学习时间和中期考试成绩，输出结果是期末考试结果，并且是用0和1表示，即这是一个二分类问题。

上图展示的就是**MLP**是一个拥有1个隐藏层的例子，输入层有3个节点，其中1个是数值为1，也就是它与节点的连接上的数值表示的是偏置值，另外两个节点传递的就是两个输入数据；隐藏层中，除了表示偏置值节点外，有两个节点，而输出层也是有两个输出节点，上面的节点表示通过（Pass）的概率，而下面的节点表示不通过（Fail）的概率。

在分类任务中，我们通常使用 [Softmax function](http://cs231n.github.io/linear-classify/#softmax)作为输出层的激活函数，这是为了保证输出值是概率值，并且输出值的和是1。**Softmax**函数会将输入的一个包含任意实数的向量变成一个包含范围在0到1的数值的向量。因此，在这里的例子中，有$Probability(Pass) + Probability(Fail) = 1$

##### Step 1: 前向传播

这里将上图中标记为**V**的隐藏层的节点拿出来介绍下前向传播的过程。假设其连接的节点上的权重分别是$w1,w2,w3$。

网络首先接受第一个训练输入，也就是输入35和67，并且期望的输出值是1，也就是通过的结果。

* 网络的输入 = [35, 67]
* 网络的期望输出 = [1, 0]

在节点**V**上执行的计算为：$V = f(1 * w1 + 35 * w2 + 67 * w3)$。其他节点的计算也类似这个。最终在输出层的两个节点将分别得到两个概率值，假设分别是0.4和0.6（因为权重是随机分配的，所以输出值也是随机的）。很明显，这两个数值不符合期望的数值，因此上图中左上角也说明这是一个错误的输出。

##### Step 2: 反向传播和权重更新

下图展示了反向传播和权重更新的过程：

![](http://7xrluf.com1.z0.glb.clouddn.com/ANN6.png)

这里会计算输出节点的误差，然后使用反向传播计算**梯度**，并将误差传回前面的网络层，进行权重的更新。这里权重的更新使用一种优化方法，比如**梯度下降算法**，目的是减小输出层的误差值。

这里假设经过权重更新，节点**V**的连接上的权重更新为新的权重$w4,w5,w6$。

然后继续还是刚才的输入，此时由于经过反向传播，权重是在向着减小误差的方向下调整的，所以应该有一个更好的输出结果，如下图所示，此时输出层的误差变成[0.2, -0.2],相比于之前的[0.6, -0.4]，这意味着网络已经学习到正确分类第一个训练数据了。

![](http://7xrluf.com1.z0.glb.clouddn.com/ANN7.png)

接下来就可以对整个训练集重复上述过程，然后就可以得到一个训练好的神经网络。此时，需要对测试例子进行输出，则只需要传递测试数据，并执行前向传播，就可以得到输出结果。

#### 5. 小结

这里简单介绍了神经网络，以及**MLP**的网络结构，介绍了前向传播和反向传播的实现过程，不过忽略了详细的数学实现公式。



