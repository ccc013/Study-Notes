## 人脸检测

参考：

- [人脸检测算法综述](https://zhuanlan.zhihu.com/p/36621308)
- [自然场景人脸检测技术实践](https://tech.meituan.com/2020/01/23/meituan-vicface.html)
- [10种轻量级人脸检测算法的比拼](https://blog.csdn.net/nihate/article/details/108798831)
- [目前最好的人脸检测算法，RetinaFace论文精读](https://www.jianshu.com/p/d4534ac94a65)



### 简介

人脸检测的目标是找出图像中所有的人脸对应的位置，算法的输出是人脸外接矩形在图像中的坐标，可能还包括姿态如倾斜角度等信息。

人脸检测算法要解决以下几个核心问题：

```text
 人脸可能出现在图像中的任何一个位置
 人脸可能有不同的大小
 人脸在图像中可能有不同的视角和姿态
 人脸可能部分被遮挡
```

评价一个人脸检测算法好坏的指标是检测率和误报率。我们将检测率定义为：
$$
检测率=\frac{检测出的人脸数}{图像中所有人脸数}
$$
误报率定义为：
$$
误报率=\frac{误报个数}{图像中所有非人脸扫描窗口数}
$$
算法要在检测率和误报率之间做平衡，**理想的情况是有高检测率，低误报率**。

经典的人脸检测算法流程是这样的：用大量的人脸和非人脸样本图像进行训练，得到一个解决2类分类问题的分类器，也称为人脸检测模板。这个分类器接受固定大小的输入图片，判断这个输入图片是否为人脸，即解决是和否的问题。

由于人脸可能出现在图像的任何位置，在检测时用固定大小的窗口对图像从上到下、从左到右扫描，判断窗口里的子图像是否为人脸，这称为滑动窗口技术（sliding window）。为了检测不同大小的人脸，还需要对图像进行放大或者缩小构造图像金字塔，对每张缩放后的图像都用上面的方法进行扫描。由于采用了滑动窗口扫描技术，并且要对图像进行反复缩放然后扫描，因此整个检测过程会非常耗时。

由于一个人脸附件可能会检测出多个候选位置框，还需要将检测结果进行合并去重，这称为非极大值抑制（NMS）。多尺度滑动窗口技术的原理如下图所示：

![](Notes/images/%E5%A4%9A%E5%B0%BA%E5%BA%A6%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3%E6%8A%80%E6%9C%AF%E5%8E%9F%E7%90%86.png)



### 算法

将整个人脸检测算法分为3个阶段，分别是早期算法，AdaBoost框架，以及深度学习时代，在接下来将分这几部分进行介绍。

传统方法在CPU上检测速度快，结果可解释性强，在相对可控的环境下可以达到较好的性能。但是，当训练数据规模成指数增长时，传统方法的性能提升相对有限，在一些复杂场景下，甚至无法满足应用需求。



#### 早期算法

早期的人脸检测算法使用了模板匹配技术，**即用一个人脸模板图像与被检测图像中的各个位置进行匹配**，确定这个位置处是否有人脸；此后机器学习算法被用于该问题，包括神经网络，支持向量机等。以上都是针对图像中某个区域进行人脸-非人脸二分类的判别。

相关文献：

```
[1] Henry A Rowley, Shumeet Baluja, Takeo Kanade. Neural network-based face detection. 1998, IEEE Transactions on Pattern Analysis and Machine Intelligence.

[2] Henry A Rowley, Shumeet Baluja, Takeo Kanade. Rotation invariant neural network-based face detection. 1998, computer vision and pattern recognition.
```

早期有代表性的成果是Rowley等人提出的方法[1][2]。他们用神经网络进行人脸检测，用20x20的人脸和非人脸图像训练了一个多层感知器模型。文献[1]的方法用于解决近似正面的人脸检测问题，原理如下图所示：

![img](https://pic1.zhimg.com/80/v2-b8632fa3bebb9d6a66ad731a729ad030_720w.jpg)

文献[2]的方法解决多角度人脸检测问题，整个系统由两个神经网络构成，第一个网络用于估计人脸的角度，第二个用于判断是否为人脸。角度估计器输出一个旋转角度，然后用整个角度对检测窗进行旋转，然后用第二个网络对旋转后的图像进行判断，确定是否为人脸。系统结构如下图所示：

![img](https://pic2.zhimg.com/80/v2-a84dcd525b7220a35369b83d57d1a5f1_720w.jpg)

Rowley的方法有不错的精度，由于分类器的设计相对复杂而且采用的是密集滑动窗口进行采样分类导致其速度太慢。



#### Adaboost 框架

boost算法是基于PAC学习理论（probably approximately correct）而建立的一套集成学习算法(ensemble learning)。其根本思想在于通过多个简单的弱分类器，构建出准确率很高的强分类器，PAC学习理论证实了这一方法的可行性。

相关文献：

```
[10] S.Z.Li, L.Zhu, Z.Q.Zhang, A.Blake, H.J.Zhang, H.Y.Shum. Statistical learning of multi-view face detection. In: Proceedings of the 7-th European Conference on Computer Vision. Copenhagen, Denmark: Springer, 2002.67-81.
```

在2001年Viola和Jones设计了一种人脸检测算法[10]。它使用简单的Haar-like特征和级联的AdaBoost分类器构造检测器，检测速度较之前的方法有2个数量级的提高，并且保持了很好的精度，我们称这种方法为VJ框架。VJ框架是人脸检测历史上第一个最具有里程碑意义的一个成果，奠定了基于AdaBoost目标检测框架的基础，所以作为重点和大家唠唠。

用级联AdaBoost分类器进行目标检测的思想是：**用多个AdaBoost分类器合作完成对候选框的分类，这些分类器组成一个流水线，对滑动窗口中的候选框图像进行判定，确定它是人脸还是非人脸。**

在这一系列AdaBoost分类器中，前面的强分类器设计很简单，包含的弱分类器很少，可以快速排除掉大量的不是人脸的窗口，但也可能会把一些不是人脸的图像判定为人脸。如果一个候选框通过了第一级分类器的筛选即被判定为人脸，则送入下一级分类器中进行判定，以此类推。如果一个待检测窗口通过了所有的强分类器，则认为是人脸，否则是非人脸。

这种思想的精髓在于**用简单的强分类器在初期快速排除掉大量的非人脸窗口，同时保证高的召回率**，使得最终能通过所有级强分类器的样本数很少。这样做的依据是在待检测图像中，绝大部分都不是人脸而是背景，**即人脸是一个稀疏事件**，如果能快速的把非人脸样本排除掉，则能大大提高目标检测的效率。

出于性能考虑，弱分类器使用了简单的Haar-like特征，这种特征源自于小波分析中的Haar小波变换，Haar小波是最简单的小波函数，用于对信号进行均值、细节分解。这里的Haar-like特征定义为图像中相邻矩形区域像素之和的差值。下图是基本Haar-like特征的示意图：


![img](https://pic2.zhimg.com/80/v2-618a485ece5fa942b42af74291f265e5_720w.jpg)

Haar-like特征是白色矩形框内的像素值之和，减去黑色区域内的像素值之和。以图像中第一个特征为例，它的计算方法如下：首先计算左边白色矩形区域里所有像素值的和，接下来计算右边黑色矩形区域内所有像素的和，最后得到的Haar-like特征值为左边的和减右边的和。

这种特征捕捉图像的边缘、变化等信息，各种特征描述在各个方向上的图像变化信息。人脸的五官有各自的亮度信息，很符合Haar-like特征的特点。

为了实现快速计算，使用了一种称为积分图（Integral Image）的机制。通过积分图可以快速计算出图像中任何一个矩形区域的像素之和，从而计算出各种类型的Haar-like特征。



在深度学习出现以前工业界的方案都是基于VJ算法。但VJ算法仍存在一些问题：

```text
（1)Haar-like特征是一种相对简单的特征，其稳定性较低；
（2)弱分类器采用简单的决策树，容易过拟合。因此，该算法对于解决正面的 人脸效果好，对于人
    脸的遮挡，姿态，表情等特殊且复杂的情况，处理效果不理想（虽然有了一些改进方案，但还是
    不够彻底！！）。
（3 基于VJ-cascade的分类器设计，进入下一个stage后，之前的信息都丢弃了，分类器评价一个
    样本不会基于它在之前stage的表现----这样的分类器鲁棒性差。
```

ACF[15]（Aggregate ChannelFeatures for Multi-view Face Detection）是一种为分类提供足够多的特征选择的方法。在对原图进行处理后，得到多通道的图像，这些通道可以是RGB的通道，可以是平滑滤波得到的，可以是x方向y方向的梯度图等等。将这些通道合起来，在此基础上提取特征向量后续采用Soft-Cascade分类器进行分类。

相较于VJ-cascade的设计，Soft-Cascade采用几个改进的方案：

```
1）每个stage的决策函数不是二值而是标量值(scalar-valued) ,且与该样本有多"容易"通过这个
     stage以及在这个stage的相对重要性成比例。
（2）生成的决策函数是需要通过之前每个阶段的值而不单单是本阶段来判定。
（3）文中把检测器的运行时间-准确率权衡通过一个叫ROC surface的3维曲面清楚的展示出来，方便
    调节参数，可以明确的知道动了哪个参数会对这个检测器的性能会有些什么影响。
```





#### 深度学习

卷积神经网络在图像分类问题上取得成功之后很快被用于人脸检测问题，在精度上大幅度超越之前的AdaBoost框架，当前已经有一些高精度、高效的算法。直接用滑动窗口加卷积网络对窗口图像进行分类的方案计算量太大很难达到实时，使用卷积网络进行人脸检测的方法采用各种手段解决或者避免这个问题。

基于深度学习的方法从算法结构上可以分为三种：

1. 基于级联的人脸检测算法；
2. 两阶段人脸检测算法
3. 单阶段人脸检测算法

其中，

第一类基于级联的人脸检测方法（如Cascade CNN[3]、MTCNN）运行速度较快、检测性能适中，**适用于算力有限、背景简单且人脸数量较少的场景**。

第二类两阶段人脸检测方法一般基于Faster-RCNN框架，在第一阶段生成候选区域，然后在第二阶段对候选区域进行分类和回归，**其检测准确率较高，缺点是检测速度较慢**，代表方法有Face R-CNN 、ScaleFace、FDNet。

最后一类单阶段的人脸检测方法主要基于 Anchor 的分类和回归，通常会在经典框架（如SSD、RetinaNet）的基础上进行优化，**其检测速度较两阶段法快，检测性能较级联法优，是一种检测性能和速度平衡的算法，也是当前人脸检测算法优化的主流方向**。





相关文献：

```
[17] Haoxiang Li, Zhe Lin, Xiaohui Shen, Jonathan Brandt, Gang Hua. A convolutional neural network cascade for face detection. 2015, computer vision and pattern recognition

[18] Lichao Huang, Yi Yang, Yafeng Deng, Yinan Yu. DenseBox: Unifying Landmark Localization with End to End Object Detection. 2015, arXiv: Computer Vision and Pattern Recognition

[19] Shuo Yang, Ping Luo, Chen Change Loy, Xiaoou Tang. Faceness-Net: Face Detection through Deep Facial Part Responses.

[20] Kaipeng Zhan, Zhanpeng Zhang, Zhifeng L, Yu Qiao. Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks. 2016, IEEE Signal Processing Letters.

[21] HR - P. Hu, D. Ramanan. Finding Tiny Faces. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2017.

[22] Face R-CNN - H. Wang, Z. Li, X. Ji, Y. Wang. Face R-CNN. arXiv preprint arXiv:1706.01061, 2017.

[23] SSH - M. Najibi, P. Samangouei, R. Chellappa, L. Davis. SSH: Single Stage Headless Face Detector. IEEE International Conference on Computer Vision (ICCV), 2017.

[24] PyramidBox - X. Tang, Daniel K. Du, Z. He, J. Liu PyramidBox: A Context-assisted Single Shot Face Detector. arXiv preprint arXiv:1803.07737, 2018.
```

##### Cascade CNN

可以认为是传统技术和深度网络相结合的一个代表，和VJ人脸检测器一样，其包含了多个分类器，**这些分类器采用级联结构进行组织**，然而不同的地方在于，Cascade CNN采用卷积网络作为每一级的分类器。

构建多尺度的人脸图像金字塔，12-net将密集的扫描这整幅图像（不同的尺寸），快速的剔除掉超过90%的检测窗口，剩下来的检测窗口送入12-calibration-net调整它的尺寸和位置，让它更接近潜在的人脸图像的附近。

![img](https://pic4.zhimg.com/80/v2-36e3586a464c5c09f773ad97501bb313_720w.jpg)

采用非极大值抑制（NMS）合并高度重叠的检测窗口，保留下来的候选检测窗口将会被归一化到24x24作为24-net的输入，这将进一步剔除掉剩下来的将近90%的检测窗口。和之前的过程一样，通过24-calibration-net矫正检测窗口，并应用NMS进一步合并减少检测窗口的数量

将通过之前所有层级的检测窗口对应的图像区域归一化到48x48送入48-net进行分类得到进一步过滤的人脸候选窗口。然后利用NMS进行窗口合并，送入48-calibration-net矫正检测窗口作为最后的输出。

Cascade CNN一定程度上解决了传统方法在开放场景中对光照、角度等敏感的问题，但是该框架的第一级还是基于密集滑动窗口的方式进行窗口过滤，在**高分辨率存在大量小人脸（tiny face）的图片上限制了算法的性能上限**。



##### DenseBox

**DenseBox**提出了一种称为DenseBox的目标检测算法，适合人脸这类小目标的检测。这种方法使用全卷积网络，在同一个网络中直接预测目标矩形框和目标类别置信度。通过在检测的同时进行关键点定位，进一步提高了检测精度。


检测时的流程如下：

1. 对待检测图像进行缩放，将各种尺度的图像送入卷积网络中处理，以检测不同大小的目标。
2. 经过多次卷积和池化操作之后，对特征图像进行上采样然后再进行卷积，得到最终的输出图像，这张图像包含了每个位置出现目标的概率，以及目标的位置、大小信息。

3. 由输出图像得到目标矩形框。非最大抑制，得到最终的检测结果。
4. 非最大抑制，得到最终的检测结果。



##### Faceness-Net

**Faceness-Net**是一个典型的由粗到精的工作流，借助了多个基于DCNN网络的facial parts分类器对人脸进行打分，然后根据每个部件的得分进行规则分析得到Proposal的人脸区域，最后通过一个Refine的网络得到最终的人脸检测结果。

系统主要包含了2个阶段:

第1阶段：生成partness map，由局部推理出人脸候选区域。

根据attribute-aware深度网络生成人脸部件map图(partness map)，如上图Faceness(a)中的颜色图，文章共使用了5个部件:hair,eye,nose,mouth,beard. 通过part的结合计算人脸的score.部件与部件之间是有相对位置关系的,比如头发在眼睛上方,嘴巴在鼻子下方,因此利用部件的spatial arrangement可以计算face likeliness. 通过这个打分对原始的人脸proposal进行重排序. 如图Faceness(b)。

第2阶段：Refining the face
hypotheses

上一阶段proposal生成的候选框已经有较高的召回率，通过训练一个人脸分类和边界回归的CNN可以进一步提升其效果。

Faceness的整体性能在当时看来非常令人兴奋。此前学术界在FDDB上取得的最好检测精度是在100个误检时达到84%的检测率，Faceness在100个误检时，检测率接近88%，提升了几乎4个百分点；除了算法本身的精度有很大提升，作者还做了很多工程上的优化比如：通过多个网络共享参数，降低网络参数量 83%；采用多任务的训练方式同一网络实现不同任务等。



##### MTCNN

顾名思义是多任务的一个方法，它将人脸区域检测和人脸关键点检测放在了一起，同Cascade CNN一样也是基于cascade的框架，但是整体思路更加巧妙合理，MTCNN总体来说分为三个部分：PNet、RNet和ONet，如下图所示：如下图所示：

![img](https://pic2.zhimg.com/80/v2-b5541a139069644e321156c9968c09c1_720w.jpg)

Cascade CNN第一级的12-net需要在整张图片上做密集窗口采样进行分类，缺陷非常明显；

MTCNN在测试第一阶段的PNet是全卷积网络（FCN），全卷积网络的优点在于可以输入任意尺寸的图像，同时使用卷积运算代替了滑动窗口运算，大幅提高了效率。

下图为不同尺度图像经过PNet的密集分类响应图，亮度越高代表该区域是人脸的概率越大（dense prediction response map）。

<img src="https://pic3.zhimg.com/80/v2-e0c047415e22bdc0dd7a1312b20bba8e_720w.jpg" alt="img" style="zoom:80%;" />

MTCNN的整体设计思路很好，将人脸检测和人脸对齐集成到了一个框架中实现，**另外整体的复杂度得到了很好的控制，可以在中端手机上跑20~30FPS。该方法目前在很多工业级场景中得到了应用**。



##### HR

小目标检测依然是检测领域的一个难题，[21]本文作者提出的检测器通过利用尺度，分辨率和上下文多种信息融合来检测小目标，在上图的总共1000个人脸中成功检测到约800个，检测器的置信度由右侧的色标表示。

针对小目标人脸检测，作者主要从三个方面做了研究：尺度不变，图像分辨率和上下文，作者的算法在FDDB和WIDERFace取得了当时最好的效果。

作者分析了小目标人脸检测的三个问题：

**Multi-task modeling of scales**

一方面，我们想要一个可以检测小人脸的小模板；另一方面，我们想要一个可以利用详细特征（即面部）的大模板来提高准确性。取代“一刀切”的方法，作者针对不同的尺度（和纵横比）分别训练了检测器。虽然这样的策略提升了大目标检测的准确率，但是检测小目标仍然具有挑战性。

**How to generalize pre-trained networks?**

关于小目标检测的问题，作者提出了两个见解。

如何从预训练的深度网络中最佳地提取尺度不变的特征。

虽然许多应用于“多分辨率”的识别系统都是处理一个图像金字塔，但我们发现在插值金字塔的最底层对于检测小目标尤为重要。

因此，作者的最终方法是：通过尺度不变方式，来处理图像金字塔以捕获大规模变化，并采用特定尺度混合检测器

**How best to encode context?**

作者证明从多个层中提取的卷积深度特征（也称为 “hypercolumn” features）是有效的“ foveal”描述符，其能捕获大感受野上的高分辨率细节和粗略的低分辨率线索。



##### Face R-CNN

[22]该方法基于**Faster R-CNN**框架做人脸检测，针对人脸检测的特殊性做了优化。

对于最后的二分类，在softmax的基础上增加了center loss。通过加入center loss使得类内的特征差异更小（起到聚类的作用），提高正负样本在特征空间的差异性从而提升分类器的性能。

加入在线困难样本挖掘（OHEM），每次从正负样本中各选出loss最大的N个样本加入下次训练，提高对困难样本的的分类能力。

多尺度训练，为了适应不同尺度影响(或者更好地检测小目标)，训练阶段图片会经过不同尺度缩放。



##### SSH

[23] SSH最大的特色就是**尺度不相关性**（scale-invariant），比如MTCNN这样的方法在预测的时候，是对不同尺度的图片分别进行预测，而SSH只需要处以一个尺度的图片就可以搞定。

实现方式就是对VGG网络不同level的卷积层输出做了3个分支（M1,M2,M3），每个分支都使用类似的流程进行检测和分类，通过针对不同尺度特征图进行分析，变相的实现了多尺度的人脸检测。

M1和M2,M3区别有点大，首先，M1的通道数为128，M2,M3的通道数为512，这里，作者使用了1*1卷积核进行了降维操作。其次，将conv4_3卷积层输出和conv5_3卷积层输出的特征进行了融合（elementwise sum），由于conv5_3卷积层输出的大小和conv4_3卷积层输出的大小不一样，作者对conv5_3卷积层的输出做了双线性插值进行上采样。



##### PyramidBox

PyramidBox [24] 从论文看主要是已有技术的组合应用，但是作者对不同技术有自己很好的理解，所以能组合的很有效，把性能刷的非常高。

针对之前方法对上下文信息的利用不够充分的问题，作者提出了自己的优化方案：

1. 提出了一种基于 anchor 的上下文信息辅助方法PyramidAnchors，从而可以引入监督信息来学习较小的、模糊的和部分遮挡的人脸的上下文特征
2. 设计了低层特征金字塔网络 ( Low-level Feature Pyramid Networks ) 来更好地融合上下文特征和面部特征，该方法在一次前向过程中（in a single shot）可以很好地处理不同尺度的人脸。
3. 文中提出了一种上下文敏感的预测模块，该模块由一个混合网络结构和max-in-out层组成，该模块可以从融合特征中学习到更准确的定位信息和分类信息（文中对正样本和负样本都采用了该策略，针对不同层级的预测模块为了提高召回率对正负样本设置了不同的参数）。max-in-out参考的maxout激活函数来自GAN模型发明人Ian J,Goodfellow，它对上一层的多个feature map跨通道取最大值作为输出，在cifar10和cifar100上相较于ReLU取得了更好的效果。
4. 文中提出了尺度敏感的Data-anchor-采样策略，改变训练样本的分布，重点关注了较小的人脸。



### 优化思路和业务应用

在自然场景应用中，为了同时满足精度需求以及达到实用的目标，可以在**数据增强和采样策略、模型结构设计和损失函数**等三方面分别进行优化。

相关文献：

```
S3FD: Zhang S, Zhu X, Lei Z, et al. S3fd: Single shot scale-invariant face detector[C]//Proceedings of the IEEE International Conference on Computer Vision. 2017: 192-201.

Pyramidbox：Tang X, Du D K, He Z, et al. Pyramidbox: A context-assisted single shot face detector[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 797-813.

ISRN：Zhang S, Zhu R, Wang X, et al. Improved selective refinement network for face detection[J]. arXiv preprint arXiv:1901.06651, 2019.

mixup：Zhang H, Cisse M, Dauphin Y N, et al. mixup: Beyond empirical risk minimization[J]. arXiv preprint arXiv:1710.09412, 2017.
```



#### 1. 数据增强和采样策略

**单阶段通用目标检测算法对数据增强方式比较敏感**，如

- 经典的SSD算法在VOC2007数据集上通过数据增强性能指标mAP提升6.7。
- 经典单阶段人脸检测算法S3FD 也设计了样本增强策略，使用了图片随机裁切，图片固定宽高比缩放，图像色彩扰动和水平翻转等。



百度在 ECCV2018 发表的 **PyramidBox** 提出了Data-Anchor采样方法，将图像中一个随机选择的人脸进行尺度变换变成一个更小 Anchor 附近尺寸的人脸，同时训练图像的尺寸也进行同步变换。这样做的好处是通过将较大的人脸生成较小的人脸，提高了小尺度上样本的多样性，在WIDER FACE数据集Easy、Medium、Hard集合上分别提升0.4（94.3->94.7），0.4（93.3->93.7），0.6（86.1->86.7）。

**ISRN** 将SSD的样本增强方式和 Data-Anchor 采样方法结合，模型检测性能进一步提高。



**mixup** 在图像分类和目标检测中已经被验证有效，现在用于人脸检测，**有效地防止了模型过拟合问题**。

考虑到业务数据中人脸存在多姿态、遮挡和模糊的样本，且这些样本在训练集中占比小，检测难度大，**因此在模型训练时动态的给这些难样本赋予更高的权重从而有可能提升这些样本的召回率**。



#### 2. 模型结构设计

人脸检测模型结构设计主要包括**检测框架、主干网络、预测模块、Anchor设置与正负样本划分**等四个部分，是单阶段人脸检测方法优化的核心。

相关文献：

```
SRN：Chi C, Zhang S, Xing J, et al. Selective refinement network for high performance face detection[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2019, 33: 8231-8238.

DSFD：Li J, Wang Y, Wang C, et al. Dsfd: dual shot face detector[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 5060-5069.

SFDet：Zhang S, Wen L, Shi H, et al. Single-shot scale-aware network for real-time face detection[J]. International Journal of Computer Vision, 2019, 127(6-7): 537-559.
```



##### 检测框架

近年来单阶段人脸检测框架取得了重要的发展，代表性的结构有 S3FD 中使用的SSD，SFDet 中使用的RetinaNet，SRN中使用的两步结构（后简称SRN）以及DSFD中使用的双重结构（后简称DSFD）。

其中，SRN是一种单阶段两步人脸检测方法，利用第一步的检测结果，在小尺度人脸上过滤易分类的负样本，改善正负样本数量的均衡性，针对大尺度的人脸采用迭代求精的方式进行人脸定位，改善大尺度人脸的定位精度，提升了人脸检测的准确率。在WIDER FACE上测评SRN取得了最好的检测效果（按标准协议用AP平均精度来衡量）。

下表是分析上述四种检测结构在 WIDER FACE上的评估结果，骨干网络采用的是 ResNet50：

![img](https://p1.meituan.net/travelcube/cbb0e2d960daedab3fc5c47bc9e0c5ad55677.png)

这里美团的网络模型 VICFace 继承了当前性能最好的SRN检测结构，同时为了更好的融合自底向上和自顶向下的特征，为不同特征不同通道赋予不同的权重。



##### 主干网络

单阶段人脸检测模型的主干网络通常使用分类任务中的经典结构（如VGG、ResNet等）。

其中，主干网络在ImageNet数据集上分类任务表现越好，其在WIDER FACE上的人脸检测性能也越高，如下表所示。

![img](https://p0.meituan.net/travelcube/421442751aae1200bbbff96185624b8b137271.png)

为了保证检测网络得到更高的召回，在性能测评时VICFace主干网络使用了在ImageNet上性能较优的ResNet152网络（其在ImageNet上Top1分类准确率为80.26），并且在实现时将Kernel为7x7，Stride为2的卷积模块调整为为3个3x3的卷积模块，其中第一个模块的Stride为2，其它的为1；将Kernel为1x1，Stride为2的下采样模块替换为Stride为2的Avgpool模块。



##### 预测模块

**利用上下文信息可以进一步提高模型的检测性能**。

SSH 是将上下文信息用于单阶段人脸检测模型的早期方案，PyramidBox、SRN、DSFD等也设计了不同上下文模块。

如下图所示，SRN上下文模块使用1xk，kx1的卷积层提供多种矩形感受野，多种不同形状的感受野助于检测极端姿势的人脸；DSFD使用多个带孔洞的卷积，极大的提升了感受野的范围。

![img](https://p1.meituan.net/travelcube/b2b3b1e4a1ba956f4e59b74f1aa9920c373837.png)

在 VICFace 中，将带孔洞的卷积模块和1xk，kx1的卷积模块联合作为Context Module，既提升了感受野的范围也有助于检测极端姿势的人脸，同时使用Maxout模块提升召回率，降低误检率。它还利用Cn层特征预测的人脸位置，校准Pn层特征对应的区域，如下图所示。Cn层预测的人脸位置相对特征位置的偏移作为可变卷积的Offset输入，Pn层特征作为可变卷积的Data输入，经过可变卷积后特征对应的区域与人脸区域对应更好，相对更具有表示能力，可以提升人脸检测模型的性能。

![img](https://p0.meituan.net/travelcube/b18b4078546f3f05d4171f53a9932515151781.png)

##### Anchor设置与正负样本划分

基于Anchor的单阶段人脸检方法通过Anchor的合理设置可以有效的控制正负样本比例和缓解不同尺度人脸定位损失差异大的问题。现有主流人脸检测方法中Anchor的大小设置主要有以下三种（S代表Stride）：

![img](https://p0.meituan.net/travelcube/4efc467b997e816a66f50030eb08ecec17466.png)

根据数据集中人脸的特点，Anchor的宽高也可以进一步丰富，如{1}，{0.8}，{1，0.67}。

在自研方案中，在C3、P3层，Anchor的大小为2S和4S，其它层Anchor大小为4S（S代表对应层的Stride），这样的Anchor设置方式在保证人脸召回率的同时，减少了负样本的数量，在一定程度上缓解了正负样本不均衡现象。根据人脸样本宽高比的统计信息，将Anchor的宽高比设置为0.8，同时将Cn层IoU大于0.7的样本划分为正样本，小于0.3的划分为负样本，Pn层IoU大于0.5的样本划分为正样本，小于0.4的划分为负样本。



#### 3. 损失函数

人脸检测的优化目标**不仅需要区分正负样本（是否是人脸），还需要定位出人脸位置和尺寸**。

S3FD 中区分正负样本使用交叉熵损失函数，定位人脸位置和尺寸使用Smooth L1 Loss，同时使用困难负样本挖掘解决正负样本数量不均衡的问题。另一种**缓解正负样本不均衡带来的性能损失更直接的方式是Lin等人提出Focal Loss**。

UnitBox 提出IoU Loss可以缓解不同尺度人脸的定位损失差异大导致的性能损失。AlnnoFace同时使用Focal Loss和IoU Loss提升了人脸检测模型的性能。引入其它相关辅助任务也可以提升人脸检测算法的性能，RetinaFace 引入关键点定位任务，提升人脸检测算法的定位精度；DFS 引入人脸分割任务，提升了特征的表示能力。

综合前述方法的优点，VICFace充分利用人脸检测及相关任务的互补信息，使用多任务方式训练人脸检测模型。在人脸分类中使用Focal Loss来缓解样本不均衡问题，同时使用人脸关键点定位和人脸分割来辅助分类目标的训练，从而提升整体的分类准确率。在人脸定位中使用Complete IoU Loss，以目标与预测框的交并比作为损失函数，缓解不同尺度人脸损失的差异较大的问题，同时兼顾目标和预测框的中心点距离和宽高比差异，从而可以达到更好整体检测性能。



### 相关代码

1. 10 种轻量级人脸检测算法实现：https://github.com/hpc203/10kinds-light-face-detector-align-recognition
2. 纯使用yolo系列的神经网络做人脸检测+人脸关键点检测，人脸关键点检测输出106个关键点：https://github.com/hpc203/yoloface-landmark106
3. yolov5检测人脸和关键点的程序，只依赖opencv库就可以运行，程序包含C++和Python两个版本的：
   - https://github.com/hpc203/yolov5-face-landmarks-opencv
   - https://github.com/hpc203/yolov5-face-landmarks-opencv-v2
4. 






------

## 表情识别

参考：

- [【技术综述】人脸表情识别研究](https://mp.weixin.qq.com/s?__biz=MzA3NDIyMjM1NA==&mid=2649029493&idx=1&sn=3a6442bfbc2f1a917420adc3eed91272&chksm=87134508b064cc1ed0e23cee897946f7a7bd4dbd43e8516cc74a6760431229da7909006bf40a#rd)
- [【人脸表情识别】基于图片的人脸表情识别，基本概念和数据集](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649042152%26idx%3D1%26sn%3Db67302f9fedd274a4d3b908d9f9d5a82%26chksm%3D87129295b0651b835a113af492e2de902dbc35bab574cc6868d5347d37f2ec10263f53c70908%26scene%3D21%23wechat_redirect)
- [【人脸表情识别】如何做好表情识别任务的图片预处理工作](https://link.zhihu.com/?target=https%3A//mp.weixin.qq.com/s%3F__biz%3DMzA3NDIyMjM1NA%3D%3D%26mid%3D2649042177%26idx%3D1%26sn%3Dd0b0049f51fe018735649d9dcd0e7f3a%26chksm%3D8712937cb0651a6a390ff073f98e3ae5473a1cb8d425492497be07b45df1e36b3e9ff24c1e9e%26scene%3D21%23wechat_redirect)





### 简介

人类的面部表情至少有21种，除了常见的高兴、吃惊、悲伤、愤怒、厌恶和恐惧6种，还有惊喜（高兴＋吃惊）、悲愤（悲伤＋愤怒）等15种可被区分的复合表情。



#### 表情的研究

面部表情的研究始于19世纪，1872年，达尔文在他著名的论著《人类和动物的表情（The Expression of the Emotions in Animals and Man，1872）》中就阐述了人的面部表情和动物的面部表情之间的联系和区别。

1971年，Ekman和Friesen对现代人脸表情识别做了开创性的工作，他们研究了人类的6种基本表情（即高兴、悲伤、惊讶、恐惧、愤怒、厌恶），确定识别对象的类别，并系统地建立了有上千幅不同表情的人脸表情图像数据库，细致的描述了每一种表情所对应的面部变化，包括眉毛、眼睛、眼睑、嘴唇等等是如何变化的。

1978年，Suwa等人对一段人脸视频动画进行了人脸表情识别的最初尝试，提出了在图像序列中进行面部表情自动分析。

20世纪90年代开始，由K.Mase和A.Pentland使用光流来判断肌肉运动的主要方向，**使用提出的光流法进行面部表情识别之后**，自动面部表情识别进入了新的时期。



#### 表情识别的应用

在线 API 方面有：

1. **Microsoft Azure**：包括了人脸验证、面部检测以及表情识别等；
2.  **Baidu AI开放平台（配备微信小程序）**：该API可以检测图中的人脸，并为人脸标记出边框。检测出人脸后，可对人脸进行分析，获得眼、口、鼻轮廓等72个关键点定位准确识别多种人脸属性，如性别，年龄，表情等信息。该技术可适应大角度侧脸，遮挡，模糊，表情变化等各种实际环境。
3. **腾讯优图AI开放平台（配备微信小程序）**：该API对于任意一幅给定的图像，采用智能策略对其进行搜索以确定其中是否含有人脸，如果是则返回人脸的位置、大小和属性分析结果。当前支持的人脸属性有：性别、表情（中性、微笑、大笑）、年龄（误差估计小于5岁）、是否佩戴眼镜（普通眼镜、墨镜）、是否佩戴帽子、是否佩戴口罩。

APP 方面有：

1. **Polygram**：Polygram是一个人工智能动力社会网络，可以理解人脸表情。它以基于人脸识别的表情包为主要特色，即能够利用人脸识别技术，对面部的真实表情进行检测，从而搜索到相应的表情，并发送该表情
2. **落网emo**：emo，是一款可以识别情绪的音乐APP



### 表情常用开源数据库

**(1) KDEF与AKDEF(karolinska directed emotional faces)数据集**

链接：http://www.emotionlab.se/kdef/

这个数据集最初是被开发用于心理和医学研究目的。它主要用于知觉，注意，情绪，记忆等实验。在创建数据集的过程中，特意使用比较均匀，柔和的光照，被采集者身穿统一的T恤颜色。这个数据集，包含70个人，35个男性，35个女性，年龄在20至30岁之间。没有胡须，耳环或眼镜，且没有明显的化妆。7种不同的表情，每个表情有5个角度。总共4900张彩色图。尺寸为562*762像素。



**(2) RaFD数据集**

链接：http://www.socsci.ru.nl:8180/RaFD2/RaFD?p=main

该数据集是Radboud大学Nijmegen行为科学研究所整理的，这是一个高质量的脸部数据库，总共包含67个模特：20名白人男性成年人，19名白人女性成年人，4个白人男孩，6个白人女孩，18名摩洛哥男性成年人。总共8040张图，包含8种表情，即愤怒，厌恶，恐惧，快乐，悲伤，惊奇，蔑视和中立。每一个表情，包含3个不同的注视方向，且使用5个相机从不同的角度同时拍摄的。

链接:http://www.whdeng.cn/RAF/model1.html

---数据采集方式：采集自互联网

---表情标签：开心、悲伤、惊讶、害怕、厌恶、生气、中立；复合情绪（可参考图6中的例子理解）

---数据集大小：总共29672张图片





**(3) Fer2013数据集**

该数据集，包含共26190张48*48灰度图，图片的分辨率比较低，共6种表情。分别为0 anger生气、1 disgust 厌恶、2 fear 恐惧、3 happy 开心、4 sad 伤心、5 surprised 惊讶、6 normal 中性。

链接:https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/overview

---数据采集方式：通过Google搜索引擎获取（这种即被定义为自然状态下自发式的表情数据）

---表情标签：开心、悲伤、惊讶、害怕、厌恶、生气、中立

---数据集大小：训练集含28709张图片, 验证集含3589张图片，测试集含3589张图片，分辨率48*48，数据及标签存放在csv文件里

该数据集是一个通过Google搜索引擎然后爬取收集成的一个人脸表情数据集，也是应该第一个比较有名的非受控条件下人脸表情的数据集。

但该数据集最大的问题是**许多与人脸表情毫无相关的图片会被贴上表情的标签，标注错误的样本也不少**，因此如果直接使用一些经典的深度学习模型去训练，在不加任何tricks的情况下，识别准确率。

针对数据集存在的问题，微软在2016年重新标定了FER2013数据集并命名为FER plus，用重新标定后的数据集识别准确率会有所提高（曾用同样的方法，在FER2013中识别的准确率50%-60%，在FER plus中准确率70%-80%）。微软的工作论文可参考 ：

```
Barsoum E, Zhang C, Ferrer C C, et al. Training deep networks for facial expression recognition with crowd-sourced label distribution[C]//Proceedings of the 18th ACM International Conference on Multimodal Interaction. 2016: 279-283.
```

代码可参考：https://github.com/microsoft/FERPlus。

近几年许多文章在实验对比中也是会直接采用FER plus作为自己方法的对比。





**(4) CelebFaces Attributes Dataset (CelebA)数据集**

链接：http://mmlab.ie.cuhk.edu.hk/projects/CelebA.html

CelebA是商汤科技的一个用于研究人脸属性的数据集，一个包含超过200K名人图像的大型人脸属性数据集，每个数据集都有40个属性注释。该数据集中的图像涵盖了大型姿态变化和复杂背景。CelebA的多样非常好，有约10万张带微笑属性的数据



**(5) Surveillance Cameras Face Database（SCface）**

链接：http://www.scface.org/

SCface是人脸静态图像的数据库。图像是在不受控制的室内环境中使用五种不同品质的视频监控摄像机拍摄的。数据库包含130个主题的4160静态图像（在可见和红外光谱中）。



**(6) Japanese Female Facial Expression (JAFFE) Database**

该数据库包含由10名日本女性模特组成的7幅面部表情（6个基本面部表情+1个中性）的213幅图像。每个图像被60个日语科目评为6个情感形容词。

链接:https://zenodo.org/record/3451524#.X2MFNG4zZPY

---数据采集方式：10位日本女性在实验室条件下摆拍指定表情获得

---表情标签：开心、悲伤、惊讶、害怕、厌恶、生气、中立

---数据集大小：共213张图片，每个人每种表情大概3-4张图片，每张图片分辨率256*256像素



**(7) MMI Facial Expression Database**

链接:https://www.mmifacedb.eu/

---数据采集方式：32位参与者在实验室条件下摆拍指定表情获得

---表情标签：开心、悲伤、惊讶、害怕、厌恶、生气、中立；AU；时序状态（表情开始帧-->峰值帧-->结束帧，onset-->apex-->offset）

---数据集大小：2900个视频以及740张图片，分辨率 720*576像素



**(8) Extended Cohn-Kanade Dataset (CK+) **

链接:http://www.pitt.edu/~emotion/ck-spread.htm

---数据采集方式：123位参与者在实验室条件下摆拍指定表情获得

---表情标签：开心、悲伤、惊讶、害怕、厌恶、生气、中立、轻蔑；AU

---数据集大小：593个视频序列，分辨率640*490或者640*480，基于图片的人脸表情识别中常常取最后几帧作为样本



**（9）AffectNet** 

链接:http://mohammadmahoor.com/affectnet/

---数据采集方式：采集自互联网

---表情标签：开心、悲伤、惊讶、害怕、厌恶、生气、中立；二维连续模型

---数据集大小：总计1800000张图片，其中450000是提供离散以及连续的标签，是目前自然条件下最大的表情识别图片数据集，也是提供二维连续模型标签中最大的数据集



### 研究方法

一般的人脸表情识别系统步骤如下：

1. 图像获取
2. 人脸检测
3. 特征提取
4. 特征分类



由于开源表情数据库目前已经比较多，图像获取难度不大，人脸检测算法也比较成熟，已经发展成为一个独立的研究方向，因此人脸表情识别的研究主要体现在系统的后面两个步骤：特征提取和特征分类上，下面将从传统研究方法和深度学习研究方法对以上两个步骤进行阐述。



#### 传统研究方法

##### 特征提取

人脸表情的产生是一个很复杂的过程，如果不考虑心理和环境因素，呈现在观察者面前的就是单纯的肌肉运动，以及由此带来的面部形体和纹理的变化。

静态图像呈现的是表情发生时**单幅图像**的表情状态，动态图像呈现的是表情在**多幅图像之间**的运动过程。

因此根据表情发生时的状态和处理对象来区分，表情特征提取算法大体分为**基于静态图像的特征提取方法**和**基于动态图像的特征提取方法**。

其中基于静态图像的特征提取算法可分为**整体法**和**局部法**，基于动态图像的特征提取算法又分为**光流法**、**模型法**和**几何法**。



**基于静态图像的特征提取方法**

**整体法**

从整体上看，人脸表情的变化会造成面部器官的明显形变，对人脸图像的全局信息带来影响，因此也有从整体角度考虑表情特征的表情识别算法。

经典算法包括了主元分析法（PCA）、独立分量分析法（ICA）和线性判别分析法（LDA）。相关的文献有：

```
[1] 何良华. 人脸表情识别中若干关键技术的研究[D]. 东南大学, 2005.

[2] 周书仁, 梁昔明, 朱灿,等. 基于ICA与HMM的表情识别[J]. 中国图象图形学报, 2008, 13(12):2321-2328.

[3] 周书仁. 人脸表情识别算法分析与研究[D]. 中南大学, 2009.

[4] 应自炉, 唐京海, 李景文,等. 支持向量鉴别分析及在人脸表情识别中的应用[J]. 电子学报, 2008, 36(4):725-730.

[5] Khorasani K. Facial expression recognition using constructive neural networks[C]// Signal Processing, Sensor Fusion, and Target Recognition X. Signal Processing, Sensor Fusion, and Target Recognition X, 2001:1588 - 1595.
```

其中：

> 文献【1-3】采用FastICA算法提取表情特征，该方法不但继承了ICA算法能够提取像素间隐藏信息的特点，而且可以通过迭代，快速地完成对表情特征的分离。
>
> 文献【4】提出了支持向量鉴别分析（SVDA）算法，该算法以Fisher线性判别分析和支持向量机基础，能够在小样本数据情况下，使表情数据具有最大的类间分离性，而且不需要构建SVM算法所需要的决策函数。实验证明了该算法的识别率高于PCA和LDA。
>
> 文献【5】依靠二维离散余弦变换，通过频域空间对人脸图像进行映射，结合神经网络实现对表情特征的分类。



**局部法**

静态图像上的人脸表情不仅有整体的变化，也存在局部的变化。面部肌肉的纹理、皱褶等局部形变所蕴含的信息，有助于精确地判断表情的属性。

局部法的经典方法是Gabor小波法和LBP算子法。

相关文献：

```
[6] Kyperountas M, Tefas A, Pitas I. Salient feature and reliableclassifier selection for facial expression classification[J]. PatternRecognition, 2010, 43(3):972-986.

[7] Zheng W, Zhou X, Zou C, et al. Facial expression recognitionusing kernel canonical correlation analysis (KCCA).[J]. IEEETransactions on Neural Networks, 2006, 17(1):233.

[8] 付晓峰. 基于二元模式的人脸识别与表情识别研究[D]. 浙江大学,2008.
```

这些算法的贡献：

> 文献【6】以Gabor小波等多种特征提取算法为手段，结合新的分类器对静态图像展开实验。
>
> 文献【7】首先人工标记了34个人脸特征点，然后将特征点的Gabor小波系数表示成标记图向量，最后计算标记图向量和表情语义向量之间的KCCA系数，以此实现对表情的分类。
>
> 文献【8】提出了CBP算子法，通过比较环形邻域的近邻点对，降低了直方图的维数。针对符号函数的修改，又增强了算法的抗噪性，使CBP算子法取得了较高的识别率。



**基于动态图像的特征提取方法：**

动态图像与静态图像的不同之处在于：动态图像反映了人脸表情发生的过程。因此动态图像的表情特征主要表现在人脸的持续形变和面部不同区域的肌肉运动上。

目前基于动态图像的特征提取方法主要分为**光流法、模型法和几何法**。

**（1）光流法**

光流法是反映动态图像中不同帧之间相应物体灰度变化的方法。早期的人脸表情识别算法多采用光流法提取动态图像的表情特征，这主要在于光流法具有突出人脸形变、反映人脸运动趋势的优点。因此该算法依旧是传统方法中来研究动态图像表情识别的重要方法。

```
[9] Yacoob Y, Davis L S. Recognizing Human Facial ExpressionsFrom Long Image Sequences Using Optical Flow[J]. PatternAnalysis & Machine Intelligence IEEE Transactions on, 1996,18(6):636-642.
```

> 文献【9】首先采用连续帧之间的光流场和梯度场，分别表示图像的时空变化，实现每帧人脸图像的表情区域跟踪；然后通过特征区域运动方向的变化，表示人脸肌肉的运动，进而对应不同的表情。



**（2）模型法**

人脸表情识别中的模型法是指对动态图像的表情信息进行参数化描述的统计方法。常用算法主要包括主动形状模型法（ASM）和主动外观模型法（AAM），两种算法都可分为形状模型和主观模型两部分。就表观模型而言，ASM反映的是图像的局部纹理信息，而AAM反映的是图像的全局纹理信息。

```
[10] Tsalakanidou F, Malassiotis S. Real-time 2D+3D facial actionand expression recognition[J]. Pattern Recognition, 2010,43(5):1763-1775.

[11] Wang J, Yin L. Static topographic modeling for facialexpression recognition and analysis[J]. Computer Vision &Image Understanding, 2007, 108(1):19-34.

[12] Sung J, Kim D. Pose-Robust Facial Expression RecognitionUsing View-Based 2D $+$ 3D AAM[J]. IEEE Transactions onSystems, Man, and Cybernetics - Part A: Systems and Humans,2008, 38(4):852-866.
```



> 文献【10】提出了基于ASM的三维人脸特征跟踪方法，该方法对人脸81个特征点进行跟踪建模，实现了对部分复合动作单元的识别。
>
> 文献【11】借助图像的地形特征模型来识别人脸动作和表情；利用AAM和人工标记的方法跟踪人脸特征点，并按照特征点取得人脸表情区域；通过计算人脸表情区域的地形直方图来获得地形特征，从而实现表情识别。
>
> 文献【12】提出了基于二维表观特征和三维形状特征的AAM算法，在人脸位置发生偏移的环境下，实现了对表情特征的提取。



**（3）几何法**

在表情特征提取方法中，研究者考虑到表情的产生与表达在很大程度上是依靠面部器官的变化来反映的。人脸的主要器官及其褶皱部分都会成为表情特征集中的区域。因此在面部器官区域标记特征点，计算特征点之间的距离和特征点所在曲线的曲率，就成为了采用几何形式提取人脸表情的方法。

```
[13] Kotsia I, Pitas I. Facial Expression Recognition in ImageSequences Using Geometric Deformation Features and SupportVector Machines[J]. IEEE Transactions on Image Processing APublication of the IEEE Signal Processing Society, 2007,16(1):172.
```



> 文献【13】使用形变网格对不同表情的人脸进行网格化表示，将第一帧与该序列表情最大帧之间的网格节点坐标变化作为几何特征，实现对表情的识别。



##### 特征分类

特征分类的目的是判断特征所对应的表情类别。在人脸表情识别中，表情的类别分为两部分：基本表情和动作单元。前者一般适用于所有的处理对象，后者主要适用于动态图像，可以将主要的特征分类方法分为**基于贝叶斯网络的分类方法**和**基于距离度量的分类方法**。



**（1）基于贝叶斯网络的分类方法**

贝叶斯网络是以贝叶斯公式为基础、基于概率推理的图形化网络。从人脸表情识别的角度出发，概率推理的作用就是从已知表情信息中推断出未知表情的概率信息的过程。基于贝叶斯网络的方法包括各种贝叶斯网络分类算法和隐马尔科夫模型（HMM）算法。

```
[14] Cohen I, Sebe N, Garg A, et al. Facial expression recognitionfrom video sequences: temporal and static modeling[J].Computer Vision & Image Understanding, 2003, 91(1–2):160-187.
```

> 文献【14】研究者 分别采用了朴素贝叶斯（NB）分类器、树增强器（TAN）和HMM实现表情特征分类。



**（2）基于距离度量的分类方法**

基于距离度量的分类方法是通过计算样本之间的距离来实现表情分类的。代表算法有近邻法和SVM算法。近邻法是比较未知样本x与所有已知类别的样本之间的欧式距离，通过距离的远近来决策x与已知样本是否同类；SVM算法则是通过优化目标函数，寻找到使不同类别样本之间距离最大的分类超平面。

```
[8] 付晓峰. 基于二元模式的人脸识别与表情识别研究[D]. 浙江大学,2008

[15] 徐文晖, 孙正兴. 面向视频序列表情分类的LSVM算法[J]. 计算机辅助设计与图形学学报, 2009, 21(4):000542-553.

[16] 徐琴珍, 章品正, 裴文江,等. 基于混淆交叉支撑向量机树的自动面部表情分类方法[J]. 中国图象图形学报, 2008, 13(7):1329-1334.
```



> 文献【8】采用了最近邻法对表情特征进行分类，并指出最近邻法的不足之处在于分类正确率的大小依赖于待分类样本的数量。
>
> 文献【15,16】分别从各自角度提出了对SVM的改进，前者将k近邻法与SVM结合起来，把近邻信息集成到SVM的构建中，提出了局部SVM分类器；后者提出的CSVMT模型将SVM和树型模块结合起来，以较低的算法复杂度解决了分类子问题。



#### 深度学习研究方法

与传统方法特征提取不同，之所以采用深度学习的方法，是因为深度学习中的网络（尤其是CNN）对图像具有较好的提取特征的能力，从而避免了人工提取特征的繁琐，人脸的人工特征包括常用的68个Facial landmarks等其他的特征，而深度学习除了预测外，往往还扮演着特征工程的角色，从而省去了人工提取特征的步骤。

关于人脸表情识别的讨论一直在继续，很多学者团队都聚焦于此。

```
[17] Benitez-Quiroz C F, Srinivasan R, Martinez A M. EmotioNet:An Accurate, Real-Time Algorithm for the Automatic Annotationof a Million Facial Expressions in the Wild[C]// Computer Visionand Pattern Recognition. IEEE, 2016:5562-5570.

[18] Benitezquiroz C F, Wang Y, Martinez A M. Recognition ofAction Units in the Wild with Deep Nets and a New Global-LocalLoss[C]// IEEE International Conference on Computer Vision.IEEE Computer Society, 2017:3990-3999.
```



> 文献【17】提出了用于注释自然情绪面部表情的一百万个图像的大型数据库（即，从因特网下载的面部图像）。首先，证明这个新提出的算法可以**跨数据库**可靠地识别AU及其强度。根据调研，这是第一个在多个数据库中识别AU及其强度的高精度结果的已发布算法。算法可以实时运行（> **30张图像/秒**），允许它处理大量**图像**和**视频序列**。其次，使用**WordNet**从互联网下载1,000,000张面部表情图像以及相关的情感关键词。然后通过我们的算法用AU，AU强度和情感类别**自动注释**这些图像。可以得到一个非常有用的数据库，可以使用语义描述轻松查询计算机视觉，情感计算，社会和认知心理学和神经科学中的应用程序。
>
> 文献【18】提出了一种深度神经体系结构，它通过在初始阶段结合学习的局部和全局特征来解决这两个问题，并在类之间复制消息传递算法，类似于后期阶段的图形模型推理方法。结果表明，通过增加对端到端训练模型的监督，在现有水平的基础上我们分别在BP4D和DISFA数据集上提高了5.3％和8.2％的技术水平。





### 图片预处理工作

人脸表情识别中也需要对人脸进行一些预处理，因为有些人脸表情数据是用户在非限制条件下拍摄产生的，这种数据可能存在姿态变换、光线、遮挡等问题，端到端的表情方法在复杂的现实条件下很容易产生误差。

所以使用合适的预处理可以减少因为图像质量对识别效果的影响，并提高算法的鲁棒性。

参考最近两年的文献：

```
[1] Li S, Deng W. Deep facial expression recognition: A survey[J]. IEEE Transactions on Affective Computing, 2020.

[2] Canedo D, Neves A J R. Facial Expression Recognition Using Computer Vision: A Systematic Review[J]. Applied Sciences, 2019, 9(21): 4678.
```



#### 1. 人脸检测

人脸检测是人脸相关的任务中最常见也会包含的一个预处理模块了，主要是从图像中提取人脸的区域，然后就可以只专注处理人脸的特征。

相关文献：

```
[3] Viola P, Jones M. Rapid object detection using a boosted cascade of simple features[C]//Proceedings of the 2001 IEEE computer society conference on computer vision and pattern recognition. CVPR 2001. IEEE, 2001, 1: I-I.

[4] Lienhart R, Maydt J. An extended set of haar-like features for rapid object detection[C]//Proceedings. international conference on image processing. IEEE, 2002, 1: I-I.

[5] Zhang K, Zhang Z, Li Z, et al. Joint face detection and alignment using multitask cascaded convolutional networks[J]. IEEE Signal Processing Letters, 2016, 23(10): 1499-1503.
```

最常使用的是 Viola和Jones在2001年在 [3] 提出的Viola-Jones（下面简称V&J）目标检测器，它基于类Haar特征以及Adaboost分类器实现了一个实时目标检测的框架。同时由于Haar特征更适合于人脸，因此其作为人脸检测工具被更多人熟知。OpenCV中的Haar分类器就是基于Viola-Jones方法的一个改进版本 [4]。

当然还有基于深度学习的人脸检测算法，如比较著名的多任务级联卷积神经网络（Multi-Task Cascaded Convolutional Network, MTCNN）[5]，或者直接用目标检测的相关算法进行人脸的检测，如Faster-CNN、SSD等进行。

有关人脸检测方法的选择，在综述[2]中，对V&J和MTCNN做了简单的对比：“尽管在大部分的论文中V&J仍然是人脸检测方法中最常见的选择，但MTCNN在保证实时性的同时，在人脸检测和人脸对齐的几个具有挑战性的基准测试中胜过V&J。V&J仍然是使用最多的人脸检测器的主要原因是，过去大多数论文都在受控条件表情数据集中对他们提出的人脸表情识别方法进行实验，**而V&J可以在受控的数据集中稳定地检测到人脸。在非受控（自然）条件表情数据库中实验人脸表情识别方法通常是使用MTCNN**”。

MTCNN检测原理如下图所示：

<img src="Notes/images/MTCNN.png" style="zoom:50%;" />



#### 2. 脸部归一化

如果是在非受控条件下，人脸数据很可能会受到姿态变换、光线、遮挡等问题的干扰，在这些情况下，人脸表达情绪的核心区域（眼睛、嘴巴）的信息就会缺失，从而大大影响模型训练/测试的效果。因此，一些研究者考虑采用**一些方法将人脸转换为归一化（normalization）的人脸（正脸、对比度合适、无遮挡）再进行表情识别模型的训练**。



**光照归一化**

```
[6] Shin M, Kim M, Kwon D S. Baseline CNN structure analysis for facial expression recognition[C]//2016 25th IEEE International Symposium on Robot and Human Interactive Communication (RO-MAN). IEEE, 2016: 724-729.

[7] Yu Z, Zhang C. Image based static facial expression recognition with multiple deep network learning[C]//Proceedings of the 2015 ACM on international conference on multimodal interaction. 2015: 435-442.

[8] Bargal S A, Barsoum E, Ferrer C C, et al. Emotion recognition in the wild from videos using images[C]//Proceedings of the 18th ACM International Conference on Multimodal Interaction. 2016: 433-436.

[9] Kuo C M, Lai S H, Sarkis M. A compact deep learning model for robust facial expression recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2018: 2121-2129.
```

早期研究 [6] 主要通过**基于各向同性扩散**（isotropic diffusion，IS）的归一化、**基于离散余弦变换**（discrete cosine transform，DCT）的归一化、**高斯函数差分**（difference of Gaussian，DoG）进行光照归一化；

随后一些研究表明，与单独使用光照归一化相比，**直方图均衡与光照归一化方法相结合**可获得更好的人脸识别性能，因此部分的研究者也将这种方法应用到表情识别中 [7,8]；

但是，直接使用直方图均衡可能会**过分突出局部对比度**， 为了解决这个问题，[9]提出了一种加权求和方法，将直方图均衡和线性映射相结合。



**姿态归一化**

```
[10] Hassner T, Harel S, Paz E, et al. Effective face frontalization in unconstrained images[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2015: 4295-4304.

[11] Yin X, Yu X, Sohn K, et al. Towards large-pose face frontalization in the wild[C]//Proceedings of the IEEE international conference on computer vision. 2017: 3990-3999.

[12] Tran L, Yin X, Liu X. Disentangled representation learning gan for pose-invariant face recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2017: 1415-1424.
```

姿态归一化主要内容就是将一些侧脸的人脸图像转化为正面的人脸图，最直接有效的方法是先进行三维重建，然后通过反投影的方式来合成原始图像的正脸图像 [10]；

通过GAN来生成正脸表情也是近些年出现比较频繁的姿态归一化方式 [11,12]。（ps：**由于姿态归一化有可能导致改变表情，尤其是在非受控条件下，因此近些年的文章也很少用姿态归一化进行预处理**）。



**去除遮挡**

```
[13] Li Y, Zeng J, Shan S, et al. Occlusion aware facial expression recognition using cnn with attention mechanism[J]. IEEE Transactions on Image Processing, 2018, 28(5): 2439-2450.

[14] Pan B, Wang S, Xia B. Occluded facial expression recognition enhanced through privileged information[C]//Proceedings of the 27th ACM International Conference on Multimedia. 2019: 566-573.
```

遮挡问题相对于光照、姿态变换更加复杂，原因在于遮挡现象的出现是很不规律的。遮挡的部位可能是人脸上任意部位，遮挡物也可以是任意东西（头发、眼睛甚至拍摄图片时的外部物体），因此更多的文章[13,14]是把带遮挡的表情识别单独作为研究命题，通过构建特殊网络对含有遮挡的人脸表情进行识别。



#### 3. 数据增强

最后一种预处理也是所有深度学习任务最常用的预处理方式——数据增强。深度学习需要足够多的训练数据才能保证算法模型的准确性与泛化能力。

在表情识别领域，即便是研究得最久远的基于图片的人脸表情识别，目前最大的数据集AffectNet是40多万张图，跟ImageNet、VGGFace2等数据集相比还是小巫见大巫，至于其他更小众的表情识别（如微表情识别）则更是少之又少。

因此，有关表情识别的论文中基本上都会包含数据增强这步预处理操作。

```
[15] Hua W, Dai F, Huang L, et al. HERO: Human emotions recognition for realizing intelligent Internet of Things[J]. IEEE Access, 2019, 7: 24321-24332.

[16] Meng Z, Liu P, Cai J, et al. Identity-aware convolutional neural network for facial expression recognition[C]//2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017). IEEE, 2017: 558-565.

[17] Liu X, Vijaya Kumar B V K, You J, et al. Adaptive deep metric learning for identity-aware facial expression recognition[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. 2017: 20-29.

[18] Kim B K, Lee H, Roh J, et al. Hierarchical committee of deep cnns with exponentially-weighted decision fusion for static facial expression recognition[C]//Proceedings of the 2015 ACM on International Conference on Multimodal Interaction. 2015: 427-434.

[19] Levi G, Hassner T. Emotion recognition in the wild via convolutional neural networks and mapped binary patterns[C]//Proceedings of the 2015 ACM on international conference on multimodal interaction. 2015: 503-510.

[20] Abbasnejad I, Sridharan S, Nguyen D, et al. Using synthetic data to improve facial expression analysis with 3d convolutional networks[C]//Proceedings of the IEEE International Conference on Computer Vision Workshops. 2017: 1609-1618.

[21] Zhang F, Zhang T, Mao Q, et al. Joint pose and expression modeling for facial expression recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. 2018: 3359-3368.
```

文献 [15] 考虑到人脸表情数据特殊性，采用的增强方式都保留人脸表达情绪的核心区域；

考虑到增强后的数据很可能会缺乏人脸表达情绪的核心区域，文献 [16,17]在测试时仅采用人脸的中央区域作为测试，文献 [18,19] 对单幅图像增强后的所有图像进行预测并用取均值的方式预测该幅图像对应表情类别；

也有通过三维模型[20]或者GAN[21]来生成表情数据进行数据增强。







### 相关的 Github

1. https://github.com/luanshiyinyang/FacialExpressionRecognition
2. https://github.com/mangipudiprashanth7/Facial-Expression-Recognition-using-Deep-Learning
3. https://github-dotcom.gateway.web.tr/topics/facial-expression-recognition：人脸表情识别相关的 github
4. https://github-dotcom.gateway.web.tr/WuJie1010/Facial-Expression-Recognition.Pytorch，pytorch 实现的，1.1k 的 stars
5. https://paperswithcode.com/task/facial-expression-recognition
6. https://github.com/siqueira-hc/Efficient-Facial-Feature-Learning-with-Wide-Ensemble-based-Convolutional-Neural-Networks，AAAI2020 的一篇论文的实现代码
7. https://github.com/kaiwang960112/Self-Cure-Network，CVPR2020 的一篇论文实现代码





------

## 肤色识别

参考

- [6种肤色检测方法的原理及实现（C++）](https://blog.csdn.net/qinqinxiansheng/article/details/112795823)

- [python opencv 肤色检测](https://blog.csdn.net/weixin_40893939/article/details/84527037)

- [OpenCV——肤色检测](https://www.cnblogs.com/farewell-farewell/p/6010816.html)



### 肤色检测

常用的几种肤色检测算法：

1. 基于HSV 颜色空间
2. 基于RGB 颜色空间
3. 基于YCbCr颜色空间的阈值肤色识别
4. 基于YCbCr颜色空间和椭圆皮肤模型
5. 基于YCbCr颜色空间的Otsu阈值
6. OpenCV 自带的函数

原理如下图所示：

<img src="Notes/images/%E8%82%A4%E8%89%B2%E6%A3%80%E6%B5%8B%E5%8E%9F%E7%90%86.png" style="zoom:80%;" />



#### 基于YCbCr颜色空间和椭圆皮肤模型

**原理：**

将RGB图像转换到YCRCB空间，肤色像素点会聚集到一个椭圆区域。先定义一个椭圆模型，然后将每个RGB像素点转换到YCRCB空间比对是否再椭圆区域，是的话判断为皮肤。

代码：

```python
def ellipse_detect(image):
    """
    :param image: 图片路径
    :return: None
    """
    img = cv2.imread(image,cv2.IMREAD_COLOR)
    skinCrCbHist = np.zeros((256,256), dtype= np.uint8 )
    cv2.ellipse(skinCrCbHist ,(113,155),(23,15),43,0, 360, (255,255,255),-1)
 
    YCRCB = cv2.cvtColor(img,cv2.COLOR_BGR2YCR_CB)
    (y,cr,cb)= cv2.split(YCRCB)
    skin = np.zeros(cr.shape, dtype=np.uint8)
    (x,y)= cr.shape
    for i in range(0,x):
        for j in range(0,y):
            CR= YCRCB[i,j,1]
            CB= YCRCB[i,j,2]
            if skinCrCbHist [CR,CB]>0:
                skin[i,j]= 255
    cv2.namedWindow(image, cv2.WINDOW_NORMAL)
    cv2.imshow(image, img)
    dst = cv2.bitwise_and(img,img,mask= skin)
    cv2.namedWindow("cutout", cv2.WINDOW_NORMAL)
    cv2.imshow("cutout",dst)
    cv2.waitKey()
```



#### YCrCb颜色空间的Cr分量+Otsu法阈值分割算法

**原理**

针对YCRCB中CR分量的处理，将RGB转换为YCRCB，对CR通道单独进行otsu处理，otsu方法opencv里用threshold

代码：

```python
def cr_otsu(image):
    """YCrCb颜色空间的Cr分量+Otsu阈值分割
    :param image: 图片路径
    :return: None
    """
    img = cv2.imread(image, cv2.IMREAD_COLOR)
    ycrcb = cv2.cvtColor(img, cv2.COLOR_BGR2YCR_CB)
 
    (y, cr, cb) = cv2.split(ycrcb)
    cr1 = cv2.GaussianBlur(cr, (5, 5), 0)
    _, skin = cv2.threshold(cr1,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)
 
    cv2.namedWindow("image raw", cv2.WINDOW_NORMAL)
    cv2.imshow("image raw", img)
    cv2.namedWindow("image CR", cv2.WINDOW_NORMAL)
    cv2.imshow("image CR", cr1)
    cv2.namedWindow("Skin Cr+OTSU", cv2.WINDOW_NORMAL)
    cv2.imshow("Skin Cr+OTSU", skin)
 
    dst = cv2.bitwise_and(img, img, mask=skin)
    cv2.namedWindow("seperate", cv2.WINDOW_NORMAL)
    cv2.imshow("seperate", dst)
    cv2.waitKey()
```



#### 基于YCrCb颜色空间Cr, Cb范围筛选法

**原理**

和第二种方式类似，只不过是对CR和CB两个通道综合考虑

**代码**

```python
def crcb_range_sceening(image):
    """
    :param image: 图片路径
    :return: None
    """
    img = cv2.imread(image,cv2.IMREAD_COLOR)
    ycrcb=cv2.cvtColor(img,cv2.COLOR_BGR2YCR_CB)
    (y,cr,cb)= cv2.split(ycrcb)
 
    skin = np.zeros(cr.shape,dtype= np.uint8)
    (x,y)= cr.shape
    for i in range(0,x):
        for j in range(0,y):
            if (cr[i][j]>140)and(cr[i][j])<175 and (cr[i][j]>100) and (cb[i][j])<120:
                skin[i][j]= 255
            else:
                skin[i][j] = 0
    cv2.namedWindow(image,cv2.WINDOW_NORMAL)
    cv2.imshow(image,img)
    cv2.namedWindow(image+"skin2 cr+cb",cv2.WINDOW_NORMAL)
    cv2.imshow(image+"skin2 cr+cb",skin)
 
    dst = cv2.bitwise_and(img,img,mask=skin)
    cv2.namedWindow("cutout",cv2.WINDOW_NORMAL)
    cv2.imshow("cutout",dst)
 
    cv2.waitKey()
```



#### HSV颜色空间H,S,V范围筛选法

**原理**

转换空间然后每个通道设置一个阈值综合考虑，进行二值化操作。

**代码**：

```python
def hsv_detect(image):
    """
    :param image: 图片路径
    :return: None
    """
    img = cv2.imread(image,cv2.IMREAD_COLOR)
    hsv=cv2.cvtColor(img,cv2.COLOR_BGR2HSV)
    (_h,_s,_v)= cv2.split(hsv)
    skin= np.zeros(_h.shape,dtype=np.uint8)
    (x,y)= _h.shape
 
    for i in range(0,x):
        for j in range(0,y):
            if(_h[i][j]>7) and (_h[i][j]<20) and (_s[i][j]>28) and (_s[i][j]<255) and (_v[i][j]>50 ) and (_v[i][j]<255):
                skin[i][j] = 255
            else:
                skin[i][j] = 0
    cv2.namedWindow(image, cv2.WINDOW_NORMAL)
    cv2.imshow(image, img)
    cv2.namedWindow(image + "hsv", cv2.WINDOW_NORMAL)
    cv2.imshow(image + "hsv", skin)
    dst = cv2.bitwise_and(img, img, mask=skin)
    cv2.namedWindow("cutout", cv2.WINDOW_NORMAL)
    cv2.imshow("cutout", dst)
    cv2.waitKey()
```

